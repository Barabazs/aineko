{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Aineko","text":""},{"location":"#what-is-aineko","title":"What is Aineko?","text":"<p>Aineko is a python framework for building powerful data applications quickly.</p> How does Aineko work under the hood? <p>Aineko automatically configures tooling needed for production-ready data apps, like message brokers, distributed compute, and more. This allows you to focus on building your application instead of spending time with configuration and infrastructure.</p>"},{"location":"#why-use-aineko","title":"Why use Aineko?","text":"Main Features Quick to Code Increase speed to develop applications by 100% to 200%.* Always Robust Production-ready from the get-go. Scale with ease. Stateful Computation Aineko supports long-running stateful computations. Code Faster Use pre-built nodes that work with popular data sources like REST APIs. Composable Scale your project easily with engineering best practices. Fast Achieve microsecond latency between nodes.* Scalable Process billions or records per day with ease.* <p>*Estimate based on development experience by an internal team.</p>"},{"location":"#try-aineko","title":"Try Aineko","text":"<p>Build your first Aineko application in minutes!\u2002 \u2192</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Aineko pipeline configuration loader and runner</li> <li> CLI tool for running pipelines</li> <li> CLI tool for visualizing pipelines</li> <li> REST API and WebSocket pre-built nodes</li> <li> Built-in API server node</li> <li> Schema validation support</li> <li> Support multiple queuing systems beyond Kafka (RabbitMQ, etc)</li> <li> Support multiple runtime engines beyond ray (k8s, etc)</li> </ul>"},{"location":"#join-the-community","title":"Join the community","text":"<p>If you have questions about anything related to Aineko, you're always welcome to ask the community on GitHub or Slack.</p>"},{"location":"aineko_dream/","title":"Aineko Dream","text":"<p>Aineko Dream leverages the power of generative AI to create a starter Aineko pipeline based on your use case. </p>"},{"location":"aineko_dream/#generating-a-project","title":"Generating a Project","text":"<p>To generate a project, invoke the Aineko Dream CLI with a prompt with:</p> <pre><code>poetry run aineko dream \"create a pipeline that scrapes twitter and analyses the results to identify trends\" \"API-KEY\"\n</code></pre> <p>replacing <code>API-KEY</code> with a valid Aineko Dream API key. Contact support@aineko.dev to get an API key to try this feature.</p> <p>Aineko Dream goes on to create a complete aineko project, including node code, pipeline configuration and more using OpenAI's GPT-4 models. Upon completion, Aineko Dream publishes the project in the public GitHub repository dream-catcher.</p>"},{"location":"aineko_dream/#creating-your-aineko-dream-project","title":"Creating your Aineko Dream project","text":"<p>Upon completion of the previous step, <code>aineko create</code> offers an easy way to get started. To initialize an aineko project using the generated files from the previous step, run</p> <pre><code>poetry run aineko create --repo Convex-Labs/dream-catcher#12345\n</code></pre> <p>where the argument after <code>--repo</code> should be the unique ID associated to your generated project. This will be output in the result of the previous section.</p>"},{"location":"contributing/","title":"Contributing to Aineko","text":"<p>Thank you for your interest in contributing to Aineko!</p> <p>Here are the steps to get started quickly:</p>"},{"location":"contributing/#install-aineko-from-source","title":"Install Aineko from source","text":"<pre><code># install poetry\n$ curl -sSL https://install.python-poetry.org | python3 -\n\n$ git clone https://github.com/aineko-dev/aineko\n\n$ cd aineko &amp;&amp; poetry install --with dev,test\n</code></pre>"},{"location":"contributing/#make-your-changes-to-aineko-source-code","title":"Make your changes to Aineko source code","text":""},{"location":"contributing/#test-using-aineko-pipeline","title":"Test using Aineko pipeline","text":"<p>We highly encourage you to validate your changes by testing the project creation process end-to-end. This means validating the changes by running a local pipeline that uses your local aineko repository.</p> First, update poetry to use your local aineko repository. <pre><code>$ poetry lock\n$ poetry install\n</code></pre> Next, create an Aineko project in the parent directory. <pre><code>$ poetry run aineko create --output-dir ../\n</code></pre> Next, update the create aineko project to use the local aineko repository. Go to <code>../my-awesome-pipeline/pyproject.toml</code> and update the following line. pyproject.toml<pre><code>[tool.poetry.dependencies]\npython = \"&gt;=3.10,&lt;3.11\"\naineko = { path = \"&lt;path/to/aineko/git/repo&gt;\", develop=true}\n</code></pre> <p>Test if your changes worked by running the aineko pipeline and any other testing methods that are relevant.</p>"},{"location":"contributing/#run-lints-and-tests","title":"Run lints and tests","text":"Finally, after you have make all the changes, it's good to validate that you adhered to the style guide and you didn't break anything. <pre><code># Within aineko git repository\nmake lint\nmake unit-test\nmake integration-test\n</code></pre>"},{"location":"contributing/#push-make-a-pull-request-and-see-you-on-github","title":"Push, make a pull request and see you on GitHub","text":""},{"location":"contributing_docs/","title":"Contributing to Aineko docs","text":"<p>Thank you for your interest in contributing to the documentation.</p> <p>Here are the steps to get started quickly.</p>"},{"location":"contributing_docs/#install-aineko-from-source","title":"Install Aineko from source","text":"<pre><code># install poetry\n$ curl -sSL https://install.python-poetry.org | python3 -\n\n$ git clone https://github.com/aineko-dev/aineko\n\n$ cd aineko &amp;&amp; poetry install --with docs\n</code></pre>"},{"location":"contributing_docs/#make-your-changes-to-aineko-docs","title":"Make your changes to Aineko docs","text":"<p>Aineko raw documentation is in the form of markdown files found in the <code>docs</code> directory.</p> <p>Aineko uses Material for MkDocs to generate the documentation static site from markdown.</p>"},{"location":"contributing_docs/#set-up-a-local-server-to-view-changes-live","title":"Set up a local server to view changes live","text":"<code>MkDocs</code> comes with a tool to display local documentation as it would on the site, allowing you to view changes as you make them. Set up a local server that automatically updates using: <pre><code>$ poetry run mkdocs serve\n</code></pre> <p>Navigate to localhost:8000 to see the documentation site.</p>"},{"location":"contributing_docs/#run-lint","title":"Run lint","text":"Once you're happy with your changes, run the linters to keep any additional code stylistically consistent. You will need to install vale (a linter for prose) first. Installation instructions  can be found here. <pre><code>$ make lint\n$ make lint-docs\n</code></pre>"},{"location":"contributing_docs/#push-make-a-pull-request-and-see-you-on-github","title":"Push, make a pull request and see you on GitHub","text":"How to make a PR? <p>To make a PR, first create and push to GitHub a branch by running the following commands.</p> <pre><code>$ git checkout -b docs/&lt;branch-name&gt;\n$ git add .\n$ git commit -m \"docs: &lt;some descriptive message&gt;\"\n$ git push --set-upstream origin docs/&lt;branch-name&gt;\n</code></pre> <p>Next, navigate to the Aineko GitHub repo and select the <code>docs/&lt;branch-name&gt;</code> branch in the compare box.</p> <p>Document versioning</p> <p>On every merge to the <code>develop</code> branch, our CI automatically packages and publishes the docs under the <code>dev</code> version. Additionally, when a version of Aineko is tagged, our CI publishes that version of the docs as under that minor version (for example, the version with tag <code>1.2.3</code> will be published under <code>1.2</code>).</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#get-started-with-aineko","title":"Get started with Aineko","text":""},{"location":"quickstart/#technical-dependencies","title":"Technical dependencies","text":"<ol> <li>Docker or Docker Desktop</li> <li>Poetry (a python dependency manager)</li> <li>Python (version 3.10)</li> <li>Pip (a python package manager)</li> </ol> Check your dependencies before starting <p>It's important to make sure you have the correct dependencies installed. The only dependency which requires a specific version is Python. The other dependencies should work with any recent version.</p> <p>Let's check each dependency one by one. You can run the following commands in your terminal to check each dependency.</p> <ul> <li><code>docker --version</code> should return something like <code>Docker version 20.10.8, build 3967b7d</code></li> <li><code>python --version</code> should return something like <code>Python 3.10.12</code> Click here if you see another version.</li> <li><code>pip --version</code> should return something like <code>pip 23.0.1 from xxx/python3.10/site-packages/pip (python 3.10)</code></li> <li><code>poetry --version</code> should return something like <code>Poetry (version 1.6.1)</code></li> </ul>"},{"location":"quickstart/#install-aineko","title":"Install Aineko","text":"<pre><code>$ pip install aineko\n</code></pre> Having trouble getting the correct version of python? <p>We recommend using pyenv to manage your Python versions. Once you have pyenv installed, you can run the following commands in your project directory to install Python 3.10.</p> <pre><code>$ pyenv install 3.10\n$ pyenv local 3.10\n$ python --version\n\nPython 3.10.12\n</code></pre> <p>Pyenv is a great tool for managing Python versions, but it can be a bit tricky to get it set up correctly. If you're having trouble, check out the pyenv documentation or this tutorial. If you're still having trouble, feel free to reach out to us on Slack!</p>"},{"location":"quickstart/#create-a-template-pipeline-with-the-cli","title":"Create a template pipeline with the CLI","text":"<p>Create a pipeline using Aineko Dream</p> <p>Interested in creating a custom pipeline for a specific use case? Leverage Aineko Dream to generate a pipeline based on a prompt that describes your use case. See more detail here</p> You will see the following prompts as <code>aineko</code> tries to create a project directory containing the boilerplate you need for a pipeline. Feel free to use the defaults suggested. <p><pre><code>$ aineko create\n</code></pre> <pre><code>[1/4] project_name (My Awesome Pipeline):\n[2/4] project_slug (my_awesome_pipeline):\n[3/4] project_description (Behold my awesome pipeline!):\n[4/4] pipeline_slug (test-aineko-pipeline):\n</code></pre></p>"},{"location":"quickstart/#install-dependencies-in-the-new-pipeline","title":"Install dependencies in the new pipeline","text":"<pre><code>$ cd my_awesome_pipeline\n$ poetry install\n</code></pre>"},{"location":"quickstart/#start-aineko-background-services","title":"Start Aineko background services","text":"<pre><code>$ poetry run aineko service start\n</code></pre> <pre><code>Container zookeeper  Creating\nContainer zookeeper  Created\nContainer broker  Creating\nContainer broker  Created\nContainer zookeeper  Starting\nContainer zookeeper  Started\nContainer broker  Starting\nContainer broker  Started\n</code></pre>"},{"location":"quickstart/#start-the-template-pipeline","title":"Start the template pipeline","text":"<pre><code>$ poetry run aineko run ./conf/pipeline.yml\n</code></pre> <pre><code>INFO - Application is starting.\nINFO - Creating dataset: aineko-pipeline.sequence: {'type': 'kafka_stream'}\nINFO - All datasets created.\nINFO worker.py:1664 -- Started a local Ray instance.\n</code></pre>"},{"location":"quickstart/#check-the-data-being-streamed","title":"Check the data being streamed","text":"To view messages running in one of the user-defined datasets: <p><pre><code>$ poetry run aineko stream --dataset test-aineko-pipeline.test_sequence --from-beginning\n</code></pre> <pre><code>{\"timestamp\": \"2023-11-10 17:27:20\", \"dataset\": \"sequence\", \"source_pipeline\": \"test-aineko-pipeline\", \"source_node\": \"sequence\", \"message\": 1}\n{\"timestamp\": \"2023-11-10 17:27:20\", \"dataset\": \"sequence\", \"source_pipeline\": \"test-aineko-pipeline\", \"source_node\": \"sequence\", \"message\": 2}\n</code></pre></p> Alternatively, to view logs stored in the built-in <code>logging</code> dataset: <p><pre><code>$ poetry run aineko stream --dataset logging --from-beginning\n</code></pre> <pre><code>{\"timestamp\": \"2023-11-10 17:46:15\", \"dataset\": \"logging\", \"source_pipeline\": \"test-aineko-pipeline\", \"source_node\": \"sum\", \"message\": {\"log\": \"Received input: 1. Adding 1...\", \"level\": \"info\"}}\n</code></pre></p> <p>Note</p> <p>User-defined datasets have the pipeline name automatically prefixed, but the special built-in dataset <code>logging</code> doesn't.</p>"},{"location":"quickstart/#stop-aineko-background-services","title":"Stop Aineko background services","text":"<pre><code>$ poetry run aineko service stop\n</code></pre> <p>So that's it to get an Aineko pipeline running. How smooth was that?</p> What does the above output mean? <p>An aineko pipeline is made up of Dataset(s) and Node(s). A Dataset can be thought of as a mailbox. Nodes pass messages to this mailbox, that can be read by many other Nodes.</p> <p>A Node is an abstraction for some computation, a function if you will. At the same time a Node can be a producer and/or a consumer of a Dataset. (mailbox)</p> <p>The output means that we have successfully created three datasets - test_sequence, test_sum and logging, and that we have created two nodes - sum and sequence.</p> <p>To learn more about Pipeline, Datasets and Nodes, see concepts.</p>"},{"location":"quickstart/#visualizing-the-pipeline","title":"Visualizing the pipeline","text":"Using the Aineko CLI, you can also see the above pipeline rendered in the browser. This is helpful for quickly checking your pipeline as you iterate and evolve your architecture. <pre><code>$ poetry run aineko visualize --browser ./conf/pipeline.yml\n</code></pre> <p>Visualization output</p> <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nN_sequence((sequence)):::nodeClass --&gt;  T_test_sequence[test_sequence]:::datasetClass\nT_test_sequence[test_sequence]:::datasetClass --&gt;  N_sum((sum)):::nodeClass\nN_sum((sum)):::nodeClass --&gt;  T_test_sum[test_sum]:::datasetClass</code></pre>"},{"location":"api_reference/abstract_node/","title":"<code>AbstractNode</code>","text":"<p>The <code>AbstractNode</code> class serves as the base class for all user-defined nodes.</p>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode","title":"<code>aineko.AbstractNode</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Node base class for all nodes in the pipeline.</p> <p>Nodes are the building blocks of the pipeline and are responsible for executing the pipeline. Nodes are designed to be modular and can be combined to create a pipeline. The node base class provides helper methods for setting up the consumer and producer for a node. The execute method is a wrapper for the _execute method which is to be implemented by subclasses. The _execute method is where the node logic is implemented by the user.</p> <p>Attributes:</p> Name Type Description <code>consumers</code> <code>dict</code> <p>dict of DatasetConsumer objects for inputs to node</p> <code>producers</code> <code>dict</code> <p>dict of DatasetProducer objects for outputs of node</p> <code>last_hearbeat</code> <code>float</code> <p>timestamp of the last heartbeat</p> <code>test</code> <code>bool</code> <p>True if node is in test mode else False</p> <code>log_levels</code> <code>tuple</code> <p>tuple of allowed log levels</p> <code>local_state</code> <code>dict</code> <p>shared local state between nodes. Used for intra- pipeline communication without dataset dependency.</p> <p>Methods:</p> Name Description <code>setup_datasets</code> <p>setup the consumers and producers for a node</p> <code>execute</code> <p>execute the node, wrapper for _execute method</p> <code>_execute</code> <p>execute the node, to be implemented by subclasses</p> Source code in <code>aineko/core/node.py</code> <pre><code>class AbstractNode(ABC):\n    \"\"\"Node base class for all nodes in the pipeline.\n\n    Nodes are the building blocks of the pipeline and are responsible for\n    executing the pipeline. Nodes are designed to be modular and can be\n    combined to create a pipeline. The node base class provides helper methods\n    for setting up the consumer and producer for a node. The execute method is\n    a wrapper for the _execute method which is to be implemented by subclasses.\n    The _execute method is where the node logic is implemented by the user.\n\n    Attributes:\n        consumers (dict): dict of DatasetConsumer objects for inputs to node\n        producers (dict): dict of DatasetProducer objects for outputs of node\n        last_hearbeat (float): timestamp of the last heartbeat\n        test (bool): True if node is in test mode else False\n        log_levels (tuple): tuple of allowed log levels\n        local_state (dict): shared local state between nodes. Used for intra-\n            pipeline communication without dataset dependency.\n\n    Methods:\n        setup_datasets: setup the consumers and producers for a node\n        execute: execute the node, wrapper for _execute method\n        _execute: execute the node, to be implemented by subclasses\n    \"\"\"\n\n    def __init__(\n        self,\n        node_name: str | None,\n        pipeline_name: str,\n        poison_pill: Optional[ray.actor.ActorHandle] = None,\n        test: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the node.\"\"\"\n        self.name = node_name or self.__class__.__name__\n        self.pipeline_name = pipeline_name\n        self.last_heartbeat = time.time()\n        self.consumers: Dict = {}\n        self.producers: Dict = {}\n        self.params: Dict = {}\n        self.test = test\n        self.log_levels = AINEKO_CONFIG.get(\"LOG_LEVELS\")\n        self.poison_pill = poison_pill\n\n    def enable_test_mode(self) -&gt; None:\n        \"\"\"Enable test mode.\"\"\"\n        self.test = True\n\n    def setup_datasets(\n        self,\n        datasets: Dict[str, dict],\n        inputs: Optional[List[str]] = None,\n        outputs: Optional[List[str]] = None,\n        prefix: Optional[str] = None,\n        has_pipeline_prefix: bool = False,\n    ) -&gt; None:\n        \"\"\"Setup the consumer and producer for a node.\n\n        Args:\n            datasets: dataset configuration\n            node: name of the node\n            pipeline: name of the pipeline\n            inputs: list of dataset names for the inputs to the node\n            outputs: list of dataset names for the outputs of the node\n            prefix: prefix for topic name (`&lt;prefix&gt;.&lt;dataset_name&gt;`)\n            has_pipeline_prefix: whether the dataset name has pipeline name\n            prefix\n        \"\"\"\n        inputs = inputs or []\n        self.consumers.update(\n            {\n                dataset_name: DatasetConsumer(\n                    dataset_name=dataset_name,\n                    node_name=self.name,\n                    pipeline_name=self.pipeline_name,\n                    dataset_config=datasets.get(dataset_name, {}),\n                    prefix=prefix,\n                    has_pipeline_prefix=has_pipeline_prefix,\n                )\n                for dataset_name in inputs\n            }\n        )\n\n        outputs = outputs or []\n        self.producers.update(\n            {\n                dataset_name: DatasetProducer(\n                    dataset_name=dataset_name,\n                    node_name=self.name,\n                    pipeline_name=self.pipeline_name,\n                    dataset_config=datasets.get(dataset_name, {}),\n                    prefix=prefix,\n                    has_pipeline_prefix=has_pipeline_prefix,\n                )\n                for dataset_name in outputs\n            }\n        )\n\n    def setup_test(\n        self,\n        inputs: Optional[dict] = None,\n        outputs: Optional[list] = None,\n        params: Optional[dict] = None,\n    ) -&gt; None:\n        \"\"\"Setup the node for testing.\n\n        Args:\n            inputs: inputs to the node, format should be {\"dataset\": [1, 2, 3]}\n            outputs: outputs of the node, format should be\n                [\"dataset_1\", \"dataset_2\", ...]\n            params: dictionary of parameters to make accessible to _execute\n\n        Raises:\n            RuntimeError: if node is not in test mode\n        \"\"\"\n        if self.test is False:\n            raise RuntimeError(\n                \"Node is not in test mode. \"\n                \"Please initialize with `enable_test_mode()`.\"\n            )\n\n        inputs = inputs or {}\n        self.consumers = {\n            dataset_name: FakeDatasetConsumer(\n                dataset_name=dataset_name,\n                node_name=self.__class__.__name__,\n                values=values,\n            )\n            for dataset_name, values in inputs.items()\n        }\n        outputs = outputs or []\n        outputs.extend(TESTING_NODE_CONFIG.get(\"DATASETS\"))\n        self.producers = {\n            dataset_name: FakeDatasetProducer(\n                dataset_name=dataset_name,\n                node_name=self.__class__.__name__,\n            )\n            for dataset_name in outputs\n        }\n        self.params = params or {}\n\n    def log(self, message: str, level: str = \"info\") -&gt; None:\n        \"\"\"Log a message to the logging dataset.\n\n        Args:\n            message: Message to log\n            level: Logging level. Defaults to \"info\". Options are:\n                \"info\", \"debug\", \"warning\", \"error\", \"critical\"\n        Raises:\n            ValueError: if invalid logging level is provided\n        \"\"\"\n        if level not in self.log_levels:\n            raise ValueError(\n                f\"Invalid logging level {level}. Valid options are: \"\n                f\"{', '.join(self.log_levels)}\"\n            )\n        out_msg = {\"log\": message, \"level\": level}\n        self.producers[DEFAULT_KAFKA_CONFIG.get(\"LOGGING_DATASET\")].produce(\n            out_msg\n        )\n\n    def _log_traceback(self) -&gt; None:\n        \"\"\"Logs the traceback of an exception.\"\"\"\n        exc_info = traceback.format_exc()\n        self.log(exc_info, level=\"debug\")\n\n    def execute(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Execute the node.\n\n        Wrapper for _execute method to be implemented by subclasses.\n\n        Args:\n            params: Parameters to use to execute the node. Defaults to None.\n        \"\"\"\n        params = params or {}\n        run_loop = True\n\n        try:\n            self._pre_loop_hook(params)\n        except Exception:  # pylint: disable=broad-except\n            self._log_traceback()\n            raise\n\n        while run_loop is not False:\n            # Monitoring\n            try:\n                run_loop = self._execute(params)  # type: ignore\n            except Exception:  # pylint: disable=broad-except\n                self._log_traceback()\n                raise\n\n        self.log(f\"Execution loop complete for node: {self.__class__.__name__}\")\n        self._post_loop_hook(params)\n\n    def activate_poison_pill(self) -&gt; None:\n        \"\"\"Activates poison pill, shutting down entire pipeline.\"\"\"\n        if self.poison_pill:\n            ray.get(self.poison_pill.activate.remote())\n\n    @abstractmethod\n    def _execute(self, params: dict) -&gt; Optional[bool]:\n        \"\"\"Execute the node.\n\n        Args:\n            params: Parameters to use to execute the node.\n\n        Note:\n            Method to be implemented by subclasses\n\n        Raises:\n            NotImplementedError: if method is not implemented in subclass\n        \"\"\"\n        raise NotImplementedError(\"_execute method not implemented\")\n\n    def run_test(self, runtime: Optional[int] = None) -&gt; dict:\n        \"\"\"Execute the node in testing mode.\n\n        Runs the steps in execute that involves the user defined methods.\n        Includes pre_loop_hook, _execute, and post_loop_hook.\n\n        Args:\n            runtime: Number of seconds to run the execute loop for.\n\n        Returns:\n            dict: dataset names and values produced by the node.\n        \"\"\"\n        if self.test is False:\n            raise RuntimeError(\n                \"Node is not in test mode. \"\n                \"Please initialize with `enable_test_mode()`.\"\n            )\n        run_loop = True\n        start_time = time.time()\n\n        self._pre_loop_hook(self.params)\n        while run_loop is not False:\n            run_loop = self._execute(self.params)  # type: ignore\n\n            # Do not end loop if runtime not exceeded\n            if runtime is not None:\n                if time.time() - start_time &lt; runtime:\n                    continue\n\n            # End loop if all consumers are empty\n            if self.consumers and all(\n                consumer.empty for consumer in self.consumers.values()\n            ):\n                run_loop = False\n\n        self._post_loop_hook(self.params)\n\n        return {\n            dataset_name: producer.values\n            for dataset_name, producer in self.producers.items()\n        }\n\n    def run_test_yield(\n        self, runtime: Optional[int] = None\n    ) -&gt; Generator[Tuple[dict, dict, \"AbstractNode\"], None, None]:\n        \"\"\"Execute the node in testing mode, yielding at each iteration.\n\n        This method is an alternative to `run_test`. Instead of returning the\n        aggregated output, it yields the most recently consumed value, the\n        produced value and the current node instance at each iteration. This is\n        useful for testing nodes that either don't produce any output or if you\n        need to test intermediate outputs. Testing state modifications is also\n        possible using this method.\n\n        Args:\n            runtime: Number of seconds to run the execute loop for.\n\n        Yields:\n            A tuple containing the most recent input value, output value and\n            the node instance.\n\n        Example:\n            &gt;&gt;&gt; for input, output, node_instance in sequencer.run_test_yield():\n            &gt;&gt;&gt;     print(f\"Input: {input}, Output: {output})\n            &gt;&gt;&gt;     print(f\"Node Instance: {node_instance}\")\n        \"\"\"\n        if self.test is False:\n            raise RuntimeError(\n                \"Node is not in test mode. \"\n                \"Please initialize with `enable_test_mode()`.\"\n            )\n        run_loop = True\n        start_time = time.time()\n\n        self._pre_loop_hook(self.params)\n        while run_loop is not False:\n            last_produced_values = {}\n            last_consumed_values = {}\n\n            run_loop = self._execute(self.params)  # type: ignore\n\n            # Do not end loop if runtime not exceeded\n            if runtime is not None:\n                if time.time() - start_time &lt; runtime:\n                    continue\n\n            # End loop if all consumers are empty\n            if self.consumers and all(\n                consumer.empty for consumer in self.consumers.values()\n            ):\n                run_loop = False\n\n            # Capture last consumed values\n            for dataset_name, consumer in self.consumers.items():\n                if consumer.values:\n                    last_value = consumer.values[0]\n                    last_consumed_values[dataset_name] = last_value\n\n            # Capture last produced values\n            for dataset_name, producer in self.producers.items():\n                if producer.values:\n                    last_value = producer.values[-1]\n                    last_produced_values[dataset_name] = last_value\n\n            yield (last_consumed_values, last_produced_values, self)\n\n        self._post_loop_hook(self.params)\n\n    def _pre_loop_hook(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Hook to be called before the node loop. User overrideable.\n\n        Args:\n            params: Parameters to use to execute the node.\n\n        Note:\n            Method (optional) to be implemented by subclasses.\n        \"\"\"\n        pass\n\n    def _post_loop_hook(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Hook to be called after the node loop. User overrideable.\n\n        Args:\n            params: Parameters to use to execute the node.\n\n        Note:\n            Method (optional) to be implemented by subclasses.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.__init__","title":"<code>__init__(node_name, pipeline_name, poison_pill=None, test=False)</code>","text":"<p>Initialize the node.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def __init__(\n    self,\n    node_name: str | None,\n    pipeline_name: str,\n    poison_pill: Optional[ray.actor.ActorHandle] = None,\n    test: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the node.\"\"\"\n    self.name = node_name or self.__class__.__name__\n    self.pipeline_name = pipeline_name\n    self.last_heartbeat = time.time()\n    self.consumers: Dict = {}\n    self.producers: Dict = {}\n    self.params: Dict = {}\n    self.test = test\n    self.log_levels = AINEKO_CONFIG.get(\"LOG_LEVELS\")\n    self.poison_pill = poison_pill\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.activate_poison_pill","title":"<code>activate_poison_pill()</code>","text":"<p>Activates poison pill, shutting down entire pipeline.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def activate_poison_pill(self) -&gt; None:\n    \"\"\"Activates poison pill, shutting down entire pipeline.\"\"\"\n    if self.poison_pill:\n        ray.get(self.poison_pill.activate.remote())\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.enable_test_mode","title":"<code>enable_test_mode()</code>","text":"<p>Enable test mode.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def enable_test_mode(self) -&gt; None:\n    \"\"\"Enable test mode.\"\"\"\n    self.test = True\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.execute","title":"<code>execute(params=None)</code>","text":"<p>Execute the node.</p> <p>Wrapper for _execute method to be implemented by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Optional[dict]</code> <p>Parameters to use to execute the node. Defaults to None.</p> <code>None</code> Source code in <code>aineko/core/node.py</code> <pre><code>def execute(self, params: Optional[dict] = None) -&gt; None:\n    \"\"\"Execute the node.\n\n    Wrapper for _execute method to be implemented by subclasses.\n\n    Args:\n        params: Parameters to use to execute the node. Defaults to None.\n    \"\"\"\n    params = params or {}\n    run_loop = True\n\n    try:\n        self._pre_loop_hook(params)\n    except Exception:  # pylint: disable=broad-except\n        self._log_traceback()\n        raise\n\n    while run_loop is not False:\n        # Monitoring\n        try:\n            run_loop = self._execute(params)  # type: ignore\n        except Exception:  # pylint: disable=broad-except\n            self._log_traceback()\n            raise\n\n    self.log(f\"Execution loop complete for node: {self.__class__.__name__}\")\n    self._post_loop_hook(params)\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.log","title":"<code>log(message, level='info')</code>","text":"<p>Log a message to the logging dataset.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message to log</p> required <code>level</code> <code>str</code> <p>Logging level. Defaults to \"info\". Options are: \"info\", \"debug\", \"warning\", \"error\", \"critical\"</p> <code>'info'</code> <p>Raises:     ValueError: if invalid logging level is provided</p> Source code in <code>aineko/core/node.py</code> <pre><code>def log(self, message: str, level: str = \"info\") -&gt; None:\n    \"\"\"Log a message to the logging dataset.\n\n    Args:\n        message: Message to log\n        level: Logging level. Defaults to \"info\". Options are:\n            \"info\", \"debug\", \"warning\", \"error\", \"critical\"\n    Raises:\n        ValueError: if invalid logging level is provided\n    \"\"\"\n    if level not in self.log_levels:\n        raise ValueError(\n            f\"Invalid logging level {level}. Valid options are: \"\n            f\"{', '.join(self.log_levels)}\"\n        )\n    out_msg = {\"log\": message, \"level\": level}\n    self.producers[DEFAULT_KAFKA_CONFIG.get(\"LOGGING_DATASET\")].produce(\n        out_msg\n    )\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.run_test","title":"<code>run_test(runtime=None)</code>","text":"<p>Execute the node in testing mode.</p> <p>Runs the steps in execute that involves the user defined methods. Includes pre_loop_hook, _execute, and post_loop_hook.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>Optional[int]</code> <p>Number of seconds to run the execute loop for.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dataset names and values produced by the node.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def run_test(self, runtime: Optional[int] = None) -&gt; dict:\n    \"\"\"Execute the node in testing mode.\n\n    Runs the steps in execute that involves the user defined methods.\n    Includes pre_loop_hook, _execute, and post_loop_hook.\n\n    Args:\n        runtime: Number of seconds to run the execute loop for.\n\n    Returns:\n        dict: dataset names and values produced by the node.\n    \"\"\"\n    if self.test is False:\n        raise RuntimeError(\n            \"Node is not in test mode. \"\n            \"Please initialize with `enable_test_mode()`.\"\n        )\n    run_loop = True\n    start_time = time.time()\n\n    self._pre_loop_hook(self.params)\n    while run_loop is not False:\n        run_loop = self._execute(self.params)  # type: ignore\n\n        # Do not end loop if runtime not exceeded\n        if runtime is not None:\n            if time.time() - start_time &lt; runtime:\n                continue\n\n        # End loop if all consumers are empty\n        if self.consumers and all(\n            consumer.empty for consumer in self.consumers.values()\n        ):\n            run_loop = False\n\n    self._post_loop_hook(self.params)\n\n    return {\n        dataset_name: producer.values\n        for dataset_name, producer in self.producers.items()\n    }\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.run_test_yield","title":"<code>run_test_yield(runtime=None)</code>","text":"<p>Execute the node in testing mode, yielding at each iteration.</p> <p>This method is an alternative to <code>run_test</code>. Instead of returning the aggregated output, it yields the most recently consumed value, the produced value and the current node instance at each iteration. This is useful for testing nodes that either don't produce any output or if you need to test intermediate outputs. Testing state modifications is also possible using this method.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>Optional[int]</code> <p>Number of seconds to run the execute loop for.</p> <code>None</code> <p>Yields:</p> Type Description <code>dict</code> <p>A tuple containing the most recent input value, output value and</p> <code>dict</code> <p>the node instance.</p> Example <p>for input, output, node_instance in sequencer.run_test_yield():     print(f\"Input: {input}, Output: {output})     print(f\"Node Instance: {node_instance}\")</p> Source code in <code>aineko/core/node.py</code> <pre><code>def run_test_yield(\n    self, runtime: Optional[int] = None\n) -&gt; Generator[Tuple[dict, dict, \"AbstractNode\"], None, None]:\n    \"\"\"Execute the node in testing mode, yielding at each iteration.\n\n    This method is an alternative to `run_test`. Instead of returning the\n    aggregated output, it yields the most recently consumed value, the\n    produced value and the current node instance at each iteration. This is\n    useful for testing nodes that either don't produce any output or if you\n    need to test intermediate outputs. Testing state modifications is also\n    possible using this method.\n\n    Args:\n        runtime: Number of seconds to run the execute loop for.\n\n    Yields:\n        A tuple containing the most recent input value, output value and\n        the node instance.\n\n    Example:\n        &gt;&gt;&gt; for input, output, node_instance in sequencer.run_test_yield():\n        &gt;&gt;&gt;     print(f\"Input: {input}, Output: {output})\n        &gt;&gt;&gt;     print(f\"Node Instance: {node_instance}\")\n    \"\"\"\n    if self.test is False:\n        raise RuntimeError(\n            \"Node is not in test mode. \"\n            \"Please initialize with `enable_test_mode()`.\"\n        )\n    run_loop = True\n    start_time = time.time()\n\n    self._pre_loop_hook(self.params)\n    while run_loop is not False:\n        last_produced_values = {}\n        last_consumed_values = {}\n\n        run_loop = self._execute(self.params)  # type: ignore\n\n        # Do not end loop if runtime not exceeded\n        if runtime is not None:\n            if time.time() - start_time &lt; runtime:\n                continue\n\n        # End loop if all consumers are empty\n        if self.consumers and all(\n            consumer.empty for consumer in self.consumers.values()\n        ):\n            run_loop = False\n\n        # Capture last consumed values\n        for dataset_name, consumer in self.consumers.items():\n            if consumer.values:\n                last_value = consumer.values[0]\n                last_consumed_values[dataset_name] = last_value\n\n        # Capture last produced values\n        for dataset_name, producer in self.producers.items():\n            if producer.values:\n                last_value = producer.values[-1]\n                last_produced_values[dataset_name] = last_value\n\n        yield (last_consumed_values, last_produced_values, self)\n\n    self._post_loop_hook(self.params)\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.setup_datasets","title":"<code>setup_datasets(datasets, inputs=None, outputs=None, prefix=None, has_pipeline_prefix=False)</code>","text":"<p>Setup the consumer and producer for a node.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Dict[str, dict]</code> <p>dataset configuration</p> required <code>node</code> <p>name of the node</p> required <code>pipeline</code> <p>name of the pipeline</p> required <code>inputs</code> <code>Optional[List[str]]</code> <p>list of dataset names for the inputs to the node</p> <code>None</code> <code>outputs</code> <code>Optional[List[str]]</code> <p>list of dataset names for the outputs of the node</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>prefix for topic name (<code>&lt;prefix&gt;.&lt;dataset_name&gt;</code>)</p> <code>None</code> <code>has_pipeline_prefix</code> <code>bool</code> <p>whether the dataset name has pipeline name</p> <code>False</code> Source code in <code>aineko/core/node.py</code> <pre><code>def setup_datasets(\n    self,\n    datasets: Dict[str, dict],\n    inputs: Optional[List[str]] = None,\n    outputs: Optional[List[str]] = None,\n    prefix: Optional[str] = None,\n    has_pipeline_prefix: bool = False,\n) -&gt; None:\n    \"\"\"Setup the consumer and producer for a node.\n\n    Args:\n        datasets: dataset configuration\n        node: name of the node\n        pipeline: name of the pipeline\n        inputs: list of dataset names for the inputs to the node\n        outputs: list of dataset names for the outputs of the node\n        prefix: prefix for topic name (`&lt;prefix&gt;.&lt;dataset_name&gt;`)\n        has_pipeline_prefix: whether the dataset name has pipeline name\n        prefix\n    \"\"\"\n    inputs = inputs or []\n    self.consumers.update(\n        {\n            dataset_name: DatasetConsumer(\n                dataset_name=dataset_name,\n                node_name=self.name,\n                pipeline_name=self.pipeline_name,\n                dataset_config=datasets.get(dataset_name, {}),\n                prefix=prefix,\n                has_pipeline_prefix=has_pipeline_prefix,\n            )\n            for dataset_name in inputs\n        }\n    )\n\n    outputs = outputs or []\n    self.producers.update(\n        {\n            dataset_name: DatasetProducer(\n                dataset_name=dataset_name,\n                node_name=self.name,\n                pipeline_name=self.pipeline_name,\n                dataset_config=datasets.get(dataset_name, {}),\n                prefix=prefix,\n                has_pipeline_prefix=has_pipeline_prefix,\n            )\n            for dataset_name in outputs\n        }\n    )\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.setup_test","title":"<code>setup_test(inputs=None, outputs=None, params=None)</code>","text":"<p>Setup the node for testing.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Optional[dict]</code> <p>inputs to the node, format should be {\"dataset\": [1, 2, 3]}</p> <code>None</code> <code>outputs</code> <code>Optional[list]</code> <p>outputs of the node, format should be [\"dataset_1\", \"dataset_2\", ...]</p> <code>None</code> <code>params</code> <code>Optional[dict]</code> <p>dictionary of parameters to make accessible to _execute</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if node is not in test mode</p> Source code in <code>aineko/core/node.py</code> <pre><code>def setup_test(\n    self,\n    inputs: Optional[dict] = None,\n    outputs: Optional[list] = None,\n    params: Optional[dict] = None,\n) -&gt; None:\n    \"\"\"Setup the node for testing.\n\n    Args:\n        inputs: inputs to the node, format should be {\"dataset\": [1, 2, 3]}\n        outputs: outputs of the node, format should be\n            [\"dataset_1\", \"dataset_2\", ...]\n        params: dictionary of parameters to make accessible to _execute\n\n    Raises:\n        RuntimeError: if node is not in test mode\n    \"\"\"\n    if self.test is False:\n        raise RuntimeError(\n            \"Node is not in test mode. \"\n            \"Please initialize with `enable_test_mode()`.\"\n        )\n\n    inputs = inputs or {}\n    self.consumers = {\n        dataset_name: FakeDatasetConsumer(\n            dataset_name=dataset_name,\n            node_name=self.__class__.__name__,\n            values=values,\n        )\n        for dataset_name, values in inputs.items()\n    }\n    outputs = outputs or []\n    outputs.extend(TESTING_NODE_CONFIG.get(\"DATASETS\"))\n    self.producers = {\n        dataset_name: FakeDatasetProducer(\n            dataset_name=dataset_name,\n            node_name=self.__class__.__name__,\n        )\n        for dataset_name in outputs\n    }\n    self.params = params or {}\n</code></pre>"},{"location":"api_reference/config/","title":"Pipeline <code>Config</code>","text":"<p>The following Pydantic schema shows the format of a pipeline configuration file. Expand the following source code blocks to view the keys at each level.</p>"},{"location":"api_reference/config/#aineko.models.config_schema.Config","title":"<code>aineko.models.config_schema.Config</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Config model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Config(BaseModel):\n    \"\"\"Config model.\"\"\"\n\n    class Pipeline(BaseModel):\n        \"\"\"Pipeline model.\"\"\"\n\n        class Dataset(BaseModel):\n            \"\"\"Dataset model.\"\"\"\n\n            type: str\n            params: Optional[dict]\n\n        class Node(BaseModel):\n            \"\"\"Node model.\"\"\"\n\n            class_name: str = Field(..., alias=\"class\")\n            node_params: Optional[dict]\n            node_settings: Optional[dict]\n            inputs: Optional[list]\n            outputs: Optional[list]\n\n        name: str\n        default_node_settings: Optional[dict]\n        nodes: dict[str, Node]\n        datasets: dict[str, Dataset]\n\n    pipeline: Pipeline\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline","title":"<code>Pipeline</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Pipeline model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Pipeline(BaseModel):\n    \"\"\"Pipeline model.\"\"\"\n\n    class Dataset(BaseModel):\n        \"\"\"Dataset model.\"\"\"\n\n        type: str\n        params: Optional[dict]\n\n    class Node(BaseModel):\n        \"\"\"Node model.\"\"\"\n\n        class_name: str = Field(..., alias=\"class\")\n        node_params: Optional[dict]\n        node_settings: Optional[dict]\n        inputs: Optional[list]\n        outputs: Optional[list]\n\n    name: str\n    default_node_settings: Optional[dict]\n    nodes: dict[str, Node]\n    datasets: dict[str, Dataset]\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.Dataset","title":"<code>Dataset</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Dataset model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Dataset(BaseModel):\n    \"\"\"Dataset model.\"\"\"\n\n    type: str\n    params: Optional[dict]\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.Node","title":"<code>Node</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Node model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Node(BaseModel):\n    \"\"\"Node model.\"\"\"\n\n    class_name: str = Field(..., alias=\"class\")\n    node_params: Optional[dict]\n    node_settings: Optional[dict]\n    inputs: Optional[list]\n    outputs: Optional[list]\n</code></pre>"},{"location":"api_reference/config_loader/","title":"<code>ConfigLoader</code>","text":"<p>Reference for the <code>ConfigLoader</code> class, which contains the logic used for parsing a pipeline configuration file into a format that can be understood by the <code>Runner</code>.</p>"},{"location":"api_reference/config_loader/#aineko.ConfigLoader","title":"<code>aineko.ConfigLoader</code>","text":"<p>Class to read yaml config files.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_config_file</code> <code>str</code> <p>path of pipeline config file. Defaults</p> required <p>Attributes:</p> Name Type Description <code>pipeline_config_file</code> <code>str</code> <p>path to the pipeline configuration file</p> <code>config_schema</code> <code>Config</code> <p>Pydantic model to validate a pipeline config</p> <p>Methods:</p> Name Description <code>load_config</code> <p>loads and validates the pipeline config from a yaml file</p> Source code in <code>aineko/core/config_loader.py</code> <pre><code>class ConfigLoader:\n    \"\"\"Class to read yaml config files.\n\n    Args:\n        pipeline_config_file: path of pipeline config file. Defaults\n        to `DEFAULT_PIPELINE_CONFIG`.\n\n    Attributes:\n        pipeline_config_file (str): path to the pipeline configuration file\n        config_schema (Config): Pydantic model to validate a pipeline config\n\n    Methods:\n        load_config: loads and validates the pipeline config from a yaml file\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline_config_file: str,\n    ):\n        \"\"\"Initialize ConfigLoader.\"\"\"\n        self.pipeline_config_file = pipeline_config_file or AINEKO_CONFIG.get(\n            \"DEFAULT_PIPELINE_CONFIG\"\n        )\n\n        # Setup config schema\n        self.config_schema = Config\n\n    def load_config(self) -&gt; dict:\n        \"\"\"Load and validate the pipeline config.\n\n        Raises:\n            ValidationError: If the config does not match the schema\n\n        Returns:\n            The pipeline config as a dictionary\n        \"\"\"\n        config = load_yaml(self.pipeline_config_file)\n\n        try:\n            Config(**config)\n        except ValidationError as e:\n            logger.error(\n                \"Schema validation failed for pipeline `%s` loaded from %s. \"\n                \"See detailed error below.\",\n                config[\"pipeline\"][\"name\"],\n                self.pipeline_config_file,\n            )\n            raise e\n\n        return config\n\n    @overload\n    def _update_params(self, value: dict, params: dict) -&gt; dict:\n        ...\n\n    @overload\n    def _update_params(self, value: list, params: dict) -&gt; list:\n        ...\n\n    @overload\n    def _update_params(self, value: str, params: dict) -&gt; str:\n        ...\n\n    @overload\n    def _update_params(self, value: int, params: dict) -&gt; int:\n        ...\n\n    def _update_params(\n        self, value: Union[dict, list, str, int], params: dict\n    ) -&gt; Union[dict, list, str, int]:\n        \"\"\"Update value with params.\n\n        Recursively calls the method if value is a list or dictionary until it\n        reaches a string or int. If string then formats the str with variable\n        mapping in params dict.\n\n        Args:\n            value: value to update\n            params: params to update value with\n\n        Returns:\n            object with updated values (dict, list, str, or int)\n        \"\"\"\n        if isinstance(value, dict):\n            new_dict_val = {}\n            for key, val in value.items():\n                new_dict_val[key] = self._update_params(val, params)\n            return new_dict_val\n        if isinstance(value, list):\n            new_list_val: list = []\n            for val in value:\n                new_list_val.append(self._update_params(val, params))\n            return new_list_val\n        if isinstance(value, str):\n            for key, val in params.items():\n                value = value.replace(f\"${key}\", val)\n            return value\n        if isinstance(value, (int, float)):\n            return value\n        raise ValueError(\n            f\"Invalid value type {type(value)}. \"\n            \"Expected dict, list, str, or int.\"\n        )\n</code></pre>"},{"location":"api_reference/config_loader/#aineko.ConfigLoader.__init__","title":"<code>__init__(pipeline_config_file)</code>","text":"<p>Initialize ConfigLoader.</p> Source code in <code>aineko/core/config_loader.py</code> <pre><code>def __init__(\n    self,\n    pipeline_config_file: str,\n):\n    \"\"\"Initialize ConfigLoader.\"\"\"\n    self.pipeline_config_file = pipeline_config_file or AINEKO_CONFIG.get(\n        \"DEFAULT_PIPELINE_CONFIG\"\n    )\n\n    # Setup config schema\n    self.config_schema = Config\n</code></pre>"},{"location":"api_reference/config_loader/#aineko.ConfigLoader.load_config","title":"<code>load_config()</code>","text":"<p>Load and validate the pipeline config.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the config does not match the schema</p> <p>Returns:</p> Type Description <code>dict</code> <p>The pipeline config as a dictionary</p> Source code in <code>aineko/core/config_loader.py</code> <pre><code>def load_config(self) -&gt; dict:\n    \"\"\"Load and validate the pipeline config.\n\n    Raises:\n        ValidationError: If the config does not match the schema\n\n    Returns:\n        The pipeline config as a dictionary\n    \"\"\"\n    config = load_yaml(self.pipeline_config_file)\n\n    try:\n        Config(**config)\n    except ValidationError as e:\n        logger.error(\n            \"Schema validation failed for pipeline `%s` loaded from %s. \"\n            \"See detailed error below.\",\n            config[\"pipeline\"][\"name\"],\n            self.pipeline_config_file,\n        )\n        raise e\n\n    return config\n</code></pre>"},{"location":"api_reference/dataset_consumer/","title":"<code>DatasetConsumer</code>","text":"<p>The <code>DatasetConsumer</code> class is used to consume from <code>Kafka</code> topics.</p>"},{"location":"api_reference/dataset_consumer/#aineko.DatasetConsumer","title":"<code>aineko.DatasetConsumer</code>","text":"<p>Wrapper class for Kafka consumer object.</p> <p>DatasetConsumer objects are designed to consume messages from a single dataset and will consume the next unconsumed message in the queue.</p> <p>When accessing kafka topics, prefixes will automatically be added to the dataset name as part of namespacing. For datasets defined in the pipeline config, <code>has_pipeline_prefix</code> will be set to <code>True</code>, so a dataset named <code>my_dataset</code> will point to a topic named <code>my_pipeline.my_dataset</code>.</p> <p>Optionally, a custom prefix can be provided that will apply to all datasets. In the above example, if the prefix is set to <code>test</code>, the topic name will be <code>test.my_pipeline.my_dataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset</p> required <code>node_name</code> <code>str</code> <p>name of the node that is consuming the dataset</p> required <code>pipeline_name</code> <code>str</code> <p>name of the pipeline</p> required <code>dataset_config</code> <code>Dict[str, Any]</code> <p>dataset config</p> required <code>bootstrap_servers</code> <code>Optional[str]</code> <p>bootstrap_servers to connect to (e.g. \"1.2.3.4:9092\")</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>prefix for topic name (.) <code>None</code> <code>has_pipeline_prefix</code> <code>bool</code> <p>whether the dataset name has pipeline name prefix</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>consumer</code> <p>Kafka consumer object</p> <code>cached</code> <p>if the high watermark offset has been cached (updated when message consumed)</p> <p>Methods:</p> Name Description <code>consume</code> <p>reads a message from the dataset</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>class DatasetConsumer:\n    \"\"\"Wrapper class for Kafka consumer object.\n\n    DatasetConsumer objects are designed to consume messages from a single\n    dataset and will consume the next unconsumed message in the queue.\n\n    When accessing kafka topics, prefixes will automatically be added to the\n    dataset name as part of namespacing. For datasets defined in the pipeline\n    config, `has_pipeline_prefix` will be set to `True`, so a dataset named\n    `my_dataset` will point to a topic named `my_pipeline.my_dataset`.\n\n    Optionally, a custom prefix can be provided that will apply to all datasets.\n    In the above example, if the prefix is set to `test`, the topic name will\n    be `test.my_pipeline.my_dataset`.\n\n    Args:\n        dataset_name: name of the dataset\n        node_name: name of the node that is consuming the dataset\n        pipeline_name: name of the pipeline\n        dataset_config: dataset config\n        bootstrap_servers: bootstrap_servers to connect to (e.g. \"1.2.3.4:9092\")\n        prefix: prefix for topic name (&lt;prefix&gt;.&lt;dataset_name&gt;)\n        has_pipeline_prefix: whether the dataset name has pipeline name prefix\n\n    Attributes:\n        consumer: Kafka consumer object\n        cached: if the high watermark offset has been cached\n            (updated when message consumed)\n\n    Methods:\n        consume: reads a message from the dataset\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_name: str,\n        node_name: str,\n        pipeline_name: str,\n        dataset_config: Dict[str, Any],\n        bootstrap_servers: Optional[str] = None,\n        prefix: Optional[str] = None,\n        has_pipeline_prefix: bool = False,\n    ):\n        \"\"\"Initialize the consumer.\"\"\"\n        self.pipeline_name = pipeline_name\n        self.kafka_config = DEFAULT_KAFKA_CONFIG\n        self.prefix = prefix\n        self.has_pipeline_prefix = has_pipeline_prefix\n        self.cached = False\n\n        consumer_config = self.kafka_config.get(\"CONSUMER_CONFIG\")\n        # Overwrite bootstrap server with broker if provided\n        if bootstrap_servers:\n            consumer_config[\"bootstrap.servers\"] = bootstrap_servers\n\n        # Override default config with dataset specific config\n        for param, value in dataset_config.get(\"params\", {}).items():\n            consumer_config[param] = value\n\n        topic_name = dataset_name\n        if has_pipeline_prefix:\n            topic_name = f\"{pipeline_name}.{dataset_name}\"\n\n        if self.prefix:\n            consumer_config[\n                \"group.id\"\n            ] = f\"{prefix}.{pipeline_name}.{node_name}\"\n            self.consumer = Consumer(consumer_config)\n            self.consumer.subscribe([f\"{prefix}.{topic_name}\"])\n\n        else:\n            consumer_config[\"group.id\"] = f\"{pipeline_name}.{node_name}\"\n            self.consumer = Consumer(consumer_config)\n            self.consumer.subscribe([topic_name])\n\n        self.topic_name = topic_name\n\n    @staticmethod\n    def _validate_message(\n        message: Optional[Message] = None,\n    ) -&gt; Optional[dict]:\n        \"\"\"Checks if a message is valid and converts it to appropriate format.\n\n        Args:\n            message: message to check\n\n        Returns:\n            message if valid, None if not\n        \"\"\"\n        # Check if message is valid\n        if message is None or message.value() is None:\n            return None\n\n        # Check if message is an error\n        if message.error():\n            logger.error(str(message.error()))\n            return None\n\n        # Convert message to dict\n        message = message.value()\n        if isinstance(message, bytes):\n            message = message.decode(\"utf-8\")\n        return json.loads(message)\n\n    def _update_offset_to_latest(self) -&gt; None:\n        \"\"\"Updates offset to latest.\n\n        Note that the initial call, for this method might take\n        a while due to consumer initialization.\n        \"\"\"\n        partitions = self.consumer.assignment()\n        # Initialize consumers if not already initialized by polling\n        while not partitions:\n            self.consumer.poll(timeout=0)\n            partitions = self.consumer.assignment()\n\n        for partition in partitions:\n            partition.offset = (\n                self.consumer.get_watermark_offsets(\n                    partition, cached=self.cached\n                )[1]\n                - 1\n            )\n\n        self.consumer.assign(partitions)\n\n    def consume(\n        self,\n        how: Literal[\"next\", \"last\"] = \"next\",\n        timeout: Optional[float] = None,\n    ) -&gt; Optional[dict]:\n        \"\"\"Polls a message from the dataset.\n\n        Args:\n            how: how to read the message.\n                \"next\": read the next message in the queue\n                \"last\": read the last message in the queue\n            timeout: seconds to poll for a response from kafka broker.\n                If using how=\"last\", set to bigger than 0.\n\n        Returns:\n            message from the dataset\n\n        Raises:\n            ValueError: if how is not \"next\" or \"last\"\n        \"\"\"\n        if how not in [\"next\", \"last\"]:\n            raise ValueError(f\"Invalid how: {how}. Expected `next` or `last`.\")\n\n        timeout = timeout or self.kafka_config.get(\"CONSUMER_TIMEOUT\")\n        if how == \"next\":\n            # next unread message from queue\n            message = self.consumer.poll(timeout=timeout)\n\n        if how == \"last\":\n            # last message from queue\n            self._update_offset_to_latest()\n            message = self.consumer.poll(timeout=timeout)\n\n        self.cached = True\n\n        return self._validate_message(message)\n\n    def _consume_message(\n        self, how: Literal[\"next\", \"last\"], timeout: Optional[float] = None\n    ) -&gt; dict:\n        \"\"\"Calls the consume method and blocks until a message is returned.\n\n        Args:\n            how: See `consume` method for available options.\n\n        Returns:\n            message from dataset\n        \"\"\"\n        while True:\n            try:\n                message = self.consume(how=how, timeout=timeout)\n                if message is not None:\n                    return message\n            except KafkaError as e:\n                if e.code() == \"_MAX_POLL_EXCEEDED\":\n                    continue\n                raise e\n\n    def next(self) -&gt; dict:\n        \"\"\"Consumes the next message from the dataset.\n\n        Wraps the `consume(how=\"next\")` method. It implements a\n        block that waits until a message is received before returning it.\n        This method ensures that every message is consumed, but the consumed\n        message may not be the most recent message if the consumer is slower\n        than the producer.\n\n        This is useful when the timeout is short and you expect the consumer\n        to often return `None`.\n\n        Returns:\n            message from the dataset\n        \"\"\"\n        return self._consume_message(how=\"next\")\n\n    def last(self, timeout: int = 1) -&gt; dict:\n        \"\"\"Consumes the last message from the dataset.\n\n        Wraps the `consume(how=\"last\")` method. It implements a\n        block that waits until a message is received before returning it.\n        This method ensures that the consumed message is always the most\n        recent message. If the consumer is slower than the producer, messages\n        might be skipped. If the consumer is faster than the producer,\n        messages might be repeated.\n\n        This is useful when the timeout is short and you expect the consumer\n        to often return `None`.\n\n        Note: The timeout must be greater than 0 to prevent\n        overwhelming the broker with requests to update the offset.\n\n        Args:\n            timeout: seconds to poll for a response from kafka broker.\n                Must be &gt;0.\n\n        Returns:\n            message from the dataset\n\n        Raises:\n            ValueError: if timeout is &lt;= 0\n        \"\"\"\n        if timeout &lt;= 0:\n            raise ValueError(\n                \"Timeout must be &gt; 0 when consuming the last message.\"\n            )\n        return self._consume_message(how=\"last\", timeout=timeout)\n\n    def consume_all(self, end_message: str | bool = False) -&gt; list:\n        \"\"\"Reads all messages from the dataset until a specific one is found.\n\n        Args:\n            end_message: Message to trigger the completion of consumption\n\n        Returns:\n            list of messages from the dataset\n        \"\"\"\n        messages = []\n        while True:\n            message = self.consume()\n            if message is None:\n                continue\n            if message[\"message\"] == end_message:\n                break\n            messages.append(message)\n        return messages\n</code></pre>"},{"location":"api_reference/dataset_consumer/#aineko.DatasetConsumer.__init__","title":"<code>__init__(dataset_name, node_name, pipeline_name, dataset_config, bootstrap_servers=None, prefix=None, has_pipeline_prefix=False)</code>","text":"<p>Initialize the consumer.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_name: str,\n    node_name: str,\n    pipeline_name: str,\n    dataset_config: Dict[str, Any],\n    bootstrap_servers: Optional[str] = None,\n    prefix: Optional[str] = None,\n    has_pipeline_prefix: bool = False,\n):\n    \"\"\"Initialize the consumer.\"\"\"\n    self.pipeline_name = pipeline_name\n    self.kafka_config = DEFAULT_KAFKA_CONFIG\n    self.prefix = prefix\n    self.has_pipeline_prefix = has_pipeline_prefix\n    self.cached = False\n\n    consumer_config = self.kafka_config.get(\"CONSUMER_CONFIG\")\n    # Overwrite bootstrap server with broker if provided\n    if bootstrap_servers:\n        consumer_config[\"bootstrap.servers\"] = bootstrap_servers\n\n    # Override default config with dataset specific config\n    for param, value in dataset_config.get(\"params\", {}).items():\n        consumer_config[param] = value\n\n    topic_name = dataset_name\n    if has_pipeline_prefix:\n        topic_name = f\"{pipeline_name}.{dataset_name}\"\n\n    if self.prefix:\n        consumer_config[\n            \"group.id\"\n        ] = f\"{prefix}.{pipeline_name}.{node_name}\"\n        self.consumer = Consumer(consumer_config)\n        self.consumer.subscribe([f\"{prefix}.{topic_name}\"])\n\n    else:\n        consumer_config[\"group.id\"] = f\"{pipeline_name}.{node_name}\"\n        self.consumer = Consumer(consumer_config)\n        self.consumer.subscribe([topic_name])\n\n    self.topic_name = topic_name\n</code></pre>"},{"location":"api_reference/dataset_consumer/#aineko.DatasetConsumer.consume","title":"<code>consume(how='next', timeout=None)</code>","text":"<p>Polls a message from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>how</code> <code>Literal['next', 'last']</code> <p>how to read the message. \"next\": read the next message in the queue \"last\": read the last message in the queue</p> <code>'next'</code> <code>timeout</code> <code>Optional[float]</code> <p>seconds to poll for a response from kafka broker. If using how=\"last\", set to bigger than 0.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[dict]</code> <p>message from the dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if how is not \"next\" or \"last\"</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>def consume(\n    self,\n    how: Literal[\"next\", \"last\"] = \"next\",\n    timeout: Optional[float] = None,\n) -&gt; Optional[dict]:\n    \"\"\"Polls a message from the dataset.\n\n    Args:\n        how: how to read the message.\n            \"next\": read the next message in the queue\n            \"last\": read the last message in the queue\n        timeout: seconds to poll for a response from kafka broker.\n            If using how=\"last\", set to bigger than 0.\n\n    Returns:\n        message from the dataset\n\n    Raises:\n        ValueError: if how is not \"next\" or \"last\"\n    \"\"\"\n    if how not in [\"next\", \"last\"]:\n        raise ValueError(f\"Invalid how: {how}. Expected `next` or `last`.\")\n\n    timeout = timeout or self.kafka_config.get(\"CONSUMER_TIMEOUT\")\n    if how == \"next\":\n        # next unread message from queue\n        message = self.consumer.poll(timeout=timeout)\n\n    if how == \"last\":\n        # last message from queue\n        self._update_offset_to_latest()\n        message = self.consumer.poll(timeout=timeout)\n\n    self.cached = True\n\n    return self._validate_message(message)\n</code></pre>"},{"location":"api_reference/dataset_consumer/#aineko.DatasetConsumer.consume_all","title":"<code>consume_all(end_message=False)</code>","text":"<p>Reads all messages from the dataset until a specific one is found.</p> <p>Parameters:</p> Name Type Description Default <code>end_message</code> <code>str | bool</code> <p>Message to trigger the completion of consumption</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>list of messages from the dataset</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>def consume_all(self, end_message: str | bool = False) -&gt; list:\n    \"\"\"Reads all messages from the dataset until a specific one is found.\n\n    Args:\n        end_message: Message to trigger the completion of consumption\n\n    Returns:\n        list of messages from the dataset\n    \"\"\"\n    messages = []\n    while True:\n        message = self.consume()\n        if message is None:\n            continue\n        if message[\"message\"] == end_message:\n            break\n        messages.append(message)\n    return messages\n</code></pre>"},{"location":"api_reference/dataset_consumer/#aineko.DatasetConsumer.last","title":"<code>last(timeout=1)</code>","text":"<p>Consumes the last message from the dataset.</p> <p>Wraps the <code>consume(how=\"last\")</code> method. It implements a block that waits until a message is received before returning it. This method ensures that the consumed message is always the most recent message. If the consumer is slower than the producer, messages might be skipped. If the consumer is faster than the producer, messages might be repeated.</p> <p>This is useful when the timeout is short and you expect the consumer to often return <code>None</code>.</p> <p>Note: The timeout must be greater than 0 to prevent overwhelming the broker with requests to update the offset.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>seconds to poll for a response from kafka broker. Must be &gt;0.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict</code> <p>message from the dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if timeout is &lt;= 0</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>def last(self, timeout: int = 1) -&gt; dict:\n    \"\"\"Consumes the last message from the dataset.\n\n    Wraps the `consume(how=\"last\")` method. It implements a\n    block that waits until a message is received before returning it.\n    This method ensures that the consumed message is always the most\n    recent message. If the consumer is slower than the producer, messages\n    might be skipped. If the consumer is faster than the producer,\n    messages might be repeated.\n\n    This is useful when the timeout is short and you expect the consumer\n    to often return `None`.\n\n    Note: The timeout must be greater than 0 to prevent\n    overwhelming the broker with requests to update the offset.\n\n    Args:\n        timeout: seconds to poll for a response from kafka broker.\n            Must be &gt;0.\n\n    Returns:\n        message from the dataset\n\n    Raises:\n        ValueError: if timeout is &lt;= 0\n    \"\"\"\n    if timeout &lt;= 0:\n        raise ValueError(\n            \"Timeout must be &gt; 0 when consuming the last message.\"\n        )\n    return self._consume_message(how=\"last\", timeout=timeout)\n</code></pre>"},{"location":"api_reference/dataset_consumer/#aineko.DatasetConsumer.next","title":"<code>next()</code>","text":"<p>Consumes the next message from the dataset.</p> <p>Wraps the <code>consume(how=\"next\")</code> method. It implements a block that waits until a message is received before returning it. This method ensures that every message is consumed, but the consumed message may not be the most recent message if the consumer is slower than the producer.</p> <p>This is useful when the timeout is short and you expect the consumer to often return <code>None</code>.</p> <p>Returns:</p> Type Description <code>dict</code> <p>message from the dataset</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>def next(self) -&gt; dict:\n    \"\"\"Consumes the next message from the dataset.\n\n    Wraps the `consume(how=\"next\")` method. It implements a\n    block that waits until a message is received before returning it.\n    This method ensures that every message is consumed, but the consumed\n    message may not be the most recent message if the consumer is slower\n    than the producer.\n\n    This is useful when the timeout is short and you expect the consumer\n    to often return `None`.\n\n    Returns:\n        message from the dataset\n    \"\"\"\n    return self._consume_message(how=\"next\")\n</code></pre>"},{"location":"api_reference/dataset_producer/","title":"<code>DatasetProducer</code>","text":"<p>The <code>DatasetProducer</code> class is used to produce to <code>kafka</code> topics.</p>"},{"location":"api_reference/dataset_producer/#aineko.DatasetProducer","title":"<code>aineko.DatasetProducer</code>","text":"<p>Wrapper class for Kafka producer object.</p> <p>See DatasetConsumer for prefix rules.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>dataset name</p> required <code>node_name</code> <code>str</code> <p>name of the node that is producing the message</p> required <code>pipeline_name</code> <code>str</code> <p>name of the pipeline</p> required <code>dataset_config</code> <code>Dict[str, Any]</code> <p>dataset config</p> required <code>prefix</code> <code>Optional[str]</code> <p>prefix for topic name (.) <code>None</code> <code>has_pipeline_prefix</code> <code>bool</code> <p>whether the dataset name has pipeline name prefix</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>producer</code> <p>Kafka producer object</p> <p>Methods:</p> Name Description <code>produce</code> <p>produce a message to the dataset</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>class DatasetProducer:\n    \"\"\"Wrapper class for Kafka producer object.\n\n    See DatasetConsumer for prefix rules.\n\n    Args:\n        dataset_name: dataset name\n        node_name: name of the node that is producing the message\n        pipeline_name: name of the pipeline\n        dataset_config: dataset config\n        prefix: prefix for topic name (&lt;prefix&gt;.&lt;dataset_name&gt;)\n        has_pipeline_prefix: whether the dataset name has pipeline name prefix\n\n    Attributes:\n        producer: Kafka producer object\n\n    Methods:\n        produce: produce a message to the dataset\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_name: str,\n        node_name: str,\n        pipeline_name: str,\n        dataset_config: Dict[str, Any],\n        prefix: Optional[str] = None,\n        has_pipeline_prefix: bool = False,\n    ):\n        \"\"\"Initialize the producer.\"\"\"\n        self.source_pipeline = pipeline_name\n        self.dataset = dataset_name\n        self.source_node = node_name\n        self.prefix = prefix\n        self.has_pipeline_prefix = has_pipeline_prefix\n\n        # Create topic name based on prefix rules\n        topic_name = dataset_name\n        if has_pipeline_prefix:\n            topic_name = f\"{pipeline_name}.{topic_name}\"\n        if prefix:\n            topic_name = f\"{prefix}.{topic_name}\"\n        self.topic_name = topic_name\n\n        # Assign kafka config\n        self.kafka_config = DEFAULT_KAFKA_CONFIG\n\n        # Set producer parameters\n        producer_config = self.kafka_config.get(\"PRODUCER_CONFIG\")\n\n        # Override default config with dataset specific config\n        if \"params\" in dataset_config:\n            for param in self.kafka_config.get(\"PRODUCER_OVERRIDABLES\"):\n                if param in dataset_config[\"params\"]:\n                    producer_config[param] = dataset_config[\"params\"][param]\n\n        # Create producer\n        self.producer = Producer(producer_config)\n\n    @staticmethod\n    def _delivery_report(err: Any, message: Message) -&gt; None:\n        \"\"\"Called once for each message produced to indicate delivery result.\n\n        Triggered by poll() or flush().\n\n        Args:\n            err: error message\n            message: message object from Kafka\n        \"\"\"\n        if err is not None:\n            logger.error(\"Message %s delivery failed: %s\", message, err)\n\n    def produce(self, message: dict, key: Optional[str] = None) -&gt; None:\n        \"\"\"Produce a message to the dataset.\n\n        Args:\n            message: message to produce to the dataset\n            key: key to use for the message\n        \"\"\"\n        message = {\n            \"timestamp\": datetime.datetime.now().strftime(\n                AINEKO_CONFIG.get(\"MSG_TIMESTAMP_FORMAT\")\n            ),\n            \"dataset\": self.dataset,\n            \"source_pipeline\": self.source_pipeline,\n            \"source_node\": self.source_node,\n            \"message\": message,\n        }\n        self.producer.poll(0)\n\n        key_bytes = str(key).encode(\"utf-8\") if key is not None else None\n\n        self.producer.produce(\n            topic=self.topic_name,\n            key=key_bytes,\n            value=json.dumps(message).encode(\"utf-8\"),\n            callback=self._delivery_report,\n        )\n        self.producer.flush()\n</code></pre>"},{"location":"api_reference/dataset_producer/#aineko.DatasetProducer.__init__","title":"<code>__init__(dataset_name, node_name, pipeline_name, dataset_config, prefix=None, has_pipeline_prefix=False)</code>","text":"<p>Initialize the producer.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_name: str,\n    node_name: str,\n    pipeline_name: str,\n    dataset_config: Dict[str, Any],\n    prefix: Optional[str] = None,\n    has_pipeline_prefix: bool = False,\n):\n    \"\"\"Initialize the producer.\"\"\"\n    self.source_pipeline = pipeline_name\n    self.dataset = dataset_name\n    self.source_node = node_name\n    self.prefix = prefix\n    self.has_pipeline_prefix = has_pipeline_prefix\n\n    # Create topic name based on prefix rules\n    topic_name = dataset_name\n    if has_pipeline_prefix:\n        topic_name = f\"{pipeline_name}.{topic_name}\"\n    if prefix:\n        topic_name = f\"{prefix}.{topic_name}\"\n    self.topic_name = topic_name\n\n    # Assign kafka config\n    self.kafka_config = DEFAULT_KAFKA_CONFIG\n\n    # Set producer parameters\n    producer_config = self.kafka_config.get(\"PRODUCER_CONFIG\")\n\n    # Override default config with dataset specific config\n    if \"params\" in dataset_config:\n        for param in self.kafka_config.get(\"PRODUCER_OVERRIDABLES\"):\n            if param in dataset_config[\"params\"]:\n                producer_config[param] = dataset_config[\"params\"][param]\n\n    # Create producer\n    self.producer = Producer(producer_config)\n</code></pre>"},{"location":"api_reference/dataset_producer/#aineko.DatasetProducer.produce","title":"<code>produce(message, key=None)</code>","text":"<p>Produce a message to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>dict</code> <p>message to produce to the dataset</p> required <code>key</code> <code>Optional[str]</code> <p>key to use for the message</p> <code>None</code> Source code in <code>aineko/core/dataset.py</code> <pre><code>def produce(self, message: dict, key: Optional[str] = None) -&gt; None:\n    \"\"\"Produce a message to the dataset.\n\n    Args:\n        message: message to produce to the dataset\n        key: key to use for the message\n    \"\"\"\n    message = {\n        \"timestamp\": datetime.datetime.now().strftime(\n            AINEKO_CONFIG.get(\"MSG_TIMESTAMP_FORMAT\")\n        ),\n        \"dataset\": self.dataset,\n        \"source_pipeline\": self.source_pipeline,\n        \"source_node\": self.source_node,\n        \"message\": message,\n    }\n    self.producer.poll(0)\n\n    key_bytes = str(key).encode(\"utf-8\") if key is not None else None\n\n    self.producer.produce(\n        topic=self.topic_name,\n        key=key_bytes,\n        value=json.dumps(message).encode(\"utf-8\"),\n        callback=self._delivery_report,\n    )\n    self.producer.flush()\n</code></pre>"},{"location":"api_reference/runner/","title":"<code>Runner</code>","text":"<p>The <code>Runner</code> builds the pipeline from the given pipeline configuration file, creating the necessary datasets and orchestrating the nodes.</p>"},{"location":"api_reference/runner/#aineko.Runner","title":"<code>aineko.Runner</code>","text":"<p>Runs the pipeline described in the config.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_config_file</code> <code>str</code> <p>Path to pipeline config file</p> required <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline</p> <code>None</code> <code>kafka_config</code> <code>dict</code> <p>Config for kafka broker</p> <code>get('BROKER_CONFIG')</code> <code>dataset_prefix</code> <code>Optional[str]</code> <p>Prefix for dataset names.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>pipeline_config_file</code> <code>str</code> <p>Path to pipeline config file</p> <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline, overrides pipeline config</p> <code>kafka_config</code> <code>dict</code> <p>Config for kafka broker</p> <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline, loaded from config</p> <code>dataset_prefix</code> <code>Optional[str]</code> <p>Prefix for dataset names</p> Source code in <code>aineko/core/runner.py</code> <pre><code>class Runner:\n    \"\"\"Runs the pipeline described in the config.\n\n    Args:\n        pipeline_config_file (str): Path to pipeline config file\n        pipeline_name (str): Name of the pipeline\n        kafka_config (dict): Config for kafka broker\n        dataset_prefix (Optional[str]): Prefix for dataset names.\n        Kafka topics will be called &lt;prefix&gt;.&lt;pipeline&gt;.&lt;dataset_name&gt;.\n\n    Attributes:\n        pipeline_config_file (str): Path to pipeline config file\n        pipeline_name (str): Name of the pipeline, overrides pipeline config\n        kafka_config (dict): Config for kafka broker\n        pipeline_name (str): Name of the pipeline, loaded from config\n        dataset_prefix (Optional[str]): Prefix for dataset names\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline_config_file: str,\n        pipeline_name: Optional[str] = None,\n        kafka_config: dict = DEFAULT_KAFKA_CONFIG.get(\"BROKER_CONFIG\"),\n        metrics_export_port: int = AINEKO_CONFIG.get(\"RAY_METRICS_PORT\"),\n        dataset_prefix: Optional[str] = None,\n    ):\n        \"\"\"Initializes the runner class.\"\"\"\n        self.pipeline_config_file = pipeline_config_file\n        self.kafka_config = kafka_config\n        self.metrics_export_port = metrics_export_port\n        self.pipeline_name = pipeline_name\n        self.dataset_prefix = dataset_prefix or \"\"\n\n    def run(self) -&gt; None:\n        \"\"\"Runs the pipeline.\n\n        Step 1: Load config for pipeline\n\n        Step 2: Set up datasets\n\n        Step 3: Set up PoisonPill node that is available to all nodes\n\n        Step 4: Set up nodes (including Node Manager)\n        \"\"\"\n        # Load pipeline config\n        pipeline_config = self.load_pipeline_config()\n        self.pipeline_name = self.pipeline_name or pipeline_config[\"name\"]\n\n        # Create the necessary datasets\n        self.prepare_datasets(\n            config=pipeline_config[\"datasets\"],\n            user_dataset_prefix=self.pipeline_name,\n        )\n\n        # Initialize ray cluster\n        ray.shutdown()\n        ray.init(\n            namespace=self.pipeline_name,\n            _metrics_export_port=self.metrics_export_port,\n        )\n\n        # Create poison pill actor\n        poison_pill = ray.remote(PoisonPill).remote()\n\n        # Add Node Manager to pipeline config\n        pipeline_config[\"nodes\"][\n            NODE_MANAGER_CONFIG.get(\"NAME\")\n        ] = NODE_MANAGER_CONFIG.get(\"NODE_CONFIG\")\n\n        # Create each node (actor)\n        results = self.prepare_nodes(\n            pipeline_config=pipeline_config,\n            poison_pill=poison_pill,  # type: ignore\n        )\n\n        ray.get(results)\n\n    def load_pipeline_config(self) -&gt; dict:\n        \"\"\"Loads the config for a given pipeline.\n\n        Returns:\n            pipeline config\n        \"\"\"\n        config = ConfigLoader(\n            pipeline_config_file=self.pipeline_config_file,\n        ).load_config()\n\n        return config[\"pipeline\"]\n\n    def prepare_datasets(\n        self, config: dict, user_dataset_prefix: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Creates the required datasets for a given pipeline.\n\n        Datasets can be configured using the `params` key, using config keys\n        found in: https://kafka.apache.org/documentation.html#topicconfigs\n\n        Args:\n            config: dataset configuration found in pipeline config\n            Should follow the schema\n                {\n                    \"dataset_name\": {\n                        \"type\": str (\"kafka_stream\"),\n                        \"params\": dict\n                }\n            user_dataset_prefix: prefix only for datasets defined by the user.\n            i.e. &lt;prefix&gt;.&lt;user_dataset_prefix&gt;.&lt;dataset_name&gt;\n\n        Returns:\n            True if successful\n\n        Raises:\n            ValueError: if dataset \"logging\" is defined in the catalog\n        \"\"\"\n        # Connect to kafka cluster\n        kafka_client = AdminClient(self.kafka_config)\n\n        # Add prefix to user defined datasets\n        if user_dataset_prefix:\n            config = {\n                f\"{user_dataset_prefix}.{dataset_name}\": dataset_config\n                for dataset_name, dataset_config in config.items()\n            }\n\n        # Fail if reserved dataset names are defined in catalog\n        for reserved_dataset in DEFAULT_KAFKA_CONFIG.get(\"DATASETS\"):\n            if reserved_dataset in config:\n                raise ValueError(\n                    f\"Unable to create dataset `{reserved_dataset}`. \"\n                    \"Reserved for internal use.\"\n                )\n\n        # Add logging dataset to catalog\n        config[DEFAULT_KAFKA_CONFIG.get(\"LOGGING_DATASET\")] = {\n            \"type\": AINEKO_CONFIG.get(\"KAFKA_STREAM_TYPE\"),\n            \"params\": DEFAULT_KAFKA_CONFIG.get(\"DATASET_PARAMS\"),\n        }\n\n        # Create all dataset defined in the catalog\n        dataset_list = []\n        for dataset_name, dataset_config in config.items():\n            logger.info(\n                \"Creating dataset: %s: %s\", dataset_name, dataset_config\n            )\n            # Create dataset for kafka streams\n            if dataset_config[\"type\"] == AINEKO_CONFIG.get(\"KAFKA_STREAM_TYPE\"):\n                # User defined\n                dataset_params = {\n                    **DEFAULT_KAFKA_CONFIG.get(\"DATASET_PARAMS\"),\n                    **dataset_config.get(\"params\", {}),\n                }\n\n                # Configure dataset\n                if self.dataset_prefix:\n                    topic_name = f\"{self.dataset_prefix}.{dataset_name}\"\n                else:\n                    topic_name = dataset_name\n\n                new_dataset = NewTopic(\n                    topic=topic_name,\n                    num_partitions=dataset_params.get(\"num_partitions\"),\n                    replication_factor=dataset_params.get(\"replication_factor\"),\n                    config=dataset_params.get(\"config\"),\n                )\n\n                # Add dataset to appropriate list\n                dataset_list.append(new_dataset)\n\n            else:\n                raise ValueError(\n                    \"Unknown dataset type. Expected: \"\n                    f\"{AINEKO_CONFIG.get('STREAM_TYPES')}.\"\n                )\n\n        # Create all configured datasets\n        datasets = kafka_client.create_topics(dataset_list)\n\n        # Block until all datasets finish creation\n        cur_time = time.time()\n        while True:\n            if all(future.done() for future in datasets.values()):\n                logger.info(\"All datasets created.\")\n                break\n            if time.time() - cur_time &gt; AINEKO_CONFIG.get(\n                \"DATASET_CREATION_TIMEOUT\"\n            ):\n                raise TimeoutError(\n                    \"Timeout while creating Kafka datasets. \"\n                    \"Please check your Kafka cluster.\"\n                )\n\n        return datasets\n\n    def prepare_nodes(\n        self, pipeline_config: dict, poison_pill: ray.actor.ActorHandle\n    ) -&gt; list:\n        \"\"\"Prepare actor handles for all nodes.\n\n        Args:\n            pipeline_config: pipeline configuration\n\n        Returns:\n            dict: mapping of node names to actor handles\n            list: list of ray objects\n        \"\"\"\n        # Collect all  actor futures\n        results = []\n\n        default_node_config = pipeline_config.get(\"default_node_settings\", {})\n\n        for node_name, node_config in pipeline_config[\"nodes\"].items():\n            # Initialize actor from specified class in config\n            target_class = imports.import_from_string(\n                attr=node_config[\"class\"], kind=\"class\"\n            )\n            actor_params = {\n                **default_node_config,\n                **node_config.get(\"node_settings\", {}),\n                \"name\": node_name,\n                \"namespace\": self.pipeline_name,\n            }\n\n            wrapped_class = ray.remote(target_class)\n            wrapped_class.options(**actor_params)\n            actor_handle = wrapped_class.remote(\n                node_name=node_name,\n                pipeline_name=self.pipeline_name,\n                poison_pill=poison_pill,\n            )\n\n            # Setup input and output datasets\n            outputs = node_config.get(\"outputs\", [])\n            actor_handle.setup_datasets.remote(\n                inputs=node_config.get(\"inputs\", None),\n                outputs=outputs,\n                datasets=pipeline_config[\"datasets\"],\n                has_pipeline_prefix=True,\n            )\n\n            # Setup internal datasets like logging, without pipeline prefix\n            actor_handle.setup_datasets.remote(\n                outputs=DEFAULT_KAFKA_CONFIG.get(\"DATASETS\"),\n                datasets=pipeline_config[\"datasets\"],\n            )\n\n            # Create actor future (for execute method)\n            results.append(\n                actor_handle.execute.remote(\n                    params=node_config.get(\"node_params\", None)\n                )\n            )\n\n        return results\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.__init__","title":"<code>__init__(pipeline_config_file, pipeline_name=None, kafka_config=DEFAULT_KAFKA_CONFIG.get('BROKER_CONFIG'), metrics_export_port=AINEKO_CONFIG.get('RAY_METRICS_PORT'), dataset_prefix=None)</code>","text":"<p>Initializes the runner class.</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def __init__(\n    self,\n    pipeline_config_file: str,\n    pipeline_name: Optional[str] = None,\n    kafka_config: dict = DEFAULT_KAFKA_CONFIG.get(\"BROKER_CONFIG\"),\n    metrics_export_port: int = AINEKO_CONFIG.get(\"RAY_METRICS_PORT\"),\n    dataset_prefix: Optional[str] = None,\n):\n    \"\"\"Initializes the runner class.\"\"\"\n    self.pipeline_config_file = pipeline_config_file\n    self.kafka_config = kafka_config\n    self.metrics_export_port = metrics_export_port\n    self.pipeline_name = pipeline_name\n    self.dataset_prefix = dataset_prefix or \"\"\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.load_pipeline_config","title":"<code>load_pipeline_config()</code>","text":"<p>Loads the config for a given pipeline.</p> <p>Returns:</p> Type Description <code>dict</code> <p>pipeline config</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def load_pipeline_config(self) -&gt; dict:\n    \"\"\"Loads the config for a given pipeline.\n\n    Returns:\n        pipeline config\n    \"\"\"\n    config = ConfigLoader(\n        pipeline_config_file=self.pipeline_config_file,\n    ).load_config()\n\n    return config[\"pipeline\"]\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.prepare_datasets","title":"<code>prepare_datasets(config, user_dataset_prefix=None)</code>","text":"<p>Creates the required datasets for a given pipeline.</p> <p>Datasets can be configured using the <code>params</code> key, using config keys found in: https://kafka.apache.org/documentation.html#topicconfigs</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>dataset configuration found in pipeline config</p> required <code>user_dataset_prefix</code> <code>Optional[str]</code> <p>prefix only for datasets defined by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if dataset \"logging\" is defined in the catalog</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def prepare_datasets(\n    self, config: dict, user_dataset_prefix: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Creates the required datasets for a given pipeline.\n\n    Datasets can be configured using the `params` key, using config keys\n    found in: https://kafka.apache.org/documentation.html#topicconfigs\n\n    Args:\n        config: dataset configuration found in pipeline config\n        Should follow the schema\n            {\n                \"dataset_name\": {\n                    \"type\": str (\"kafka_stream\"),\n                    \"params\": dict\n            }\n        user_dataset_prefix: prefix only for datasets defined by the user.\n        i.e. &lt;prefix&gt;.&lt;user_dataset_prefix&gt;.&lt;dataset_name&gt;\n\n    Returns:\n        True if successful\n\n    Raises:\n        ValueError: if dataset \"logging\" is defined in the catalog\n    \"\"\"\n    # Connect to kafka cluster\n    kafka_client = AdminClient(self.kafka_config)\n\n    # Add prefix to user defined datasets\n    if user_dataset_prefix:\n        config = {\n            f\"{user_dataset_prefix}.{dataset_name}\": dataset_config\n            for dataset_name, dataset_config in config.items()\n        }\n\n    # Fail if reserved dataset names are defined in catalog\n    for reserved_dataset in DEFAULT_KAFKA_CONFIG.get(\"DATASETS\"):\n        if reserved_dataset in config:\n            raise ValueError(\n                f\"Unable to create dataset `{reserved_dataset}`. \"\n                \"Reserved for internal use.\"\n            )\n\n    # Add logging dataset to catalog\n    config[DEFAULT_KAFKA_CONFIG.get(\"LOGGING_DATASET\")] = {\n        \"type\": AINEKO_CONFIG.get(\"KAFKA_STREAM_TYPE\"),\n        \"params\": DEFAULT_KAFKA_CONFIG.get(\"DATASET_PARAMS\"),\n    }\n\n    # Create all dataset defined in the catalog\n    dataset_list = []\n    for dataset_name, dataset_config in config.items():\n        logger.info(\n            \"Creating dataset: %s: %s\", dataset_name, dataset_config\n        )\n        # Create dataset for kafka streams\n        if dataset_config[\"type\"] == AINEKO_CONFIG.get(\"KAFKA_STREAM_TYPE\"):\n            # User defined\n            dataset_params = {\n                **DEFAULT_KAFKA_CONFIG.get(\"DATASET_PARAMS\"),\n                **dataset_config.get(\"params\", {}),\n            }\n\n            # Configure dataset\n            if self.dataset_prefix:\n                topic_name = f\"{self.dataset_prefix}.{dataset_name}\"\n            else:\n                topic_name = dataset_name\n\n            new_dataset = NewTopic(\n                topic=topic_name,\n                num_partitions=dataset_params.get(\"num_partitions\"),\n                replication_factor=dataset_params.get(\"replication_factor\"),\n                config=dataset_params.get(\"config\"),\n            )\n\n            # Add dataset to appropriate list\n            dataset_list.append(new_dataset)\n\n        else:\n            raise ValueError(\n                \"Unknown dataset type. Expected: \"\n                f\"{AINEKO_CONFIG.get('STREAM_TYPES')}.\"\n            )\n\n    # Create all configured datasets\n    datasets = kafka_client.create_topics(dataset_list)\n\n    # Block until all datasets finish creation\n    cur_time = time.time()\n    while True:\n        if all(future.done() for future in datasets.values()):\n            logger.info(\"All datasets created.\")\n            break\n        if time.time() - cur_time &gt; AINEKO_CONFIG.get(\n            \"DATASET_CREATION_TIMEOUT\"\n        ):\n            raise TimeoutError(\n                \"Timeout while creating Kafka datasets. \"\n                \"Please check your Kafka cluster.\"\n            )\n\n    return datasets\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.prepare_nodes","title":"<code>prepare_nodes(pipeline_config, poison_pill)</code>","text":"<p>Prepare actor handles for all nodes.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_config</code> <code>dict</code> <p>pipeline configuration</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>list</code> <p>mapping of node names to actor handles</p> <code>list</code> <code>list</code> <p>list of ray objects</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def prepare_nodes(\n    self, pipeline_config: dict, poison_pill: ray.actor.ActorHandle\n) -&gt; list:\n    \"\"\"Prepare actor handles for all nodes.\n\n    Args:\n        pipeline_config: pipeline configuration\n\n    Returns:\n        dict: mapping of node names to actor handles\n        list: list of ray objects\n    \"\"\"\n    # Collect all  actor futures\n    results = []\n\n    default_node_config = pipeline_config.get(\"default_node_settings\", {})\n\n    for node_name, node_config in pipeline_config[\"nodes\"].items():\n        # Initialize actor from specified class in config\n        target_class = imports.import_from_string(\n            attr=node_config[\"class\"], kind=\"class\"\n        )\n        actor_params = {\n            **default_node_config,\n            **node_config.get(\"node_settings\", {}),\n            \"name\": node_name,\n            \"namespace\": self.pipeline_name,\n        }\n\n        wrapped_class = ray.remote(target_class)\n        wrapped_class.options(**actor_params)\n        actor_handle = wrapped_class.remote(\n            node_name=node_name,\n            pipeline_name=self.pipeline_name,\n            poison_pill=poison_pill,\n        )\n\n        # Setup input and output datasets\n        outputs = node_config.get(\"outputs\", [])\n        actor_handle.setup_datasets.remote(\n            inputs=node_config.get(\"inputs\", None),\n            outputs=outputs,\n            datasets=pipeline_config[\"datasets\"],\n            has_pipeline_prefix=True,\n        )\n\n        # Setup internal datasets like logging, without pipeline prefix\n        actor_handle.setup_datasets.remote(\n            outputs=DEFAULT_KAFKA_CONFIG.get(\"DATASETS\"),\n            datasets=pipeline_config[\"datasets\"],\n        )\n\n        # Create actor future (for execute method)\n        results.append(\n            actor_handle.execute.remote(\n                params=node_config.get(\"node_params\", None)\n            )\n        )\n\n    return results\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.run","title":"<code>run()</code>","text":"<p>Runs the pipeline.</p> <p>Step 1: Load config for pipeline</p> <p>Step 2: Set up datasets</p> <p>Step 3: Set up PoisonPill node that is available to all nodes</p> <p>Step 4: Set up nodes (including Node Manager)</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Runs the pipeline.\n\n    Step 1: Load config for pipeline\n\n    Step 2: Set up datasets\n\n    Step 3: Set up PoisonPill node that is available to all nodes\n\n    Step 4: Set up nodes (including Node Manager)\n    \"\"\"\n    # Load pipeline config\n    pipeline_config = self.load_pipeline_config()\n    self.pipeline_name = self.pipeline_name or pipeline_config[\"name\"]\n\n    # Create the necessary datasets\n    self.prepare_datasets(\n        config=pipeline_config[\"datasets\"],\n        user_dataset_prefix=self.pipeline_name,\n    )\n\n    # Initialize ray cluster\n    ray.shutdown()\n    ray.init(\n        namespace=self.pipeline_name,\n        _metrics_export_port=self.metrics_export_port,\n    )\n\n    # Create poison pill actor\n    poison_pill = ray.remote(PoisonPill).remote()\n\n    # Add Node Manager to pipeline config\n    pipeline_config[\"nodes\"][\n        NODE_MANAGER_CONFIG.get(\"NAME\")\n    ] = NODE_MANAGER_CONFIG.get(\"NODE_CONFIG\")\n\n    # Create each node (actor)\n    results = self.prepare_nodes(\n        pipeline_config=pipeline_config,\n        poison_pill=poison_pill,  # type: ignore\n    )\n\n    ray.get(results)\n</code></pre>"},{"location":"developer_guide/aineko_project/","title":"Aineko project","text":"<p>Note</p> <p>This is a continuation of the previous section (Quick Start). Before starting, make sure you have already created a template project using <code>aineko create</code>.</p>"},{"location":"developer_guide/aineko_project/#directory-contents","title":"Directory contents","text":"<pre><code>.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 conf\n\u2502   \u2514\u2500\u2500 pipeline.yml\n\u251c\u2500\u2500 my_awesome_pipeline\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u2514\u2500\u2500 nodes.py\n\u251c\u2500\u2500 poetry.lock\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 test_nodes.py\n</code></pre> <p>This is how the boilerplate directory look - many of these files are boilerplate files to make things work.</p> <p>Let's zoom in on the more interesting files to take note of:</p> <ol> <li><code>conf/pipeline.yml</code> - This contains your pipeline definition that you are expected to modify to define your own pipeline. It is defined in YAML.</li> <li><code>my_awesome_pipeline/nodes.py</code> - Remember how nodes are abstractions for computations? These nodes are implemented in Python. You do not have to strictly define them in this file. You can define them anywhere you like within the directory as long as you reference them correctly in <code>pipeline.yml</code>.</li> </ol>"},{"location":"developer_guide/aineko_project/#defining-a-pipeline","title":"Defining a pipeline","text":"<p>Pipelines are defined using a <code>.yml</code> file that contains specific keys. In this configuration file, you can assemble a pipeline from nodes and datasets.</p> <p>Refer to this for a detailed breakdown on pipeline configuration.</p>"},{"location":"developer_guide/aineko_project/#implementing-a-node","title":"Implementing a node","text":"<p>A node requires:</p> <ul> <li>Inheriting the base node class <code>aineko.core.node.AbstractNode</code></li> <li>Implementing at least the abstract method <code>_execute</code> and optionally <code>_pre_loop_hook</code> .</li> </ul> <p><code>_pre_loop_hook</code> (optional) is used to initialize the node's state before it starts to process data from the dataset.</p> <p><code>_execute</code> is the main logic that run recurrently. As of writing, user should explicitly produce and consume within this method like so:</p> <pre><code>for dataset, consumer in self.consumers.items():\n    # dataset is the name of the dataset as defined in the pipeline yml configuration\n    # consumer is a DatasetConsumer object\n    msg = consumer.consume(how=\"next\")\n</code></pre>"},{"location":"developer_guide/cli/","title":"CLI documentation","text":"<p>The Aineko CLI is a development tool that allows you to get started quickly and introspect your pipeline runs more expediently.</p>"},{"location":"developer_guide/cli/#aineko","title":"aineko","text":"<p>Aineko CLI.</p> <p>Usage:</p> <pre><code>aineko [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --version  Show the version and exit.\n  --help     Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-create","title":"aineko create","text":"<p>Creates boilerplate code and config required for an Aineko pipeline.</p> <p>If repo is set, attempts to clone and use the files from the repo. The following files are required:</p> <pre><code>- aineko.yml: Contains required data to create an aineko pipeline.\n    Requires keys:\n    - aineko_version: Version of aineko to use.\n    - project_name: Name of the aineko project.\n    - project_slug: (optional) slug of project. Can only contain\n        alphanumeric characters and underscores. Will be derived from\n        project_name if not provided.\n    - project_description: (optional) Description of the pipeline.\n    - pipeline_slug: (optional) name of pipeline (alphanumeric\n        characters, dashes and underscores only).\n- &lt;&lt;project_name&gt;&gt;/nodes.py or &lt;&lt;project_name&gt;&gt;/nodes/*.py: Either a\n    file containing all node code or a directory containing multiple\n    node code files.\n- conf/*.yml: Directory containing config files for pipelines.\n</code></pre> <p>The following files are optional, and will overwrite the default files:     - README.md: README file for the repo.     - pyproject.toml: Project configuration, including poetry requirements     - deploy.yml: Project deployment configuration.     - &lt;&gt;/tests/*.py: Test directory <p>Args:     deployment_config: If True, include a deploy file when generating, else     do not include.     output_dir: Directory to create pipeline in. Defaults to current.     no_input: If True, do not prompt for parameters and use defaults.     repo: GitHub repo to create pipeline from. Link should contain branch         or rev to reference.</p> <p>Usage:</p> <pre><code>aineko create [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -d, --deployment-config  Include deploy.yml that\n                           facilitates deployment\n                           of pipelines.\n  -o, --output-dir TEXT    Directory to create\n                           pipeline in. Defaults\n                           to current directory.\n  -n, --no-input           Do not prompt for\n                           parameters and use\n                           defaults.\n  -r, --repo TEXT          GitHub repo to create\n                           pipeline from. Link\n                           should contain branch\n                           or rev to reference.\n                           (Ex aineko-dev/dream-\n                           catcher#my-branch)\n                           Refer to CLI docs for\n                           more info on repo\n                           structure.\n  --help                   Show this message and\n                           exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-dream","title":"aineko dream","text":"<p>Main function to generate an aineko project using Aineko Dream.</p> <p>Args:     prompt: Prompt to generate a project from.     api_key: API key to use for the Aineko Dream API.     url: URL to use for the Aineko Dream API.</p> <p>Usage:</p> <pre><code>aineko dream [OPTIONS] PROMPT API_KEY\n</code></pre> <p>Options:</p> <pre><code>  -u, --url TEXT  API url to use for the Aineko\n                  Dream API.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-run","title":"aineko run","text":"<p>Main function to run a pipeline from the command line.</p> <p>Args:     pipeline_config_file: Path to the file containing the pipeline config     pipeline_name: Name of the pipeline to run, overrides pipeline config     retry: If true, retry running the pipeline on failure every 10 seconds</p> <p>Usage:</p> <pre><code>aineko run [OPTIONS] PIPELINE_CONFIG_FILE\n</code></pre> <p>Options:</p> <pre><code>  -p, --pipeline-name TEXT  Name of the pipeline\n                            to run.\n  -r, --retry               Retry running the\n                            pipeline on failure.\n  --help                    Show this message and\n                            exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service","title":"aineko service","text":"<p>Manage Aineko docker services (Kafka and Zookeeper containers).</p> <p>Usage:</p> <pre><code>aineko service [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -c, --config TEXT  Path to the custom Docker\n                     Compose config file.\n  --help             Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service-down","title":"aineko service down","text":"<p>Kill Aineko docker services.</p> <p>Usage:</p> <pre><code>aineko service down [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service-restart","title":"aineko service restart","text":"<p>Restart Aineko docker services.</p> <p>Usage:</p> <pre><code>aineko service restart [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -H, --hard  Forces full restart Aineko docker\n              services.Clears data from Kafka\n              cache.\n  --help      Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service-start","title":"aineko service start","text":"<p>Start Aineko docker services.</p> <p>Usage:</p> <pre><code>aineko service start [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service-stop","title":"aineko service stop","text":"<p>Stop Aineko docker services.</p> <p>Usage:</p> <pre><code>aineko service stop [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-stream","title":"aineko stream","text":"<p>Stream messages from a dataset.</p> <p>Usage:</p> <pre><code>aineko stream [OPTIONS] DATASET\n</code></pre> <p>Options:</p> <pre><code>  -b, --from-beginning  If messages should be\n                        streamed from the start\n  --help                Show this message and\n                        exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-visualize","title":"aineko visualize","text":"<p>Builds mermaid graph from an Aineko pipeline config.</p> <p>Args:     config_path: file path to pipeline yaml file     direction: direction of the graph.     legend: include a legend in the graph.     browser: Whether to render graph in browser. Prints     graph to stdout otherwise.</p> <p>Usage:</p> <pre><code>aineko visualize [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> <pre><code>  -d, --direction [TD|LR]  Direction of the graph.\n                           Either LR (left to\n                           right) or TD (top\n                           down).\n  -l, --legend             Include a legend in the\n                           graph.\n  -b, --browser            Render graph in\n                           browser. Prints graph\n                           to stdout otherwise.\n  --help                   Show this message and\n                           exit.\n</code></pre>"},{"location":"developer_guide/cli/#shell-completion","title":"Shell completion","text":"<p>Aineko supports shell completion for Bash and Zsh. To enable it, follow the instructions below.</p> <p>Please select your shell</p> BashZsh <p>Add this to <code>~/.bashrc</code>:</p> <pre><code>eval \"$(_AINEKO_COMPLETE=bash_source aineko)\"\n</code></pre> <p>Add this to <code>~/.zshrc</code>:</p> <pre><code>eval \"$(_AINEKO_COMPLETE=zsh_source aineko)\"\n</code></pre>"},{"location":"developer_guide/concepts/","title":"Concepts","text":"What is an Aineko Pipeline? <p>In day-to-day conversations, the term pipeline frequently denotes either a Pipeline definition or a Pipeline execution. Aineko documentation aims to differentiate them explicitly.</p> <p>An Aineko pipeline is a streaming workflow. This means that data is continuously being transmitted and sent over to different components in a way that allows for real-time processing of the data.</p> <p>In a pipeline, you may implement arbitrary computation units (Nodes), and specify where they consume data from, and where they send that data to (Datasets). An Aineko Pipeline allows you to construct complex processing graphs that processes streaming data.</p>"},{"location":"developer_guide/concepts/#pipeline-definition","title":"Pipeline definition","text":"<p>A pipeline definition is a specialised Program that you write - to tell Aineko what a pipeline comprises. A Pipeline definition is defined in YAML and essentially allows Aineko to compose computation nodes together by specifying the input and output buffers that they consume data from and produce data to.</p> <p>See here to learn about writing a pipeline definition.</p>"},{"location":"developer_guide/concepts/#pipeline-execution","title":"Pipeline execution","text":"<p>If a pipeline definition is a program, then a pipeline execution is a process. You can run multiple pipeline executions for a single pipeline definition.</p>"},{"location":"developer_guide/concepts/#dataset","title":"Dataset","text":"<p>A Dataset is an abstraction for a buffer for data that you can define producers and consumers for. Producers write data to a dataset, while consumers read data from the dataset. It's analogous to a pub-sub topic or channel. In the current version of aineko, it's a Kafka Topic, but in future, other implementations of message channels could be pluggable too.</p>"},{"location":"developer_guide/concepts/#node","title":"Node","text":"<p>A Node is an abstraction for some computation, akin to a function. At the same time a Node can be a producer and/or a consumer of a Dataset.</p> <p>A node can optionally consume from topics, process that data and produce the output to another buffer that you can chain other Node consumers on.</p>"},{"location":"developer_guide/config_kafka/","title":"Configuring Kafka","text":"Aineko uses <code>kafka</code> under the hood for sending messages between nodes. As part of running Aineko locally, it's recommended to run a local <code>kafka</code> and <code>zookeeper</code> server using <pre><code>$ poetry run aineko service start\n</code></pre> <p>To use a different <code>kafka</code> cluster, such as in deployment settings, Aineko allows for configuring of <code>kafka</code> parameters through environment variables. Typically, you would want to modify configuration for the consumer and producer to point to the desired cluster.</p> <p>See below for default <code>kafka</code> configuration that ships with <code>aineko</code> and how to override them.</p>"},{"location":"developer_guide/config_kafka/#aineko.config","title":"<code>aineko.config</code>","text":"<p>Configuration file for Aineko modules.</p> <p>Kafka configuration can be set using the following environment variables:</p> <p>KAFKA_CONFIG: JSON string with kafka configuration (see https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md for all options)</p> <p>Additionally, the following environment variables can be used to specify certain configuration values. They correspond to configuration keys found in the above link, but with a prefix. For example, <code>KAFKA_CONFIG_BOOTSTRAP_SERVERS</code> corresponds to <code>bootstrap.servers</code>.</p> <ul> <li>KAFKA_CONFIG_BOOTSTRAP_SERVERS (e.g. <code>localhost:9092,localhost:9093</code>)</li> <li>KAFKA_CONFIG_SASL_USERNAME</li> <li>KAFKA_CONFIG_SASL_PASSWORD</li> <li>KAFKA_CONFIG_SECURITY_PROTOCOL</li> <li>KAFKA_CONFIG_SASL_MECHANISM</li> </ul>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG","title":"<code>DEFAULT_KAFKA_CONFIG</code>","text":"<p>             Bases: <code>BaseConfig</code></p> <p>Kafka configuration.</p> Source code in <code>aineko/config.py</code> <pre><code>class DEFAULT_KAFKA_CONFIG(BaseConfig):\n    \"\"\"Kafka configuration.\"\"\"\n\n    # Default Kafka broker settings\n    BROKER_CONFIG = {\n        \"bootstrap.servers\": \"localhost:9092\",\n    }\n\n    kafka_config = os.environ.get(\"KAFKA_CONFIG\", \"{}\")\n    BROKER_CONFIG.update(json.loads(kafka_config))\n\n    # Override these fields if set\n    OVERRIDABLES = {\n        \"KAFKA_CONFIG_BOOTSTRAP_SERVERS\": \"bootstrap.servers\",\n        \"KAFKA_CONFIG_SASL_USERNAME\": \"sasl.username\",\n        \"KAFKA_CONFIG_SASL_PASSWORD\": \"sasl.password\",\n        \"KAFKA_CONFIG_SECURITY_PROTOCOL\": \"security.protocol\",\n        \"KAFKA_CONFIG_SASL_MECHANISM\": \"sasl.mechanism\",\n    }\n    for env, config in OVERRIDABLES.items():\n        value = os.environ.get(env)\n        if value:\n            BROKER_CONFIG[config] = value\n\n    # Config for default kafka consumer\n    CONSUMER_CONFIG: Dict[str, str] = {\n        **BROKER_CONFIG,\n        \"auto.offset.reset\": \"earliest\",\n    }\n\n    # Config for default kafka producer\n    PRODUCER_CONFIG: Dict[str, str] = {**BROKER_CONFIG}\n\n    # Default dataset config\n    DATASET_PARAMS = {\n        # One single partition for each dataset\n        \"num_partitions\": 1,\n        # No replication\n        \"replication_factor\": 1,\n        \"config\": {\n            # Keep messages for 7 days\n            \"retention.ms\": 1000\n            * 60\n            * 60\n            * 24\n            * 7,\n        },\n    }\n\n    # Default Kafka consumer settings\n    # Timeout for kafka consumer polling (seconds)\n    CONSUMER_TIMEOUT = 0\n    # Max number of messages to retreive when getting the last message\n    CONSUMER_MAX_MESSAGES = 1000000\n\n    # Default Kafka producer settings\n    # Producer overridables\n    # See: https://kafka.apache.org/documentation/#producerconfigs\n    # Empty list means no overridable settings\n    PRODUCER_OVERRIDABLES = []  # type: ignore\n\n    # Default datasets to create for every pipeline\n    LOGGING_DATASET = \"logging\"\n    DATASETS = [LOGGING_DATASET]\n</code></pre>"},{"location":"developer_guide/node_implementation/","title":"Building a node","text":"<p>Nodes are essentially units of compute that encapsulate any event-driven logic you can define in python. Whether it's a transformation, an API call or a data transfer, as long as you can express it in python, it can be contained in a node.</p>"},{"location":"developer_guide/node_implementation/#implementing-a-node","title":"Implementing a node","text":"<p>To illustrate how a node should be constructed, we will go through an example of a simple node that consumes a number from an input dataset, increments it by 1, then produces it to an output dataset.</p> sum_node.py<pre><code>from aineko.internals.node import AbstractNode\n\nclass MySumNode(AbstractNode):\n\n    def _pre_loop_hook(self, params=None):\n        \"\"\"Optional; used to initialize node state.\"\"\"\n        self.state = params.get(\"initial_state\", 0)\n\n    def _execute(self, params=None):\n        \"\"\"Required; function repeatedly executes.\"\"\"\n        msg = self.consumers[\"test_sequence\"].next()\n        self.log(\n            f\"Received input: {msg['message']}. Adding {params['increment']}...\"\n        )\n        self.state = int(msg[\"message\"]) + int(params[\"increment\"])\n        self.producers[\"test_sum\"].produce(self.state)\n</code></pre>"},{"location":"developer_guide/node_implementation/#_pre_loop_hook","title":"<code>_pre_loop_hook</code>","text":"<p>You can optionally define a <code>_pre_loop_hook</code> method in your node class to initialize the state of your node with class variables. If the <code>node_params</code> key is defined in <code>pipeline.yml</code>, it will be passed in under the <code>params</code> argument.</p> sum_node.py<pre><code>from aineko.internals.node import AbstractNode\n\nclass MySumNode(AbstractNode):\n\n    def _pre_loop_hook(self, params=None):\n        \"\"\"Optional; used to initialize node state.\"\"\"\n        self.state = params.get(\"initial_state\", 0)\n\n    def _execute(self, params=None):\n        \"\"\"Required; function repeatedly executes.\"\"\"\n        msg = self.consumers[\"test_sequence\"].next()\n        self.log(\n            f\"Received input: {msg['message']}. Adding {params['increment']}...\"\n        )\n        self.state = int(msg[\"message\"]) + int(params[\"increment\"])\n        self.producers[\"test_sum\"].produce(self.state)\n</code></pre>"},{"location":"developer_guide/node_implementation/#_execute","title":"<code>_execute</code>","text":"<p>The <code>_execute</code> method is repeatedly executed as the pipeline runs. We recommend nodes to follow a design pattern of constantly polling for new data and taking action when new data is received.</p> sum_node.py<pre><code>from aineko.internals.node import AbstractNode\n\nclass MySumNode(AbstractNode):\n\n    def _pre_loop_hook(self, params=None):\n        \"\"\"Optional; used to initialize node state.\"\"\"\n        self.state = params.get(\"initial_state\", 0)\n\n    def _execute(self, params=None):\n        \"\"\"Required; function repeatedly executes.\"\"\"\n        msg = self.consumers[\"test_sequence\"].next()\n        self.log(\n            f\"Received input: {msg['message']}. Adding {params['increment']}...\"\n        )\n        self.state = int(msg[\"message\"]) + int(params[\"increment\"])\n        self.producers[\"test_sum\"].produce(self.state)\n</code></pre> <p>A node will only terminate when the entire pipeline goes down or when the poison pill is activated. </p>"},{"location":"developer_guide/node_implementation/#producers-consumers","title":"Producers &amp; consumers","text":"<p>Node classes inherit attributes named <code>self.producers</code> and <code>self.consumers</code> that are each a dictionary, keyed by dataset name with values being <code>DatasetProducer</code> and <code>DatasetConsumer</code> objects respectively. These objects allow you to produce/consume data to/from a dataset from your catalog configuration.</p> sum_node.py<pre><code>from aineko.internals.node import AbstractNode\n\nclass MySumNode(AbstractNode):\n\n    def _pre_loop_hook(self, params=None):\n        \"\"\"Optional; used to initialize node state.\"\"\"\n        self.state = params.get(\"initial_state\", 0)\n\n    def _execute(self, params=None):\n        \"\"\"Required; function repeatedly executes.\"\"\"\n        msg = self.consumers[\"test_sequence\"].next()\n        self.log(\n            f\"Received input: {msg['message']}. Adding {params['increment']}...\"\n        )\n        self.state = int(msg[\"message\"]) + int(params[\"increment\"])\n        self.producers[\"test_sum\"].produce(self.state)\n</code></pre> <p>Producers and Consumers must be included in the pipeline configuration</p> <p>They must be defined in the <code>outputs</code> and <code>inputs</code> list respectively to be available to the node. If a dataset is not available in a Node's catalog, a <code>KeyError</code> will be raised.</p> <p>A node can produce to a dataset, consume from a dataset, or both. Nodes that consume are triggered to action by the arrival of new data in the dataset they consume from.</p> <p>Examples on possible ways to connect nodes with datasets</p> Produce onlyConsume onlyConsume and Produce <p>This node only produces to two datasets, and acts like a source for datasets:</p> <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nN_node_producer_only((node_producer_only)):::nodeClass --&gt;  T_produced_dataset_1[produced_dataset_1]:::datasetClass\nN_node_producer_only((node_producer_only)):::nodeClass --&gt;  T_produced_dataset_2[produced_dataset_2]:::datasetClass</code></pre> <p>This node only consumes from two datasets, and acts like a sink for datasets: <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_consumed_dataset_1[consumed_dataset_1]:::datasetClass --&gt;  N_node_consumer_only((node_consumer_only)):::nodeClass\nT_consumed_dataset_2[consumed_dataset_2]:::datasetClass --&gt;  N_node_consumer_only((node_consumer_only)):::nodeClass</code></pre></p> <p>A node that both consumes and produces datasets acts like a transformer for datasets. The consumed datasets are the inputs to the transformer, and the produced datasets are the outputs of the transformer: <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_consumed_dataset[consumed_dataset]:::datasetClass --&gt;  N_node_transformer((node_transformer)):::nodeClass\nN_node_transformer((node_transformer)):::nodeClass --&gt;  T_produced_dataset[produced_dataset]:::datasetClass</code></pre></p>"},{"location":"developer_guide/node_implementation/#logging","title":"Logging","text":"Node classes inherit a method named <code>self.log</code> that allows users to log messages. You can set the appropriate level from: <code>info</code>, <code>debug</code>, <code>warning</code>, <code>error</code>, an <code>critical</code>. You can log from inside of the <code>_pre_loop_hook</code> method, the <code>_execute</code> method, or any other method you add to your node. <pre><code>self.log(f\"Produced {self.cur_integer}\", level=\"info\")\n</code></pre>"},{"location":"developer_guide/node_implementation/#poisonpill","title":"PoisonPill","text":"Poison pills refers to an \"emergency shut down\" button that can be triggered in times of emergency. Every node has access to a <code>activate_poison_pill</code> method that will terminate the entire pipeline and kill all nodes. To invoke it, use the following syntax. <pre><code>node.activate_poison_pill()\n</code></pre>"},{"location":"developer_guide/pipeline_configuration/","title":"Pipeline configuration","text":"<p>At a high-level, building a pipeline requires defining a pipeline and implementing at least a node.</p>"},{"location":"developer_guide/pipeline_configuration/#defining-a-pipeline","title":"Defining a pipeline","text":"<p>The simplest possible pipeline would consist of a node and a dataset as shown below.</p> <p><pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nN_sequence((sequence)):::nodeClass --&gt;  T_test_sequence[test_sequence]:::datasetClass</code></pre> For the sake of simplicity, we reference a truncated version of the pipeline definition:</p> Example <code>pipeline.yml</code> configuration file <pre><code>pipeline:\n  name: test-aineko-pipeline\n\n  default_node_settings:\n    num_cpus: 0.5\n\n  nodes:\n    sequence:\n      class: my_awesome_pipeline.nodes.MySequencerNode\n      outputs:\n        - test_sequence\n      node_params:\n        initial_state: 0\n        increment: 1\n\n\n  datasets:\n    test_sequence:\n      type: kafka_stream\n</code></pre> <p>A pipeline definition should have the following attributes:</p>"},{"location":"developer_guide/pipeline_configuration/#keys","title":"Keys","text":""},{"location":"developer_guide/pipeline_configuration/#pipeline","title":"<code>pipeline</code>","text":"<p>This is the top-level key in a pipeline configuration file, a configuration map to define the name, default settings, nodes, and datasets for a pipeline.</p> Key Required Type Description <code>name</code> Y string Name of the pipeline. <code>default_node_settings</code> N map Defines common default values for node attributes which can be overridden at the node level. <code>nodes</code> Y map Defines the compute nodes for a pipeline, mapping to node names. <code>datasets</code> Y map Defines the compute nodes for a pipeline, mapping to structs with node name keys."},{"location":"developer_guide/pipeline_configuration/#default_node_settings","title":"<code>default_node_settings</code>","text":"<p>This optional section can be used to set common default settings for all nodes in the pipeline. These settings are passed into ray actors as parameters, and accept any of the arguments found here. The most common one we usually use is <code>num_cpus</code>.</p> Key Required Type Description <code>&lt;setting&gt;</code> N multiple Any of the parameters found here. <code>num_cpus</code> N float Defines default number of CPUs for a node. Can be less than one. <p>These can be overridden at the node level.</p>"},{"location":"developer_guide/pipeline_configuration/#nodes","title":"<code>nodes</code>","text":"<p>This section defines the compute nodes for a pipeline.</p> Key Required Type Description <code>&lt;name of node&gt;</code> Y map Defines map of node names to node structures in the pipeline."},{"location":"developer_guide/pipeline_configuration/#node_name","title":"<code>&lt;node_name&gt;</code>","text":"<p>A particular node instance in the pipeline, defined by a unique name. Any parameters defined at the individual node level will locally overwrite any default settings defined at the <code>default_node_settings</code> level.</p> Key Required Type Description <code>class</code> Y string Python module to run for the node. This should exist within the python module in the same repository . <code>inputs</code> N list of strings Defines which datasets to consume from if applicable. <code>outputs</code> N list of strings Defines which datasets to produce to if applicable. <code>node_params</code> N map Defines any arbitrary parameters relevant for node's application logic. In the example above, we defined <code>initial_state</code> and <code>increment</code> parameters, which are both integers. <code>num_cpus</code> Y float Number of CPUs allocated to a node. Required either for each node definition or at <code>default_node_settings</code> level."},{"location":"developer_guide/pipeline_configuration/#datasets","title":"<code>datasets</code>","text":"<p>This section defines the datasets for a pipeline.</p> Key Required Type Description <code>&lt;name of dataset&gt;</code> Y map Defines map of dataset names to dataset structures in the pipeline."},{"location":"developer_guide/pipeline_configuration/#name-of-dataset","title":"<code>&lt;name of dataset&gt;</code>","text":"<p>A particular dataset instance in the pipeline, defined by a unique name. Each dataset is defined by a type.</p> Key Required Type Description <code>type</code> Y string Defines which type of dataset to use. Currently, only <code>kafka_stream</code> is supported. <p>Note</p> <p>Aineko is currently in the Beta release stage and is constantly improving.</p> <p>If you have any feedback, questions, or suggestions, please reach out to us.</p>"},{"location":"examples/aineko_dream/","title":"\"Aineko Dream\": code generation using ChatGPT with real-time quality assurance","text":"<p> View on GitHub Try on Slack </p> <p>We built an app called Aineko Dream to test-drive Aineko's ability to enable generative AI features. Aineko Dream uses the OpenAI API and the Aineko docs to generate template code for an Aineko pipeline based on a prompt. The pipeline automatically checks the LLM response to ensure it passes some tests and either generates a prompt to fix the errors or passes the response back to the user.</p> <p>This app demonstrates how you can rapidly prototype features that use foundation models by leveraging Aineko features, such as REST client connectors, stateful feedback loops, and API endpoints. Real-time QA allows us to ensure the quality of LLM responses while maintaining an interactive experience for users.</p> <p>Give it a try using the Aineko Dream bot in our Slack. Or checkout the code on GitHub.</p> <p>What will you dream up with Aineko?</p> <p> </p> <p>Aineko Dream in Action</p> <p>Featured Unlocks</p> <p>Tell a Story with Your Data - With Aineko, you retain all critical information with the context to help you tell a story in real-time and retroactively.</p> <p>Aineko made it simple to introduce a feedback loop for real-time QA for ChatGPT responses. We keep track of response evaluation results and submit new prompts to ChatGPT to fix the errors, without any human intervention. Additionally, since we are tracking all prompts, responses, and their evaluation results, we generate a rich dataset of our app\u2019s performance which we can use to track and improve performance.</p> <p>Move Fast, Break Nothing - By representing our use case as an Aineko pipeline, it was clear how we could swap building blocks in and out with ease.</p> <p>Aineko made it easy to try out new foundation models. Once the pipeline was constructed with GPT-3 it was trivial to use GPT-4. Later, we built a second connector node for the Cohere API and used it as a drop-replacement for OpenAI. We were able to run all three models with standalone pipelines, each with its own unique API endpoint. We can use the various endpoints to route traffic for different users or use one for production and the others for development.</p>"},{"location":"examples/aineko_dream/#tech-overview","title":"Tech overview","text":"<p>With the Aineko framework in mind, we broke the problem down into a few key steps:</p> <ol> <li>Fetch GitHub events</li> <li>Update documentation used for prompt</li> <li>Retrieve prompt from user</li> <li>Prompt engineering</li> <li>Query an LLM</li> <li>Evaluate result</li> <li>Return to step 4, or return LLM response to user</li> </ol> <p>We used the following libraries and APIs to build our app.</p> <p>FastAPI &amp; Uvicorn: used to run an API server on Aineko and configure endpoints used by the application.</p> <p>PyGitHub: used as a client to interact with GitHub to fetch relevant documents.</p> <p>OpenAI &amp; Cohere: used to run inference using their LLMs.</p> <p>Bandit: used to run security tests against Python code that gets generated by the LLMs.</p>"},{"location":"examples/aineko_dream/#the-pipeline","title":"The pipeline","text":"<p>We start first with a simple example represented by the following diagram.</p> <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_github_event[github_event]:::datasetClass --&gt;  N_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass\nN_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass --&gt;  T_document[document]:::datasetClass\nT_user_prompt[user_prompt]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nT_document[document]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nN_PromptModel((PromptModel)):::nodeClass --&gt;  T_generated_prompt[generated_prompt]:::datasetClass\nT_generated_prompt[generated_prompt]:::datasetClass --&gt;  N_GPT3Client((GPT3Client)):::nodeClass\nN_GPT3Client((GPT3Client)):::nodeClass --&gt;  T_llm_response[llm_response]:::datasetClass\nT_llm_response[llm_response]:::datasetClass --&gt;  N_APIServer((APIServer)):::nodeClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_user_prompt[user_prompt]:::datasetClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_github_event[github_event]:::datasetClass</code></pre> <p>The API server accepts commit events from GitHub and prompt requests from users. The commit events trigger updates to the document that is used to engineer the LLM prompt. The engineered prompt is passed to the OpenAI API and the response is returned to the API server.</p> <p>If we wanted to add a few evaluation steps to ensure that the LLMs response is valid, we can do so as shown in the following diagram.</p> <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_github_event[github_event]:::datasetClass --&gt;  N_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass\nN_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass --&gt;  T_document[document]:::datasetClass\nT_user_prompt[user_prompt]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nT_document[document]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nN_PromptModel((PromptModel)):::nodeClass --&gt;  T_generated_prompt[generated_prompt]:::datasetClass\nT_generated_prompt[generated_prompt]:::datasetClass --&gt;  N_GPT3Client((GPT3Client)):::nodeClass\nN_GPT3Client((GPT3Client)):::nodeClass --&gt;  T_llm_response[llm_response]:::datasetClass\nT_llm_response[llm_response]:::datasetClass --&gt;  N_PythonEvaluation((PythonEvaluation)):::nodeClass\nN_PythonEvaluation((PythonEvaluation)):::nodeClass --&gt;  T_evaluation_result[evaluation_result]:::datasetClass\nT_llm_response[llm_response]:::datasetClass --&gt;  N_SecurityEvaluation((SecurityEvaluation)):::nodeClass\nN_SecurityEvaluation((SecurityEvaluation)):::nodeClass --&gt;  T_evaluation_result[evaluation_result]:::datasetClass\nT_evaluation_result[evaluation_result]:::datasetClass --&gt;  N_EvaluationModel((EvaluationModel)):::nodeClass\nN_EvaluationModel((EvaluationModel)):::nodeClass --&gt;  T_final_response[final_response]:::datasetClass\nN_EvaluationModel((EvaluationModel)):::nodeClass --&gt;  T_generated_prompt[generated_prompt]:::datasetClass\nT_final_response[final_response]:::datasetClass --&gt;  N_APIServer((APIServer)):::nodeClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_user_prompt[user_prompt]:::datasetClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_github_event[github_event]:::datasetClass</code></pre> <p>Here we add 2 evaluation steps and an evaluation model:</p> <ul> <li>The <code>PythonEvaluation</code> node validates that the LLM proposes valid Python code.</li> <li>The <code>SecurityEvaluation</code> node runs checks using Bandit to ensure that the Python code that is proposed doesn\u2019t contain any known security concerns.</li> <li>The <code>EvaluationModel</code> node consumes evaluation results and decides wether to generate another prompt or submit the final result to the API server. It keeps track of the evaluation results and the number of times we query the LLM.</li> </ul>"},{"location":"examples/aineko_dream/#pipeline-configuration","title":"Pipeline configuration","text":"<p>Here is the pipeline configuration used to generate this example. We could even configure and run three separate pipelines that use different models and expose different endpoints for each of them.</p> <pre><code>pipeline:\n  name: gpt3-template-generator\n\n  nodes:\n    # Prompt generation\n    # This node can be configured to target any GitHub repo\n    GitHubDocFetcher:\n      class: aineko_dream.nodes.GitHubDocFetcher\n      inputs:\n        - github_event\n      outputs:\n        - document\n      node_params:\n        organization: \"aineko-dev\"\n        repo: \"aineko\"\n        branch: \"documentation\"\n        file_path: \"/docs/\"\n    PromptModel:\n      class: aineko_dream.nodes.PromptModel\n      inputs:\n        - user_prompt\n        - document\n      outputs:\n        - generated_prompt\n    # LLM Client: defines model to use. Change to use another model like GPT-4\n    # If we wanted to use Cohere, switch `OpenAIClient` to `Cohere`.\n    GPT3Client:\n      class: aineko_dream.nodes.OpenAIClient\n      inputs:\n        - generated_prompt\n      outputs:\n        - llm_response\n      node_params:\n        model: \"gpt-3.5-turbo-16k\"\n        max_tokens: 4000\n        temperature: 0.1\n    # Response evaluation\n    PythonEvaluation:\n      class: aineko_dream.nodes.PythonEvaluation\n      inputs:\n        - llm_response\n      outputs:\n        - evaluation_result\n    SecurityEvaluation:\n      class: aineko_dream.nodes.SecurityEvaluation\n      inputs:\n        - llm_response\n      outputs:\n        - evaluation_result\n    EvaluationModel:\n      class: aineko_dream.nodes.EvaluationModel\n      inputs:\n        - evaluation_result\n      outputs:\n        - final_response\n        - generated_prompt\n            node_params:\n                max_cycles: 2\n    # API\n    APIServer:\n      class: aineko_dream.nodes.APIServer\n      inputs:\n        - final_response\n      outputs:\n        - user_prompt\n        - github_event\n      node_params:\n        app: aineko_dream.api.main:app\n        port: 8000\n</code></pre>"},{"location":"examples/aineko_dream/#node-code","title":"Node code","text":"<p>Here are some samples of the node code used to run this pipeline.</p> <p>The <code>GitHubDocFetcher</code> emits the latest document when it initializes and updates the document based on triggers from a GitHub webhook that is configured to target the API server. The document is passed to the <code>PromptModel</code> to engineer a prompt based on the latest document.</p> <pre><code>class GitHubDocFetcher(AbstractNode):\n    \"\"\"Node that fetches code documents from GitHub.\"\"\"\n\n    def _pre_loop_hook(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Initialize connection with GitHub and fetch latest document.\"\"\"\n        # Set parameters\n        self.access_token = os.environ.get(\"GITHUB_ACCESS_TOKEN\")\n        self.organization = params.get(\"organization\")\n        self.repo = params.get(\"repo\")\n        self.branch = params.get(\"branch\")\n        self.file_path = params.get(\"file_path\")\n\n        # Initialize github client\n        auth = Auth.Token(token=self.access_token)\n        self.github_client = Github(auth=auth)\n\n        # Fetch current document\n        self.emit_new_document()\n\n    def _execute(self, params: Optional[dict] = None) -&gt; Optional[bool]:\n        \"\"\"Update document in response to commit events.\"\"\"\n        # Check for new commit events from GitHub\n        message = self.consumers[\"github_event\"].consume()\n        if message is None:\n            return\n\n        # Fetch latest document and send update\n        self.log(\"Received event from GitHub, fetching latest document.\")\n        self.emit_new_document()\n\n    def emit_new_document(self) -&gt; None:\n        \"\"\"Emit new document.\"\"\"\n        repo = self.github_client.get_repo(f\"{self.organization}/{self.repo}\")\n        contents = repo.get_contents(self.file_path, ref=self.branch)\n        github_contents = {f.path: f.decoded_content.decode(\"utf-8\") for f in contents}\n        self.producers[\"document\"].produce(github_contents)\n        self.log(\n            f\"Fetched documents for {self.organization}/{self.repo} branch {self.branch}\"\n        )\n</code></pre> <p>The <code>OpenAIClient</code> node creates a connection to the OpenAI API and submits requests to the configured model using the ChatCompletion interface. The response is passed on to the evaluation nodes.</p> <pre><code>class OpenAIClient(AbstractNode):\n    \"\"\"Node that queries OpenAI LLMs.\"\"\"\n\n    def _pre_loop_hook(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Initialize connection with OpenAI.\"\"\"\n        self.model = params.get(\"model\")\n        self.max_tokens = params.get(\"max_tokens\")\n        self.temperature = params.get(\"temperature\")\n        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    def _execute(self, params: Optional[dict] = None) -&gt; Optional[bool]:\n        \"\"\"Query OpenAI LLM.\"\"\"\n        message = self.consumers[\"generated_prompt\"].consume()\n        if message is None:\n            return\n        messages = message[\"message\"][\"chat_messages\"]\n        # Query OpenAI LLM\n        self.log(\"Querying OpenAI LLM...\")\n        response = openai.ChatCompletion.create(\n            messages=messages,\n            stream=False,\n            model=self.model,\n            max_tokens=self.max_tokens,\n            temperature=self.temperature,\n          )\n        message[\"message\"][\"chat_messages\"].append(\n            {\n                \"role\": \"assistant\",\n                \"content\": response.choices[0].message.content,\n            }\n        )\n        self.producers[\"llm_response\"].produce(\n                        message[\"message\"][\"chat_messages\"]\n                )\n</code></pre> <p>The <code>SecurityEvaluation</code> node takes the LLM response and creates a temporary file with the contents. Aineko Dream then uses Bandit to run a test against the file and collect a list of issues. After cleaning up, the results are submitted to the <code>EvaluationModel</code> node.</p> <pre><code>class SecurityEvaluation(AbstractNode):\n    \"\"\"Node that evaluates security of code.\"\"\"\n\n    def _execute(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Evaluate Python code.\"\"\"\n        message = self.consumers[\"llm_response\"].consume()\n        if message is None:\n            return\n\n                # Make a temporary file with the LLM code\n        python_code = message[\"message\"]\n        issues_list = []\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False, mode=\"w\") as tmpfile:\n            tmpfile.write(python_code)\n\n        # Setup Bandit and run tests on the temporary file\n        b_mgr = bandit.manager.BanditManager(bandit.config.BanditConfig(), 'file')\n        b_mgr.discover_files([tmpfile.name], None)\n        b_mgr.run_tests()\n\n        # Store results\n        results = b_mgr.get_issue_list(\n            sev_level=bandit.constants.LOW,\n            conf_level=bandit.constants.LOW,\n            )\n\n        # Cleanup (remove the temporary file)\n        tmpfile.close()\n        os.remove(tmpfile.name)\n\n        if results:\n            self.producers[\"evaluation_result\"].produce(results)\n</code></pre>"},{"location":"examples/aineko_dream/#try-running-locally","title":"Try running locally","text":"<p>Visit GitHub for the latest version of the code and see if you can run it yourself.</p> <p>You can install the app using poetry after cloning from GitHub using:</p> <pre><code>poetry install\n</code></pre> <p>First, make sure that docker is running and run the required docker services in the background using:</p> <pre><code>poetry run aineko service start\n</code></pre> <p>Then start the pipeline using:</p> <pre><code>poetry run aineko run ./conf/gpt3.yml\n</code></pre> <p>When the pipeline is live, you can visit http://127.0.0.1:8000/docs in your browser to interact with the endpoints via the Swagger UI.</p> <p> </p> <p>Swagger UI</p>"},{"location":"examples/aineko_dream/#join-the-community","title":"Join the community","text":"<p>If you have questions about anything related to Aineko, you're always welcome to ask the community on GitHub or Slack.</p>"},{"location":"extras/extra_nodes/","title":"Extra Nodes","text":"<p>Extra nodes are nodes that come shipped with the <code>aineko</code> package. They cover popular use-cases and follow best-practice patterns to accelerate your speed of development.</p> <p>To use one of these nodes, simply add it to your dependencies and reference in it in the pipeline configuration.</p>"},{"location":"extras/extra_nodes/#adding-dependencies","title":"Adding Dependencies","text":"To install the dependencies of an extra node submodule, modify your <code>pyproject.toml</code> file by adding the <code>extras</code> key to the <code>aineko</code> package. pyproject.toml<pre><code>[tool.poetry.dependencies]\npython = \"&gt;=3.10,&lt;3.11\"\naineko = {version = \"^0.2.5\", extras=[\"fastapi\"]}\n</code></pre> Once added, install the required dependencies using  <pre><code>$ poetry lock\n$ poetry install\n</code></pre>"},{"location":"extras/extra_nodes/#reference-in-pipeline-configuration","title":"Reference in Pipeline Configuration","text":"To use such a node, simply reference the class in your pipeline configuration. pipeline.yml<pre><code>nodes:\n  fastapi:\n    class: aineko.extras.FastAPI\n    inputs:\n      - test_sequence\n    node_params:\n      app: my_awesome_pipeline.fastapi:app\n      port: 8000\n</code></pre> <p>Refer to the in-depth pages on each extra node for more detail on how to use them.</p>"},{"location":"extras/fastapi/","title":"FastAPI Node","text":"The FastAPI extra node can be used by adding the following to <code>pyproject.toml</code> pyproject.toml<pre><code>[tool.poetry.dependencies]\naineko = {version = \"^0.2.5\", extras=[\"fastapi\"]}\n</code></pre>"},{"location":"extras/fastapi/#api-reference","title":"API Reference","text":""},{"location":"extras/fastapi/#aineko.extras.fastapi.main.FastAPI","title":"<code>aineko.extras.fastapi.main.FastAPI</code>","text":"<p>             Bases: <code>AbstractNode</code></p> <p>Node for creating a FastAPI app with a gunicorn server.</p> <p><code>node_params</code> should contain the following keys:</p> <pre><code>app: path to FastAPI app\nport (optional): port to run the server on. Defaults to 8000.\nlog_level (optional): log level to log messages from the uvicorn server.\n    Defaults to \"info\".\n</code></pre> <p>To access the consumers and producers from your FastAPI app, import the <code>consumers</code> and <code>producers</code> variables from <code>aineko.extras.fastapi</code>. Use them as you would use <code>self.consumers</code> and <code>self.producers</code> in a regular node.</p> <p>We recommend no more than 1 FastAPI node per pipeline since the Consumer and Producer objects are namespaced at the pipeline level.</p> <p>Example usage in pipeline.yml: pipeline.yml<pre><code>pipeline:\n  nodes:\n    fastapi:\n      class: aineko.extras.FastAPI\n      inputs:\n        - test_sequence\n      node_params:\n        app: my_awesome_pipeline.fastapi:app\n        port: 8000\n</code></pre> where the app points to a FastAPI app. See FastAPI documentation on how to create a FastAPI app.</p> <p>Example usage in FastAPI app: fastapi.py<pre><code>from aineko.extras.fastapi import consumers, producers\n\n@app.get(\"/query\")\nasync def query():\n    msg = consumers[\"test_sequence\"].next()\n    return msg\n</code></pre></p> Source code in <code>aineko/extras/fastapi/main.py</code> <pre><code>class FastAPI(AbstractNode):\n    \"\"\"Node for creating a FastAPI app with a gunicorn server.\n\n    `node_params` should contain the following keys:\n\n        app: path to FastAPI app\n        port (optional): port to run the server on. Defaults to 8000.\n        log_level (optional): log level to log messages from the uvicorn server.\n            Defaults to \"info\".\n\n    To access the consumers and producers from your FastAPI app, import the\n    `consumers` and `producers` variables from `aineko.extras.fastapi`. Use\n    them as you would use `self.consumers` and `self.producers` in a regular\n    node.\n\n    We recommend no more than 1 FastAPI node per pipeline since the Consumer\n    and Producer objects are namespaced at the pipeline level.\n\n    Example usage in pipeline.yml:\n    ```yaml title=\"pipeline.yml\"\n    pipeline:\n      nodes:\n        fastapi:\n          class: aineko.extras.FastAPI\n          inputs:\n            - test_sequence\n          node_params:\n            app: my_awesome_pipeline.fastapi:app\n            port: 8000\n    ```\n    where the app points to a FastAPI app. See\n    [FastAPI documentation](https://fastapi.tiangolo.com/){:target=\"\\_blank\"}\n    on how to create a FastAPI app.\n\n    Example usage in FastAPI app:\n    ```python title=\"fastapi.py\"\n    from aineko.extras.fastapi import consumers, producers\n\n    @app.get(\"/query\")\n    async def query():\n        msg = consumers[\"test_sequence\"].next()\n        return msg\n    ```\n    \"\"\"\n\n    def _pre_loop_hook(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Initialize node state. Set env variables for Fast API app.\"\"\"\n        for key, value in self.consumers.items():\n            consumers[key] = value\n        for key, value in self.producers.items():\n            producers[key] = value\n\n    def _execute(self, params: dict) -&gt; None:\n        \"\"\"Start the API server.\"\"\"\n        config = uvicorn.Config(\n            app=params.get(\"app\"),  # type: ignore\n            port=params.get(\"port\", 8000),\n            log_level=params.get(\"log_level\", \"info\"),\n            host=\"0.0.0.0\",\n        )\n        server = uvicorn.Server(config)\n        server.run()\n</code></pre>"}]}