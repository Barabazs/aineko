{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Aineko","text":"<p>Aineko simplifies the developer experience, and helps team create fast and reliable intelligent applications by abstracting away the complexities of deployment.</p>"},{"location":"#what-is-aineko","title":"What is Aineko?","text":"<p>Aineko is a python framework for building powerful data applications quickly. You can use it to simplify development in many ways, including build your back-end and data stacks, process streaming data, or manage generative AI feedback loops.</p> <ul> <li>Code Faster  Use pre-built nodes that work with popular data sources like REST APIs</li> <li>Always Robust  Production-ready from the get-go. Scale with ease.</li> <li>Stateful Computation  Aineko supports long-running stateful computations.</li> <li>Composable  Scale your project easily with engineering best practices.</li> <li>Fast  Achieve microsecond latency between nodes.*</li> <li>Scalable  Process billions or records per day with ease.*</li> </ul> <p>* As measured from an internal use case. Performance may vary depending on exact use case.</p>"},{"location":"#how-to-get-started","title":"How to Get Started?","text":"<p>As an open source framework, you can install and build pipelines on your local machines. You can explore how easy it is to go from idea to pipeline, see its scale and modularity, and see how fast it processes real-time data. </p> <p>To unlock the true power of the framework, try Aineko Cloud. Aineko Cloud  simplifies your tooling by bundling tedious infrastructure, helping you scale, automate deployment, and collaborate as a team. </p>"},{"location":"#aineko-products","title":"Aineko Products","text":"<ul> <li> <p>Open Source Framework</p> <p>Prototype locally, and discover:</p> <ul> <li>Rapid development cycles</li> <li>Scale and modularity</li> <li>Real-time data processing</li> </ul> <p>Quickstart Guide</p> </li> <li> <p>Aineko Cloud</p> <p>Upgrade to unlock the full power of Aineko:</p> <ul> <li>Run your pipelines in the cloud</li> <li>Share projects &amp; collaborate</li> <li>Automate infrastructure deployments</li> </ul> <p>Demo Cloud</p> </li> </ul>"},{"location":"#developer-guide","title":"Developer Guide","text":""},{"location":"#core-concepts","title":"Core Concepts","text":"<ul> <li>Pipeline</li> <li>Node</li> <li>Dataset</li> <li>Workflow</li> <li>Aineko Cloud</li> </ul>"},{"location":"#creating-building-pipelines","title":"Creating &amp; Building Pipelines","text":"<ol> <li>Create an Aineko Project</li> <li>Pipeline Configuration</li> <li>Building a Node</li> <li>CLI Documentation</li> <li>Configuring Kafka</li> </ol>"},{"location":"#plugins","title":"Plugins","text":"<p>Supercharge your development by using pre-built nodes directly in your pipeline. Aineko Plugins contains nodes that are production-ready out of the box. Visit the Aineko Plugins page to see the full list of available plugins.</p>"},{"location":"#api-documentation","title":"API Documentation","text":"<ul> <li><code>AbstractNode</code></li> <li><code>AbstractDataset</code></li> <li><code>KafkaDataset</code></li> <li><code>Config</code></li> <li><code>ConfigLoader</code></li> <li><code>Runner</code></li> </ul>"},{"location":"#examples-templates","title":"Examples &amp; Templates","text":""},{"location":"#common-use-cases","title":"Common use cases","text":"<ul> <li> <p>Self-checking AI Agents</p> <p>Create an AI agent that accepts user prompts, iterates on an answer, and outputs it after all specified checks pass.</p> <p> Aineko Dream</p> </li> </ul>"},{"location":"#join-the-community","title":"Join the Community","text":"<p>Have questions or \"ah-ha\" moments? Building something cool with Aineko? Go ahead and share it with the Aineko community.</p> <p> Join our Slack</p> <p>Contribution Guide</p>"},{"location":"aineko_dream/","title":"Aineko Dream","text":"<p>Aineko Dream leverages the power of generative AI to create a starter Aineko pipeline based on your use case. </p>"},{"location":"aineko_dream/#generating-a-project","title":"Generating a Project","text":"<p>To generate a project, invoke the Aineko Dream CLI with a prompt with:</p> <pre><code>poetry run aineko dream create --api-key API_KEY \"create a pipeline that scrapes twitter and analyses the results to identify trends\"\n</code></pre> <p>replacing <code>API-KEY</code> with a valid Aineko Dream API key. Contact support@aineko.dev to get an API key to try this feature.</p> <p>Aineko Dream goes on to create a complete aineko project, including node code, pipeline configuration and more using OpenAI's GPT-4 models. Upon completion, Aineko Dream publishes the project in the public GitHub repository dream-catcher.</p>"},{"location":"aineko_dream/#creating-your-aineko-dream-project","title":"Creating your Aineko Dream project","text":"<p>Upon completion of the previous step, <code>aineko create</code> offers an easy way to get started. To initialize an aineko project using the generated files from the previous step, run</p> <pre><code>poetry run aineko create --repo Convex-Labs/dream-catcher#12345\n</code></pre> <p>where the argument after <code>--repo</code> should be the unique ID associated to your generated project. This will be output in the result of the previous section.</p>"},{"location":"aineko_dream/#checking-on-the-status-of-your-aineko-dream-request","title":"Checking on the status of your Aineko Dream request","text":"<p>After sending a request to the Aineko Dream CLI, you will be provided with a request ID that looks like <code>2c8341b7-9cb4-41f1-87ad-f363925fd2fa</code>. To check on the status of the request, use the following Aineko Dream CLI command:</p> <pre><code>poetry run aineko dream check 2c8341b7-9cb4-41f1-87ad-f363925fd2fa\n</code></pre>"},{"location":"api_reference/abstract_dataset/","title":"<code>AbstractDataset</code>","text":"<p>The <code>AbstractDataset</code> class is the base class for defining synchronous datasets.</p>"},{"location":"api_reference/abstract_dataset/#aineko.core.dataset.AbstractDataset","title":"aineko.core.dataset.AbstractDataset","text":"<p>             Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Base class for defining new synchronous Aineko datasets.</p> <p>Subclass implementations can be instantiated using the <code>from_config</code> method.</p> <p>When defining a new dataset, the following methods must be implemented:</p> <pre><code>- `read`\n- `write`\n- `create`\n- `delete`\n- `initialize`\n- `exists`\n</code></pre> <p>Example: <pre><code>class MyDataset(AbstractDataset):\n    def read(self, **kwargs) -&gt; Any:\n        pass\n\n    def write(self, **kwargs) -&gt; Any:\n        pass\n\n    def create(self, **kwargs) -&gt; Any:\n        pass\n\n    def delete(self, **kwargs) -&gt; Any:\n        pass\n\n    def initialize(self, **kwargs) -&gt; Any:\n        pass\n\n    def exists(self, **kwargs) -&gt; bool:\n        pass\n</code></pre></p> <p>If <code>MyDataset</code> was defined in the file <code>./aineko/datasets/mydataset.py</code>, a new dataset can be created using the <code>from_config</code> method:</p> <p>Example: <pre><code>dataset = AbstractDataset.from_config(\n    name=\"my_dataset_instance\",\n    config={\n        \"type\": \"aineko.datasets.mydataset.MyDataset\",\n        \"location\": \"foo\",\n        \"params\": {\n            \"param_1\": \"bar\"\n        }\n    }\n)\n</code></pre></p>"},{"location":"api_reference/abstract_dataset/#aineko.core.dataset.AbstractDataset.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_reference/abstract_dataset/#aineko.core.dataset.AbstractDataset.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Return the string representation of the dataset.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the dataset.\"\"\"\n    return f\"{self.__class__.__name__}({self.name})\"\n</code></pre>"},{"location":"api_reference/abstract_dataset/#aineko.core.dataset.AbstractDataset.create","title":"create  <code>abstractmethod</code>","text":"<pre><code>create(*args: T, **kwargs: T) -&gt; Any\n</code></pre> <p>Subclass implementation to create the dataset.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>@abc.abstractmethod\ndef create(self, *args: T, **kwargs: T) -&gt; Any:\n    \"\"\"Subclass implementation to create the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/abstract_dataset/#aineko.core.dataset.AbstractDataset.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(*args: T, **kwargs: T) -&gt; Any\n</code></pre> <p>Subclass implementation to delete the dataset.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>@abc.abstractmethod\ndef delete(self, *args: T, **kwargs: T) -&gt; Any:\n    \"\"\"Subclass implementation to delete the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/abstract_dataset/#aineko.core.dataset.AbstractDataset.exists","title":"exists  <code>abstractmethod</code>","text":"<pre><code>exists(*args: T, **kwargs: T) -&gt; bool\n</code></pre> <p>Subclass implementation to check if the dataset exists.</p> <p>This method should return True if the dataset exists, otherwise False.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>@abc.abstractmethod\ndef exists(self, *args: T, **kwargs: T) -&gt; bool:\n    \"\"\"Subclass implementation to check if the dataset exists.\n\n    This method should return True if the dataset exists, otherwise False.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/abstract_dataset/#aineko.core.dataset.AbstractDataset.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    name: str, config: Dict[str, Any]\n) -&gt; AbstractDataset\n</code></pre> <p>Create a dataset from a configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the dataset.</p> required <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary.</p> required <p>Returns:</p> Type Description <code>AbstractDataset</code> <p>Instance of an <code>AbstractDataset</code> subclass.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>@classmethod\ndef from_config(\n    cls, name: str, config: Dict[str, Any]\n) -&gt; \"AbstractDataset\":\n    \"\"\"Create a dataset from a configuration dictionary.\n\n    Args:\n        name: The name of the dataset.\n        config: The configuration dictionary.\n\n    Returns:\n        Instance of an `AbstractDataset` subclass.\n    \"\"\"\n    dataset_config = DatasetConfig(**dict(config))\n\n    class_obj = import_from_string(dataset_config.type, kind=\"class\")\n    class_instance = class_obj(name, dict(dataset_config))\n    class_instance.name = name\n    return class_instance\n</code></pre>"},{"location":"api_reference/abstract_dataset/#aineko.core.dataset.AbstractDataset.initialize","title":"initialize  <code>abstractmethod</code>","text":"<pre><code>initialize(*args: T, **kwargs: T) -&gt; Any\n</code></pre> <p>Subclass implementation to initialize the dataset query layer.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>@abc.abstractmethod\ndef initialize(self, *args: T, **kwargs: T) -&gt; Any:\n    \"\"\"Subclass implementation to initialize the dataset query layer.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/abstract_dataset/#aineko.core.dataset.AbstractDataset.read","title":"read  <code>abstractmethod</code>","text":"<pre><code>read(*args: T, **kwargs: T) -&gt; Any\n</code></pre> <p>Subclass implementation to read an entry from the dataset.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>@abc.abstractmethod\ndef read(self, *args: T, **kwargs: T) -&gt; Any:\n    \"\"\"Subclass implementation to read an entry from the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/abstract_dataset/#aineko.core.dataset.AbstractDataset.write","title":"write  <code>abstractmethod</code>","text":"<pre><code>write(*args: T, **kwargs: T) -&gt; Any\n</code></pre> <p>Subclass implementation to write an entry to the dataset.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>@abc.abstractmethod\ndef write(self, *args: T, **kwargs: T) -&gt; Any:\n    \"\"\"Subclass implementation to write an entry to the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/abstract_node/","title":"<code>AbstractNode</code>","text":"<p>The <code>AbstractNode</code> class serves as the base class for all user-defined nodes.</p>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode","title":"aineko.AbstractNode","text":"<pre><code>AbstractNode(\n    pipeline_name: str,\n    node_name: Optional[str] = None,\n    poison_pill: Optional[ActorHandle] = None,\n    test: bool = False,\n)\n</code></pre> <p>             Bases: <code>ABC</code></p> <p>Node base class for all nodes in the pipeline.</p> <p>Nodes are the building blocks of the pipeline and are responsible for executing the pipeline. Nodes are designed to be modular and can be combined to create a pipeline. The node base class provides helper methods for setting up the dataset inputs and outputs for a node. The execute method is a wrapper for the _execute method which is to be implemented by subclasses. The _execute method is where the node logic is implemented by the user.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>name of the node</p> <code>pipeline_name</code> <code>str</code> <p>name of the pipeline</p> <code>params</code> <code>dict</code> <p>dict of parameters to be used by the node</p> <code>inputs</code> <code>dict</code> <p>dict of AbstractDataset objects for inputs that node can read / consume.</p> <code>outputs</code> <code>dict</code> <p>dict of AbstractDataset objects for outputs that node can write / produce.</p> <code>last_hearbeat</code> <code>float</code> <p>timestamp of the last heartbeat</p> <code>test</code> <code>bool</code> <p>True if node is in test mode else False</p> <code>log_levels</code> <code>tuple</code> <p>tuple of allowed log levels</p> <code>local_state</code> <code>dict</code> <p>shared local state between nodes. Used for intra- pipeline communication without dataset dependency.</p> <p>Methods:</p> Name Description <code>setup_datasets</code> <p>setup the dataset query layer for a node</p> <code>execute</code> <p>execute the node, wrapper for _execute method</p> <code>_execute</code> <p>execute the node, to be implemented by subclasses</p> <p>Initialize the node.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def __init__(\n    self,\n    pipeline_name: str,\n    node_name: Optional[str] = None,\n    poison_pill: Optional[ray.actor.ActorHandle] = None,\n    test: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the node.\"\"\"\n    self.name = node_name or self.__class__.__name__\n    self.pipeline_name = pipeline_name\n    self.last_heartbeat = time.time()\n    self.inputs: dict = {}\n    self.outputs: dict = {}\n    self.params: Dict = {}\n    self.test = test\n    self.log_levels = AINEKO_CONFIG.get(\"LOG_LEVELS\")\n    self.poison_pill = poison_pill\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.inputs","title":"inputs  <code>instance-attribute</code>","text":"<pre><code>inputs: dict = {}\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.last_heartbeat","title":"last_heartbeat  <code>instance-attribute</code>","text":"<pre><code>last_heartbeat = time()\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.log_levels","title":"log_levels  <code>instance-attribute</code>","text":"<pre><code>log_levels = get('LOG_LEVELS')\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = node_name or __name__\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.outputs","title":"outputs  <code>instance-attribute</code>","text":"<pre><code>outputs: dict = {}\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params: Dict = {}\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.pipeline_name","title":"pipeline_name  <code>instance-attribute</code>","text":"<pre><code>pipeline_name = pipeline_name\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.poison_pill","title":"poison_pill  <code>instance-attribute</code>","text":"<pre><code>poison_pill = poison_pill\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.test","title":"test  <code>instance-attribute</code>","text":"<pre><code>test = test\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.activate_poison_pill","title":"activate_poison_pill","text":"<pre><code>activate_poison_pill() -&gt; None\n</code></pre> <p>Activates poison pill, shutting down entire pipeline.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def activate_poison_pill(self) -&gt; None:\n    \"\"\"Activates poison pill, shutting down entire pipeline.\"\"\"\n    if self.poison_pill:\n        ray.get(self.poison_pill.activate.remote())\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.enable_test_mode","title":"enable_test_mode","text":"<pre><code>enable_test_mode() -&gt; None\n</code></pre> <p>Enable test mode.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def enable_test_mode(self) -&gt; None:\n    \"\"\"Enable test mode.\"\"\"\n    self.test = True\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.execute","title":"execute","text":"<pre><code>execute(params: Optional[dict] = None) -&gt; None\n</code></pre> <p>Execute the node.</p> <p>Wrapper for _execute method to be implemented by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Optional[dict]</code> <p>Parameters to use to execute the node. Defaults to None.</p> <code>None</code> Source code in <code>aineko/core/node.py</code> <pre><code>def execute(self, params: Optional[dict] = None) -&gt; None:\n    \"\"\"Execute the node.\n\n    Wrapper for _execute method to be implemented by subclasses.\n\n    Args:\n        params: Parameters to use to execute the node. Defaults to None.\n    \"\"\"\n    params = params or {}\n    run_loop = True\n\n    try:\n        self._pre_loop_hook(params)\n    except Exception:  # pylint: disable=broad-except\n        self._log_traceback()\n        raise\n\n    while run_loop is not False:\n        # Monitoring\n        try:\n            run_loop = self._execute(params)  # type: ignore\n        except Exception:  # pylint: disable=broad-except\n            self._log_traceback()\n            raise\n\n    self.log(f\"Execution loop complete for node: {self.__class__.__name__}\")\n    self._post_loop_hook(params)\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.log","title":"log","text":"<pre><code>log(message: str, level: str = 'info') -&gt; None\n</code></pre> <p>Log a message to the logging dataset.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message to log</p> required <code>level</code> <code>str</code> <p>Logging level. Defaults to \"info\". Options are: \"info\", \"debug\", \"warning\", \"error\", \"critical\"</p> <code>'info'</code> <p>Raises:     ValueError: if invalid logging level is provided</p> Source code in <code>aineko/core/node.py</code> <pre><code>def log(self, message: str, level: str = \"info\") -&gt; None:\n    \"\"\"Log a message to the logging dataset.\n\n    Args:\n        message: Message to log\n        level: Logging level. Defaults to \"info\". Options are:\n            \"info\", \"debug\", \"warning\", \"error\", \"critical\"\n    Raises:\n        ValueError: if invalid logging level is provided\n    \"\"\"\n    if level not in self.log_levels:\n        raise ValueError(\n            f\"Invalid logging level {level}. Valid options are: \"\n            f\"{', '.join(self.log_levels)}\"\n        )\n    out_msg = {\"log\": message, \"level\": level}\n\n    self.outputs[DEFAULT_KAFKA_CONFIG.get(\"LOGGING_DATASET\")].write(out_msg)\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.run_test","title":"run_test","text":"<pre><code>run_test(runtime: Optional[int] = None) -&gt; dict\n</code></pre> <p>Execute the node in testing mode.</p> <p>Runs the steps in execute that involves the user defined methods. Includes pre_loop_hook, _execute, and post_loop_hook.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>Optional[int]</code> <p>Number of seconds to run the execute loop for.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dataset names and values produced by the node.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def run_test(self, runtime: Optional[int] = None) -&gt; dict:\n    \"\"\"Execute the node in testing mode.\n\n    Runs the steps in execute that involves the user defined methods.\n    Includes pre_loop_hook, _execute, and post_loop_hook.\n\n    Args:\n        runtime: Number of seconds to run the execute loop for.\n\n    Returns:\n        dict: dataset names and values produced by the node.\n    \"\"\"\n    if self.test is False:\n        raise RuntimeError(\n            \"Node is not in test mode. \"\n            \"Please initialize with `enable_test_mode()`.\"\n        )\n    run_loop = True\n    start_time = time.time()\n\n    self._pre_loop_hook(self.params)\n    while run_loop is not False:\n        run_loop = self._execute(self.params)  # type: ignore\n\n        # Do not end loop if runtime not exceeded\n        if runtime is not None:\n            if time.time() - start_time &lt; runtime:\n                continue\n\n        # End loop if all inputs are empty\n        if self.inputs and all(\n            input.empty for input in self.inputs.values()\n        ):\n            run_loop = False\n\n    self._post_loop_hook(self.params)\n\n    return {\n        dataset_name: output.output_values\n        for dataset_name, output in self.outputs.items()\n    }\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.run_test_yield","title":"run_test_yield","text":"<pre><code>run_test_yield(\n    runtime: Optional[int] = None,\n) -&gt; Generator[Tuple[dict, dict, AbstractNode], None, None]\n</code></pre> <p>Execute the node in testing mode, yielding at each iteration.</p> <p>This method is an alternative to <code>run_test</code>. Instead of returning the aggregated output, it yields the most recently read value, the written value and the current node instance at each iteration. This is useful for testing nodes that either don't produce any output or if you need to test intermediate outputs. Testing state modifications is also possible using this method.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>Optional[int]</code> <p>Number of seconds to run the execute loop for.</p> <code>None</code> <p>Yields:</p> Type Description <code>dict</code> <p>A tuple containing the most recent input value, output value and</p> <code>dict</code> <p>the node instance.</p> Example <p>for input, output, node_instance in sequencer.run_test_yield():     print(f\"Input: {input}, Output: {output})     print(f\"Node Instance: {node_instance}\")</p> Source code in <code>aineko/core/node.py</code> <pre><code>def run_test_yield(\n    self, runtime: Optional[int] = None\n) -&gt; Generator[Tuple[dict, dict, \"AbstractNode\"], None, None]:\n    \"\"\"Execute the node in testing mode, yielding at each iteration.\n\n    This method is an alternative to `run_test`. Instead of returning the\n    aggregated output, it yields the most recently read value, the\n    written value and the current node instance at each iteration. This is\n    useful for testing nodes that either don't produce any output or if you\n    need to test intermediate outputs. Testing state modifications is also\n    possible using this method.\n\n    Args:\n        runtime: Number of seconds to run the execute loop for.\n\n    Yields:\n        A tuple containing the most recent input value, output value and\n        the node instance.\n\n    Example:\n        &gt;&gt;&gt; for input, output, node_instance in sequencer.run_test_yield():\n        &gt;&gt;&gt;     print(f\"Input: {input}, Output: {output})\n        &gt;&gt;&gt;     print(f\"Node Instance: {node_instance}\")\n    \"\"\"\n    if self.test is False:\n        raise RuntimeError(\n            \"Node is not in test mode. \"\n            \"Please initialize with `enable_test_mode()`.\"\n        )\n    run_loop = True\n    start_time = time.time()\n\n    self._pre_loop_hook(self.params)\n    while run_loop is not False:\n        last_produced_values = {}\n        last_consumed_values = {}\n\n        # Capture last read values\n        for dataset_name, input_dataset in self.inputs.items():\n            if input_dataset.input_values:\n                last_value = input_dataset.input_values[0]\n                last_consumed_values[dataset_name] = last_value\n\n        run_loop = self._execute(self.params)  # type: ignore\n\n        # Do not end loop if runtime not exceeded\n        if runtime is not None:\n            if time.time() - start_time &lt; runtime:\n                continue\n\n        # End loop if all inputs are empty\n        if self.inputs and all(\n            input.empty for input in self.inputs.values()\n        ):\n            run_loop = False\n\n        # Capture last produced values\n        for dataset_name, output in self.outputs.items():\n            if output.output_values:\n                last_value = output.output_values[-1]\n                last_produced_values[dataset_name] = last_value\n\n        yield (last_consumed_values, last_produced_values, self)\n\n    self._post_loop_hook(self.params)\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.setup_datasets","title":"setup_datasets","text":"<pre><code>setup_datasets(\n    datasets: Dict[str, Dict],\n    inputs: Optional[List[str]] = None,\n    outputs: Optional[List[str]] = None,\n    prefix: Optional[str] = None,\n    has_pipeline_prefix: bool = False,\n) -&gt; None\n</code></pre> <p>Setup the dataset inputs and outputs for a node.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Dict[str, Dict]</code> <p>dataset configuration</p> required <code>inputs</code> <code>Optional[List[str]]</code> <p>list of dataset names for the inputs to the node</p> <code>None</code> <code>outputs</code> <code>Optional[List[str]]</code> <p>list of dataset names for the outputs of the node</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>prefix for topic name (<code>&lt;prefix&gt;.&lt;dataset_name&gt;</code>)</p> <code>None</code> <code>has_pipeline_prefix</code> <code>bool</code> <p>whether the dataset name has pipeline name prefix</p> <code>False</code> Source code in <code>aineko/core/node.py</code> <pre><code>def setup_datasets(\n    self,\n    datasets: Dict[str, Dict],\n    inputs: Optional[List[str]] = None,\n    outputs: Optional[List[str]] = None,\n    prefix: Optional[str] = None,\n    has_pipeline_prefix: bool = False,\n) -&gt; None:\n    \"\"\"Setup the dataset inputs and outputs for a node.\n\n    Args:\n        datasets: dataset configuration\n        inputs: list of dataset names for the inputs to the node\n        outputs: list of dataset names for the outputs of the node\n        prefix: prefix for topic name (`&lt;prefix&gt;.&lt;dataset_name&gt;`)\n        has_pipeline_prefix: whether the dataset name has pipeline name\n            prefix\n    \"\"\"\n    inputs = inputs or []\n    self.inputs.update(\n        {\n            dataset_name: AbstractDataset.from_config(\n                name=dataset_name, config=datasets.get(dataset_name, {})\n            )\n            for dataset_name in inputs\n        }\n    )\n\n    outputs = outputs or []\n    self.outputs.update(\n        {\n            dataset_name: AbstractDataset.from_config(\n                name=dataset_name, config=datasets.get(dataset_name, {})\n            )\n            for dataset_name in outputs\n        }\n    )\n\n    for dataset_name in inputs:\n        if self.inputs[dataset_name].type == \"kafka\":\n            consumer_params = ConsumerParams(\n                **{\n                    \"dataset_name\": dataset_name,\n                    \"node_name\": self.name,\n                    \"pipeline_name\": self.pipeline_name,\n                    \"prefix\": prefix,\n                    \"has_pipeline_prefix\": has_pipeline_prefix,\n                    \"consumer_config\": DEFAULT_KAFKA_CONFIG.get(\n                        \"CONSUMER_CONFIG\"\n                    ),\n                }\n            )\n            self.inputs[dataset_name].initialize(\n                create=\"consumer\", connection_params=consumer_params\n            )\n    for dataset_name in outputs:\n        if self.outputs[dataset_name].type == \"kafka\":\n            producer_params = ProducerParams(\n                **{\n                    \"dataset_name\": dataset_name,\n                    \"node_name\": self.name,\n                    \"pipeline_name\": self.pipeline_name,\n                    \"prefix\": prefix,\n                    \"has_pipeline_prefix\": has_pipeline_prefix,\n                    \"producer_config\": DEFAULT_KAFKA_CONFIG.get(\n                        \"PRODUCER_CONFIG\"\n                    ),\n                }\n            )\n            self.outputs[dataset_name].initialize(\n                create=\"producer\", connection_params=producer_params\n            )\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.setup_test","title":"setup_test","text":"<pre><code>setup_test(\n    inputs: Optional[dict] = None,\n    outputs: Optional[list] = None,\n    params: Optional[dict] = None,\n) -&gt; None\n</code></pre> <p>Setup the node for testing.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Optional[dict]</code> <p>inputs to the node, format should be {\"dataset\": [1, 2, 3]}</p> <code>None</code> <code>outputs</code> <code>Optional[list]</code> <p>outputs of the node, format should be [\"dataset_1\", \"dataset_2\", ...]</p> <code>None</code> <code>params</code> <code>Optional[dict]</code> <p>dictionary of parameters to make accessible to _execute</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if node is not in test mode</p> Source code in <code>aineko/core/node.py</code> <pre><code>def setup_test(\n    self,\n    inputs: Optional[dict] = None,\n    outputs: Optional[list] = None,\n    params: Optional[dict] = None,\n) -&gt; None:\n    \"\"\"Setup the node for testing.\n\n    Args:\n        inputs: inputs to the node, format should be {\"dataset\": [1, 2, 3]}\n        outputs: outputs of the node, format should be [\"dataset_1\",\n            \"dataset_2\", ...]\n        params: dictionary of parameters to make accessible to _execute\n\n    Raises:\n        RuntimeError: if node is not in test mode\n    \"\"\"\n    if self.test is False:\n        raise RuntimeError(\n            \"Node is not in test mode. \"\n            \"Please initialize with `enable_test_mode()`.\"\n        )\n\n    inputs = inputs or {}\n\n    self.inputs = {\n        dataset_name: FakeKafka(\n            dataset_name=dataset_name,\n            node_name=self.__class__.__name__,\n            input_values=values,\n        )\n        for dataset_name, values in inputs.items()\n    }\n    outputs = outputs or []\n    outputs.extend(TESTING_NODE_CONFIG.get(\"DATASETS\"))\n\n    self.outputs = {\n        dataset_name: FakeKafka(\n            dataset_name=dataset_name,\n            node_name=self.__class__.__name__,\n        )\n        for dataset_name in outputs\n    }\n    self.params = params or {}\n</code></pre>"},{"location":"api_reference/config/","title":"Pipeline <code>Config</code>","text":"<p>The following Pydantic schema shows the format of a pipeline configuration file. Expand the following source code blocks to view the keys at each level.</p>"},{"location":"api_reference/config/#aineko.models.config_schema.Config","title":"aineko.models.config_schema.Config","text":"<p>Pipeline configuration model.</p> <p>Pipeline configurations are defined by the user in a YAML file. This model is a representation of a serialized pipeline configuration.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Config(BaseModel):\n    \"\"\"Pipeline configuration model.\n\n    Pipeline configurations are defined by the user in a YAML file. This model\n    is a representation of a serialized pipeline configuration.\n    \"\"\"\n\n    class Pipeline(BaseModel):\n        \"\"\"Pipeline model.\"\"\"\n\n        class Node(BaseModel):\n            \"\"\"Node model.\"\"\"\n\n            class_name: str = Field(..., alias=\"class\")\n            node_params: Optional[dict] = None\n            node_settings: Optional[NodeSettings] = None\n            inputs: Optional[List[str]] = None\n            outputs: Optional[List[str]] = None\n\n        name: str\n        default_node_settings: Optional[NodeSettings] = None\n        nodes: Dict[str, Node]\n        datasets: Dict[str, DatasetConfig]\n\n    pipeline: Pipeline\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.pipeline","title":"pipeline  <code>instance-attribute</code>","text":"<pre><code>pipeline: Pipeline\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline","title":"Pipeline","text":"<p>Pipeline model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Pipeline(BaseModel):\n    \"\"\"Pipeline model.\"\"\"\n\n    class Node(BaseModel):\n        \"\"\"Node model.\"\"\"\n\n        class_name: str = Field(..., alias=\"class\")\n        node_params: Optional[dict] = None\n        node_settings: Optional[NodeSettings] = None\n        inputs: Optional[List[str]] = None\n        outputs: Optional[List[str]] = None\n\n    name: str\n    default_node_settings: Optional[NodeSettings] = None\n    nodes: Dict[str, Node]\n    datasets: Dict[str, DatasetConfig]\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.datasets","title":"datasets  <code>instance-attribute</code>","text":"<pre><code>datasets: Dict[str, DatasetConfig]\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.default_node_settings","title":"default_node_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_node_settings: Optional[NodeSettings] = None\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.nodes","title":"nodes  <code>instance-attribute</code>","text":"<pre><code>nodes: Dict[str, Node]\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.Node","title":"Node","text":"<p>Node model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Node(BaseModel):\n    \"\"\"Node model.\"\"\"\n\n    class_name: str = Field(..., alias=\"class\")\n    node_params: Optional[dict] = None\n    node_settings: Optional[NodeSettings] = None\n    inputs: Optional[List[str]] = None\n    outputs: Optional[List[str]] = None\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.Node.class_name","title":"class_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_name: str = Field(..., alias='class')\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.Node.inputs","title":"inputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>inputs: Optional[List[str]] = None\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.Node.node_params","title":"node_params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>node_params: Optional[dict] = None\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.Node.node_settings","title":"node_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>node_settings: Optional[NodeSettings] = None\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.Node.outputs","title":"outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>outputs: Optional[List[str]] = None\n</code></pre>"},{"location":"api_reference/config_loader/","title":"<code>ConfigLoader</code>","text":"<p>Reference for the <code>ConfigLoader</code> class, which contains the logic used for parsing a pipeline configuration file into a format that can be understood by the <code>Runner</code>.</p>"},{"location":"api_reference/config_loader/#aineko.ConfigLoader","title":"aineko.ConfigLoader","text":"<pre><code>ConfigLoader(pipeline_config_file: str)\n</code></pre> <p>Class to read yaml config files.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_config_file</code> <code>str</code> <p>path of pipeline config file. Defaults to <code>DEFAULT_PIPELINE_CONFIG</code>.</p> required <p>Attributes:</p> Name Type Description <code>pipeline_config_file</code> <code>str</code> <p>path to the pipeline configuration file</p> <p>Methods:</p> Name Description <code>load_config</code> <p>loads and validates the pipeline config from a yaml file</p> <code>inject_env_vars</code> <p>injects environment variables into node params</p> <p>Initialize ConfigLoader.</p> Source code in <code>aineko/core/config_loader.py</code> <pre><code>def __init__(\n    self,\n    pipeline_config_file: str,\n):\n    \"\"\"Initialize ConfigLoader.\"\"\"\n    self.pipeline_config_file = pipeline_config_file or AINEKO_CONFIG.get(\n        \"DEFAULT_PIPELINE_CONFIG\"\n    )\n</code></pre>"},{"location":"api_reference/config_loader/#aineko.ConfigLoader.pipeline_config_file","title":"pipeline_config_file  <code>instance-attribute</code>","text":"<pre><code>pipeline_config_file = pipeline_config_file or get(\n    \"DEFAULT_PIPELINE_CONFIG\"\n)\n</code></pre>"},{"location":"api_reference/config_loader/#aineko.ConfigLoader.inject_env_vars","title":"inject_env_vars","text":"<pre><code>inject_env_vars(\n    node_params: Optional[\n        Union[Dict, List, str, int, float, bool]\n    ] = None\n) -&gt; Optional[Union[Dict, List, str, int, float, bool]]\n</code></pre> <p>Inject environment variables into node params.</p> <p>This function is used to recursively inject environment variables into strings passed through node params via the pipeline config. We only recursively parse strings, dicts, and lists, as these are the only types that can contain environment variables (i.e. excluding ints, floats, and Nones).</p> <p>Environment variables are identified in strings by the pattern {$ENV_VAR} where ENV_VAR is the name of the environment variable to inject. For example, given the following environment variables:</p> <pre><code>$ export SECRET1=secret1\n$ export SECRET2=secret2\n</code></pre> <p>The following node params dict:</p> <pre><code>```\n{\n    \"key1\": \"A string with a {$SECRET1} and a {$SECRET2}.\",\n    \"key2\": {\n        \"key3\": \"A string with a {$SECRET1} and a {$SECRET2}.\",\n        \"key4\": [\n            \"A string with a {$SECRET1} and a {$SECRET2}.\",\n            \"A string with a {$SECRET1} and a {$SECRET2}.\"\n        ]\n    }\n}\n```\n</code></pre> <p>Will be transformed to:</p> <pre><code>    ```\n    {\n        \"key1\": \"A string with a secret1 and a secret2.\",\n        \"key2\": {\n            \"key3\": \"A string with a secret1 and a secret2.\",\n            \"key4\": [\n                \"A string with a secret1 and a secret2.\",\n                \"A string with a secret1 and a secret2.\"\n            ]\n        }\n    }\n    ```\n</code></pre> Source code in <code>aineko/core/config_loader.py</code> <pre><code>def inject_env_vars(\n    self,\n    node_params: Optional[Union[Dict, List, str, int, float, bool]] = None,\n) -&gt; Optional[Union[Dict, List, str, int, float, bool]]:\n    \"\"\"Inject environment variables into node params.\n\n    This function is used to recursively inject environment variables\n    into strings passed through node params via the pipeline config.\n    We only recursively parse strings, dicts, and lists, as these are\n    the only types that can contain environment variables (i.e.\n    excluding ints, floats, and Nones).\n\n    Environment variables are identified in strings by the pattern\n    {$ENV_VAR} where ENV_VAR is the name of the environment variable\n    to inject. For example, given the following environment variables:\n\n    ```\n    $ export SECRET1=secret1\n    $ export SECRET2=secret2\n    ```\n\n    The following node params dict:\n\n        ```\n        {\n            \"key1\": \"A string with a {$SECRET1} and a {$SECRET2}.\",\n            \"key2\": {\n                \"key3\": \"A string with a {$SECRET1} and a {$SECRET2}.\",\n                \"key4\": [\n                    \"A string with a {$SECRET1} and a {$SECRET2}.\",\n                    \"A string with a {$SECRET1} and a {$SECRET2}.\"\n                ]\n            }\n        }\n        ```\n\n    Will be transformed to:\n\n            ```\n            {\n                \"key1\": \"A string with a secret1 and a secret2.\",\n                \"key2\": {\n                    \"key3\": \"A string with a secret1 and a secret2.\",\n                    \"key4\": [\n                        \"A string with a secret1 and a secret2.\",\n                        \"A string with a secret1 and a secret2.\"\n                    ]\n                }\n            }\n            ```\n    \"\"\"\n    if isinstance(node_params, dict):\n        for k, v in list(node_params.items()):\n            node_params[k] = self.inject_env_vars(v)\n    elif isinstance(node_params, list):\n        for i, v in enumerate(node_params):\n            node_params[i] = self.inject_env_vars(v)\n    elif isinstance(node_params, str):\n        env_var_pattern = r\"\\{\\$.*?\\}\"\n        env_var_match = re.search(env_var_pattern, node_params, re.DOTALL)\n        if env_var_match:\n            env_var_env_str = env_var_match.group()\n            env_var_value = os.getenv(\n                env_var_env_str[2:][:-1], default=None\n            )\n            if env_var_value is None:\n                raise ValueError(\n                    \"Failed to inject environment variable. \"\n                    f\"{env_var_env_str[2:][:-1]} was not found.\"\n                )\n            node_params = node_params.replace(\n                env_var_env_str, env_var_value\n            )\n            return self.inject_env_vars(node_params)\n\n    return node_params\n</code></pre>"},{"location":"api_reference/config_loader/#aineko.ConfigLoader.load_config","title":"load_config","text":"<pre><code>load_config() -&gt; Config\n</code></pre> <p>Load and validate the pipeline config.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the config does not match the schema</p> <p>Returns:</p> Type Description <code>Config</code> <p>The validated pipeline config as a Pydantic Config object</p> Source code in <code>aineko/core/config_loader.py</code> <pre><code>def load_config(self) -&gt; Config:\n    \"\"\"Load and validate the pipeline config.\n\n    Raises:\n        ValidationError: If the config does not match the schema\n\n    Returns:\n        The validated pipeline config as a Pydantic Config object\n    \"\"\"\n    raw_config = load_yaml(self.pipeline_config_file)\n\n    try:\n        config = Config(**raw_config)\n    except ValidationError as e:\n        logger.error(\n            \"Schema validation failed for pipeline `%s` loaded from %s. \"\n            \"See detailed error below.\",\n            raw_config[\"pipeline\"][\"name\"],\n            self.pipeline_config_file,\n        )\n        raise e\n\n    # Inject environment variables into node params\n    for node in config.pipeline.nodes.values():\n        if node.node_params is not None:\n            node.node_params = self.inject_env_vars(node.node_params)\n\n    return config\n</code></pre>"},{"location":"api_reference/kafka_dataset/","title":"<code>KafkaDataset</code>","text":"<p>The <code>KafkaDataset</code> class is a subclass of the <code>AbstractDataset</code> class.</p>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset","title":"aineko.datasets.kafka.KafkaDataset","text":"<pre><code>KafkaDataset(name: str, params: Dict[str, Any])\n</code></pre> <p>             Bases: <code>AbstractDataset</code></p> <p>Kafka dataset.</p> <p>Dataset Storage Layer is a Kafka topic.</p> <p>Dataset Query Layer is a Kafka Consumer and Producer.</p> <p><code>read</code> method consumes from a Kakfa topic.</p> <p><code>write</code> method produces to a Kafka topic.</p> <p><code>create</code> method creates the dataset topic in the Kafka cluster.</p> <p><code>initialize</code> method can be used to create a consumer or producer.</p> <p><code>delete</code> method deletes the dataset topic in the Kafka cluster.</p> <p><code>exists</code> method checks if the dataset topic exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the dataset</p> required <code>params</code> <code>Dict[str, Any]</code> <p>dataset configuration parameters</p> required <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>name of the dataset</p> <code>topic_name</code> <code>str</code> <p>name of the Kafka topic</p> <code>params</code> <code>dict</code> <p>dataset configuration parameters</p> <code>type</code> <code>str</code> <p>type of the dataset</p> <code>credentials</code> <code>KafkaCredentials</code> <p>Kafka credentials</p> <code>dataset_config</code> <code>dict</code> <p>dataset configuration</p> <code>_consumer</code> <code>Consumer</code> <p>Kafka consumer</p> <code>_producer</code> <code>Producer</code> <p>Kafka producer</p> <code>_admin_client</code> <code>AdminClient</code> <p>Kafka AdminClient</p> <code>cached</code> <code>bool</code> <p>True if the consumer has been polled, False otherwise</p> <code>location</code> <code>str</code> <p>location of the dataset</p> <code>consumer_name</code> <code>str</code> <p>name of the consumer</p> <p>Raises:</p> Type Description <code>KafkaDatasetError</code> <p>if an error occurs while creating the dataset</p> <p>Initialize the dataset.</p> Source code in <code>aineko/datasets/kafka.py</code> <pre><code>def __init__(self, name: str, params: Dict[str, Any]):\n    \"\"\"Initialize the dataset.\"\"\"\n    self.name = name\n    self.topic_name = name\n    self.params = params\n    self.type = \"kafka\"\n    self.consumer_name: Optional[str] = None\n    self.credentials = KafkaCredentials(\n        **params.get(\"kafka_credentials\", {})\n    )\n    self.dataset_config = params\n    self.location = self._update_location()\n    self.cached = False\n    self.source_node: str\n    self.source_pipeline: str\n    self._consumer: Consumer\n    self._producer: Producer\n    self._create_admin_client()\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.cached","title":"cached  <code>instance-attribute</code>","text":"<pre><code>cached = False\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.consumer_name","title":"consumer_name  <code>instance-attribute</code>","text":"<pre><code>consumer_name: Optional[str] = None\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.credentials","title":"credentials  <code>instance-attribute</code>","text":"<pre><code>credentials = KafkaCredentials(\n    **get(\"kafka_credentials\", {})\n)\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.dataset_config","title":"dataset_config  <code>instance-attribute</code>","text":"<pre><code>dataset_config = params\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.location","title":"location  <code>instance-attribute</code>","text":"<pre><code>location = _update_location()\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params = params\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.source_node","title":"source_node  <code>instance-attribute</code>","text":"<pre><code>source_node: str\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.source_pipeline","title":"source_pipeline  <code>instance-attribute</code>","text":"<pre><code>source_pipeline: str\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.topic_name","title":"topic_name  <code>instance-attribute</code>","text":"<pre><code>topic_name = name\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type = 'kafka'\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.consume_all","title":"consume_all","text":"<pre><code>consume_all(end_message: Union[str, bool] = False) -&gt; list\n</code></pre> <p>Reads all messages from the dataset until a specific one is found.</p> <p>Parameters:</p> Name Type Description Default <code>end_message</code> <code>Union[str, bool]</code> <p>Message to trigger the completion of consumption</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>list of messages from the dataset</p> Source code in <code>aineko/datasets/kafka.py</code> <pre><code>def consume_all(self, end_message: Union[str, bool] = False) -&gt; list:\n    \"\"\"Reads all messages from the dataset until a specific one is found.\n\n    Args:\n        end_message: Message to trigger the completion of consumption\n\n    Returns:\n        list of messages from the dataset\n    \"\"\"\n    messages = []\n    while True:\n        message = self._consume()\n        if message is None:\n            continue\n        if message[\"message\"] == end_message:\n            break\n        messages.append(message)\n    return messages\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.create","title":"create","text":"<pre><code>create(\n    topic_params: TopicParams = TopicParams(),\n) -&gt; DatasetCreateStatus\n</code></pre> <p>Create the dataset storage layer kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic_params</code> <code>TopicParams</code> <p>initialization parameters for the dataset topic</p> <code>TopicParams()</code> <p>Returns:</p> Type Description <code>DatasetCreateStatus</code> <p>status of dataset creation.</p> Source code in <code>aineko/datasets/kafka.py</code> <pre><code>def create(\n    self,\n    topic_params: TopicParams = TopicParams(),\n) -&gt; DatasetCreateStatus:\n    \"\"\"Create the dataset storage layer kafka topic.\n\n    Args:\n        topic_params: initialization parameters for the dataset topic\n\n    Returns:\n      status of dataset creation.\n    \"\"\"\n    return self._create_topic(\n        dataset_name=self.name, topic_params=topic_params\n    )\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.delete","title":"delete","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Delete the dataset topic from the Kafka cluster.</p> <p>Raises:</p> Type Description <code>KafkaDatasetError</code> <p>if an error occurs while deleting the topic</p> Source code in <code>aineko/datasets/kafka.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"Delete the dataset topic from the Kafka cluster.\n\n    Raises:\n        KafkaDatasetError: if an error occurs while deleting the topic\n    \"\"\"\n    try:\n        self._admin_client.delete_topics([self.topic_name])\n    except Exception as err:\n        raise KafkaDatasetError(\n            f\"Error deleting topic {self.topic_name}: {str(err)}\"\n        ) from err\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.exists","title":"exists","text":"<pre><code>exists() -&gt; bool\n</code></pre> <p>Check if the dataset exists.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the dataset topic exists, False otherwise</p> Source code in <code>aineko/datasets/kafka.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Check if the dataset exists.\n\n    Returns:\n        True if the dataset topic exists, False otherwise\n    \"\"\"\n    return self.topic_name in self._admin_client.list_topics().topics\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.initialize","title":"initialize","text":"<pre><code>initialize(\n    connection_params: Union[\n        ConsumerParams, ProducerParams\n    ],\n    create: Literal[\"consumer\", \"producer\"],\n) -&gt; None\n</code></pre> <p>Create query layer reader or writer for the dataset.</p> <p>This method can be called in 2 different ways:</p> <pre><code>1. `self.initialize(create=\"consumer\",\nconnection_params=ConsumerParams(...))`:\n    creates a Kafka Consumer and subscribes to the\n    dataset topic.\n\n2. `self.initialize(create=\"producer\",\nconnection_params=ProducerParams(...)`:\n    creates a Kafka Producer.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>create</code> <code>Literal['consumer', 'producer']</code> <p>if \"consumer\", create a Kafka Consumer and subscribe to the dataset topic. If \"producer\", create a Kafka Producer</p> required <code>connection_params</code> <code>Union[ConsumerParams, ProducerParams]</code> <p>connection parameters for the dataset</p> required <p>Raises:</p> Type Description <code>KafkaDatasetError</code> <p>if an error occurs while creating the consumer or producer</p> Source code in <code>aineko/datasets/kafka.py</code> <pre><code>def initialize(\n    self,\n    connection_params: Union[ConsumerParams, ProducerParams],\n    create: Literal[\"consumer\", \"producer\"],\n) -&gt; None:\n    \"\"\"Create query layer reader or writer for the dataset.\n\n    This method can be called in 2 different ways:\n\n        1. `self.initialize(create=\"consumer\",\n        connection_params=ConsumerParams(...))`:\n            creates a Kafka Consumer and subscribes to the\n            dataset topic.\n\n        2. `self.initialize(create=\"producer\",\n        connection_params=ProducerParams(...)`:\n            creates a Kafka Producer.\n\n    Args:\n        create: if \"consumer\", create a Kafka Consumer and\n            subscribe to the dataset topic. If \"producer\",\n            create a Kafka Producer\n        connection_params: connection parameters for the dataset\n\n    Raises:\n        KafkaDatasetError: if an error occurs while creating the consumer\n            or producer\n    \"\"\"\n    if create == \"consumer\":\n        try:\n            if not isinstance(connection_params, ConsumerParams):\n                raise KafkaDatasetError(\n                    \"Invalid connection_params for creating consumer.\"\n                )\n            self._create_consumer(consumer_params=connection_params)\n            logger.info(\"Consumer for %s created.\", self.topic_name)\n        except KafkaError as err:\n            raise KafkaDatasetError(\n                f\"Error creating consumer for {self.topic_name}: {str(err)}\"\n            ) from err\n        return\n    elif create == \"producer\":\n        try:\n            if not isinstance(connection_params, ProducerParams):\n                raise KafkaDatasetError(\n                    \"Invalid connection_params for creating producer.\"\n                )\n            self._create_producer(producer_params=connection_params)\n            logger.info(\"Producer for %s created.\", self.topic_name)\n        except KafkaError as err:\n            raise KafkaDatasetError(\n                f\"Error creating producer for {self.topic_name}: {str(err)}\"\n            ) from err\n        return\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.last","title":"last","text":"<pre><code>last(timeout: int = 1) -&gt; Dict\n</code></pre> <p>Consumes the last message from the dataset.</p> <p>Wraps the <code>_consume_message(how=\"last\")</code> method. It implements a block that waits until a message is received before returning it. This method ensures that the consumed message is always the most recent message. If the consumer is slower than the producer, messages might be skipped. If the consumer is faster than the producer, messages might be repeated.</p> <p>This is useful when the timeout is short and you expect the consumer to often return <code>None</code>.</p> <p>Note: The timeout must be greater than 0 to prevent overwhelming the broker with requests to update the offset.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>seconds to poll for a response from kafka broker. Must be &gt;0.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dict</code> <p>message from the dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if timeout is &lt;= 0</p> Source code in <code>aineko/datasets/kafka.py</code> <pre><code>def last(self, timeout: int = 1) -&gt; Dict:\n    \"\"\"Consumes the last message from the dataset.\n\n    Wraps the `_consume_message(how=\"last\")` method. It implements a\n    block that waits until a message is received before returning it.\n    This method ensures that the consumed message is always the most\n    recent message. If the consumer is slower than the producer, messages\n    might be skipped. If the consumer is faster than the producer,\n    messages might be repeated.\n\n    This is useful when the timeout is short and you expect the consumer\n    to often return `None`.\n\n    Note: The timeout must be greater than 0 to prevent\n    overwhelming the broker with requests to update the offset.\n\n    Args:\n        timeout: seconds to poll for a response from kafka broker.\n            Must be &gt;0.\n\n    Returns:\n        message from the dataset\n\n    Raises:\n        ValueError: if timeout is &lt;= 0\n    \"\"\"\n    if timeout &lt;= 0:\n        raise ValueError(\n            \"Timeout must be &gt; 0 when consuming the last message.\"\n        )\n    return self._consume_message(how=\"last\", timeout=timeout)\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.next","title":"next","text":"<pre><code>next() -&gt; Dict\n</code></pre> <p>Consumes the next message from the dataset.</p> <p>Wraps the <code>_consume_message(how=\"next\")</code> method. It implements a block that waits until a message is received before returning it. This method ensures that every message is consumed, but the consumed message may not be the most recent message if the consumer is slower than the producer.</p> <p>This is useful when the timeout is short and you expect the consumer to often return <code>None</code>.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>message from the dataset</p> Source code in <code>aineko/datasets/kafka.py</code> <pre><code>def next(self) -&gt; Dict:\n    \"\"\"Consumes the next message from the dataset.\n\n    Wraps the `_consume_message(how=\"next\")` method. It implements a\n    block that waits until a message is received before returning it.\n    This method ensures that every message is consumed, but the consumed\n    message may not be the most recent message if the consumer is slower\n    than the producer.\n\n    This is useful when the timeout is short and you expect the consumer\n    to often return `None`.\n\n    Returns:\n        message from the dataset\n    \"\"\"\n    return self._consume_message(how=\"next\")\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.read","title":"read","text":"<pre><code>read(\n    how: Literal[\"next\", \"last\"],\n    timeout: Optional[float] = None,\n    block: bool = False,\n) -&gt; Optional[Dict]\n</code></pre> <p>Read the dataset message via the query layer.</p> <p>Parameters:</p> Name Type Description Default <code>how</code> <code>Literal['next', 'last']</code> <p>how to read the message \"next\": read the next message in the queue \":last\": read the last message in the queue</p> required <code>timeout</code> <code>Optional[float]</code> <p>seconds to poll for a response from kafka broker. If using how=\"last\", set to bigger than 0.</p> <code>None</code> <code>block</code> <code>bool</code> <p>if True, block until a message is received</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[Dict]</code> <p>message from the dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if how is not \"next\" or \"last\"</p> Source code in <code>aineko/datasets/kafka.py</code> <pre><code>def read(\n    self,\n    how: Literal[\"next\", \"last\"],\n    timeout: Optional[float] = None,\n    block: bool = False,\n) -&gt; Optional[Dict]:\n    \"\"\"Read the dataset message via the query layer.\n\n    Args:\n        how: how to read the message\n            \"next\": read the next message in the queue\n            \":last\": read the last message in the queue\n        timeout: seconds to poll for a response from kafka broker.\n            If using how=\"last\", set to bigger than 0.\n        block: if True, block until a message is received\n\n    Returns:\n        message from the dataset\n\n    Raises:\n        ValueError: if how is not \"next\" or \"last\"\n    \"\"\"\n    if block:\n        return self._consume_message(how=how, timeout=timeout)\n    else:\n        return self._consume(how=how, timeout=timeout)\n</code></pre>"},{"location":"api_reference/kafka_dataset/#aineko.datasets.kafka.KafkaDataset.write","title":"write","text":"<pre><code>write(msg: Dict, key: Optional[str] = None) -&gt; None\n</code></pre> <p>Produce a message to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Dict</code> <p>message to produce to the dataset</p> required <code>key</code> <code>Optional[str]</code> <p>key to use for the message</p> <code>None</code> <p>Raises:</p> Type Description <code>KafkaDatasetError</code> <p>if an error occurs while writing to the topic</p> Source code in <code>aineko/datasets/kafka.py</code> <pre><code>def write(self, msg: Dict, key: Optional[str] = None) -&gt; None:\n    \"\"\"Produce a message to the dataset.\n\n    Args:\n        msg: message to produce to the dataset\n        key: key to use for the message\n\n    Raises:\n        KafkaDatasetError: if an error occurs while writing to the topic\n    \"\"\"\n    # Note, this will be re-written to use the dataset's schema,\n    # without added metadata.\n    message = {\n        \"timestamp\": datetime.datetime.now().strftime(\n            AINEKO_CONFIG.get(\"MSG_TIMESTAMP_FORMAT\")\n        ),\n        \"dataset\": self.name,\n        \"source_pipeline\": self.source_pipeline,\n        \"source_node\": self.source_node,\n        \"message\": msg,\n    }\n    self._producer.poll(0)\n\n    key_bytes = str(key).encode(\"utf-8\") if key is not None else None\n\n    self._producer.produce(\n        topic=self.topic_name,\n        key=key_bytes,\n        value=json.dumps(message).encode(\"utf-8\"),\n        callback=self._delivery_report,\n    )\n    self._producer.flush()\n</code></pre>"},{"location":"api_reference/models/","title":"Internal Models","text":"<p>This section describes the models used internally by Aineko to serialize and deserialize user configurations.</p>"},{"location":"api_reference/models/#pipeline-configuration","title":"Pipeline Configuration","text":""},{"location":"api_reference/models/#aineko.models.config_schema.Config","title":"aineko.models.config_schema.Config","text":"<p>Pipeline configuration model.</p> <p>Pipeline configurations are defined by the user in a YAML file. This model is a representation of a serialized pipeline configuration.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Config(BaseModel):\n    \"\"\"Pipeline configuration model.\n\n    Pipeline configurations are defined by the user in a YAML file. This model\n    is a representation of a serialized pipeline configuration.\n    \"\"\"\n\n    class Pipeline(BaseModel):\n        \"\"\"Pipeline model.\"\"\"\n\n        class Node(BaseModel):\n            \"\"\"Node model.\"\"\"\n\n            class_name: str = Field(..., alias=\"class\")\n            node_params: Optional[dict] = None\n            node_settings: Optional[NodeSettings] = None\n            inputs: Optional[List[str]] = None\n            outputs: Optional[List[str]] = None\n\n        name: str\n        default_node_settings: Optional[NodeSettings] = None\n        nodes: Dict[str, Node]\n        datasets: Dict[str, DatasetConfig]\n\n    pipeline: Pipeline\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.pipeline","title":"pipeline  <code>instance-attribute</code>","text":"<pre><code>pipeline: Pipeline\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.Pipeline","title":"Pipeline","text":"<p>Pipeline model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Pipeline(BaseModel):\n    \"\"\"Pipeline model.\"\"\"\n\n    class Node(BaseModel):\n        \"\"\"Node model.\"\"\"\n\n        class_name: str = Field(..., alias=\"class\")\n        node_params: Optional[dict] = None\n        node_settings: Optional[NodeSettings] = None\n        inputs: Optional[List[str]] = None\n        outputs: Optional[List[str]] = None\n\n    name: str\n    default_node_settings: Optional[NodeSettings] = None\n    nodes: Dict[str, Node]\n    datasets: Dict[str, DatasetConfig]\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.Pipeline.datasets","title":"datasets  <code>instance-attribute</code>","text":"<pre><code>datasets: Dict[str, DatasetConfig]\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.Pipeline.default_node_settings","title":"default_node_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_node_settings: Optional[NodeSettings] = None\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.Pipeline.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.Pipeline.nodes","title":"nodes  <code>instance-attribute</code>","text":"<pre><code>nodes: Dict[str, Node]\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.Pipeline.Node","title":"Node","text":"<p>Node model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Node(BaseModel):\n    \"\"\"Node model.\"\"\"\n\n    class_name: str = Field(..., alias=\"class\")\n    node_params: Optional[dict] = None\n    node_settings: Optional[NodeSettings] = None\n    inputs: Optional[List[str]] = None\n    outputs: Optional[List[str]] = None\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.Pipeline.Node.class_name","title":"class_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_name: str = Field(..., alias='class')\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.Pipeline.Node.inputs","title":"inputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>inputs: Optional[List[str]] = None\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.Pipeline.Node.node_params","title":"node_params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>node_params: Optional[dict] = None\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.Pipeline.Node.node_settings","title":"node_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>node_settings: Optional[NodeSettings] = None\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.Config.Pipeline.Node.outputs","title":"outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>outputs: Optional[List[str]] = None\n</code></pre>"},{"location":"api_reference/models/#node-configuration","title":"Node Configuration","text":""},{"location":"api_reference/models/#aineko.models.config_schema.NodeSettings","title":"aineko.models.config_schema.NodeSettings","text":"<p>Node settings model.</p> <p>This model is used to define settings for a node. The main use case is to define the number of CPUs to use for a node. However, it can be extended to include all Ray remote parameters as described in the Ray documentation.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class NodeSettings(BaseModel):\n    \"\"\"Node settings model.\n\n    This model is used to define settings for a node. The main use case is to\n    define the number of CPUs to use for a node. However, it can be extended to\n    include all Ray remote parameters as described in the [Ray documentation.](\n    https://docs.ray.io/en/latest/ray-core/api/doc/ray.remote.html#ray.remote/)\n    {:target=\"_blank\"}\n    \"\"\"\n\n    num_cpus: Optional[float] = Field(\n        None,\n        description=\"The number of CPUs to use for a node.\",\n        gt=0.0,\n        examples=[1.0, 0.5],\n    )\n    model_config = {\"extra\": \"allow\"}\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.NodeSettings.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = {'extra': 'allow'}\n</code></pre>"},{"location":"api_reference/models/#aineko.models.config_schema.NodeSettings.num_cpus","title":"num_cpus  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_cpus: Optional[float] = Field(\n    None,\n    description=\"The number of CPUs to use for a node.\",\n    gt=0.0,\n    examples=[1.0, 0.5],\n)\n</code></pre>"},{"location":"api_reference/models/#dataset-configuration","title":"Dataset Configuration","text":""},{"location":"api_reference/models/#aineko.models.dataset_config_schema.DatasetConfig","title":"aineko.models.dataset_config_schema.DatasetConfig","text":"<p>Dataset configuration model.</p> Source code in <code>aineko/models/dataset_config_schema.py</code> <pre><code>class DatasetConfig(BaseModel):\n    \"\"\"Dataset configuration model.\"\"\"\n\n    type: str = Field(\n        ...,\n        description=\"A dotted path to the dataset class implementation.\",\n        examples=[\n            \"aineko.datasets.kafka.KafkaDataset\",\n            \"foo.bar.baz.BazDataset\",\n        ],\n    )\n    location: Optional[str] = Field(\n        None,\n        description=(\n            \"Location of the dataset storage layer. For example, a kafka \"\n            \"broker address.\"\n        ),\n        examples=[\"localhost:9092\"],\n    )\n    params: Optional[Dict[str, Any]] = Field(\n        None,\n        description=\"The initialization parameters for the dataset.\",\n        examples=[{\"param_1\": \"bar\"}],\n    )\n</code></pre>"},{"location":"api_reference/models/#aineko.models.dataset_config_schema.DatasetConfig.location","title":"location  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>location: Optional[str] = Field(\n    None,\n    description=\"Location of the dataset storage layer. For example, a kafka broker address.\",\n    examples=[\"localhost:9092\"],\n)\n</code></pre>"},{"location":"api_reference/models/#aineko.models.dataset_config_schema.DatasetConfig.params","title":"params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>params: Optional[Dict[str, Any]] = Field(\n    None,\n    description=\"The initialization parameters for the dataset.\",\n    examples=[{\"param_1\": \"bar\"}],\n)\n</code></pre>"},{"location":"api_reference/models/#aineko.models.dataset_config_schema.DatasetConfig.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: str = Field(\n    ...,\n    description=\"A dotted path to the dataset class implementation.\",\n    examples=[\n        \"aineko.datasets.kafka.KafkaDataset\",\n        \"foo.bar.baz.BazDataset\",\n    ],\n)\n</code></pre>"},{"location":"api_reference/runner/","title":"<code>Runner</code>","text":"<p>The <code>Runner</code> builds the pipeline from the given pipeline configuration file, creating the necessary datasets and orchestrating the nodes.</p>"},{"location":"api_reference/runner/#aineko.Runner","title":"aineko.Runner","text":"<pre><code>Runner(\n    pipeline_config_file: str,\n    pipeline_name: Optional[str] = None,\n    kafka_config: dict = DEFAULT_KAFKA_CONFIG.get(\n        \"BROKER_CONFIG\"\n    ),\n    metrics_export_port: int = AINEKO_CONFIG.get(\n        \"RAY_METRICS_PORT\"\n    ),\n    dataset_prefix: Optional[str] = None,\n)\n</code></pre> <p>Runs the pipeline described in the config.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_config_file</code> <code>str</code> <p>Path to pipeline config file</p> required <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline</p> <code>None</code> <code>kafka_config</code> <code>dict</code> <p>Config for kafka broker</p> <code>get('BROKER_CONFIG')</code> <code>dataset_prefix</code> <code>Optional[str]</code> <p>Prefix for dataset names. Kafka topics will be called <code>&lt;prefix&gt;.&lt;pipeline&gt;.&lt;dataset_name&gt;</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>pipeline_config_file</code> <code>str</code> <p>Path to pipeline config file</p> <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline, overrides pipeline config</p> <code>kafka_config</code> <code>dict</code> <p>Config for kafka broker</p> <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline, loaded from config</p> <code>dataset_prefix</code> <code>Optional[str]</code> <p>Prefix for dataset names</p> <p>Initializes the runner class.</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def __init__(\n    self,\n    pipeline_config_file: str,\n    pipeline_name: Optional[str] = None,\n    kafka_config: dict = DEFAULT_KAFKA_CONFIG.get(\"BROKER_CONFIG\"),\n    metrics_export_port: int = AINEKO_CONFIG.get(\"RAY_METRICS_PORT\"),\n    dataset_prefix: Optional[str] = None,\n):\n    \"\"\"Initializes the runner class.\"\"\"\n    self.pipeline_config_file = pipeline_config_file\n    self.kafka_config = kafka_config\n    self.metrics_export_port = metrics_export_port\n    self.pipeline_name = pipeline_name\n    self.dataset_prefix = dataset_prefix or \"\"\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.dataset_prefix","title":"dataset_prefix  <code>instance-attribute</code>","text":"<pre><code>dataset_prefix = dataset_prefix or ''\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.kafka_config","title":"kafka_config  <code>instance-attribute</code>","text":"<pre><code>kafka_config = kafka_config\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.metrics_export_port","title":"metrics_export_port  <code>instance-attribute</code>","text":"<pre><code>metrics_export_port = metrics_export_port\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.pipeline_config_file","title":"pipeline_config_file  <code>instance-attribute</code>","text":"<pre><code>pipeline_config_file = pipeline_config_file\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.pipeline_name","title":"pipeline_name  <code>instance-attribute</code>","text":"<pre><code>pipeline_name = pipeline_name\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.load_pipeline_config","title":"load_pipeline_config","text":"<pre><code>load_pipeline_config() -&gt; Pipeline\n</code></pre> <p>Loads the config for a given pipeline.</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>pipeline config</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def load_pipeline_config(self) -&gt; Config.Pipeline:\n    \"\"\"Loads the config for a given pipeline.\n\n    Returns:\n        pipeline config\n    \"\"\"\n    config = ConfigLoader(\n        pipeline_config_file=self.pipeline_config_file,\n    ).load_config()\n\n    return config.pipeline\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.prepare_datasets","title":"prepare_datasets","text":"<pre><code>prepare_datasets(\n    config: Dict, user_dataset_prefix: Optional[str] = None\n) -&gt; List[AbstractDataset]\n</code></pre> <p>Creates the required datasets for a given pipeline.</p> <p>Datasets can be configured using the <code>params</code> key, using config keys found in: https://kafka.apache.org/documentation.html#topicconfigs</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict</code> <p>dataset configuration found in pipeline config Should follow the schema below: <pre><code>    {\n        \"dataset_name\": {\n            \"type\": str (\"aineko.datasets.kafka.KafkaDataset\"),\n            \"location\": str (\"localhost:9092\"),\n            \"params\": dict\n    }\n</code></pre></p> required <code>user_dataset_prefix</code> <code>Optional[str]</code> <p>prefix only for datasets defined by the user. i.e. <code>&lt;prefix&gt;.&lt;user_dataset_prefix&gt;.&lt;dataset_name&gt;</code></p> <code>None</code> <p>Returns:</p> Type Description <code>List[AbstractDataset]</code> <p>True if successful</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if dataset \"logging\" is defined in the catalog</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def prepare_datasets(\n    self, config: Dict, user_dataset_prefix: Optional[str] = None\n) -&gt; List[AbstractDataset]:\n    \"\"\"Creates the required datasets for a given pipeline.\n\n    Datasets can be configured using the `params` key, using config keys\n    found in: https://kafka.apache.org/documentation.html#topicconfigs\n\n    Args:\n        config: dataset configuration found in pipeline config\n            Should follow the schema below:\n            ```python\n                {\n                    \"dataset_name\": {\n                        \"type\": str (\"aineko.datasets.kafka.KafkaDataset\"),\n                        \"location\": str (\"localhost:9092\"),\n                        \"params\": dict\n                }\n            ```\n        user_dataset_prefix: prefix only for datasets defined by the user.\n            i.e. `&lt;prefix&gt;.&lt;user_dataset_prefix&gt;.&lt;dataset_name&gt;`\n\n    Returns:\n        True if successful\n\n    Raises:\n        ValueError: if dataset \"logging\" is defined in the catalog\n    \"\"\"\n    # Create all configured dataset objects\n    datasets = []\n    if user_dataset_prefix:\n        config = {\n            f\"{user_dataset_prefix}.{dataset_name}\": dataset_config\n            for dataset_name, dataset_config in config.items()\n        }\n\n    for reserved_dataset in DEFAULT_KAFKA_CONFIG.get(\"DATASETS\"):\n        if reserved_dataset in config:\n            raise ValueError(\n                f\"Unable to create dataset `{reserved_dataset}`. \"\n                \"Reserved for internal use.\"\n            )\n\n    for dataset_name, dataset_config in config.items():\n        logger.info(\n            \"Creating dataset: %s: %s\", dataset_name, dataset_config\n        )\n        # update dataset config here:\n        dataset: AbstractDataset = AbstractDataset.from_config(\n            dataset_name, dataset_config\n        )\n        datasets.append(dataset)\n\n    # Create logging dataset\n    logging_config = {\n        \"location\": \"localhost:9092\",\n        \"type\": \"aineko.datasets.kafka.KafkaDataset\",\n    }\n    logging_dataset_name = DEFAULT_KAFKA_CONFIG.get(\"LOGGING_DATASET\")\n    logger.info(\n        \"Creating dataset: %s: %s\", logging_dataset_name, logging_config\n    )\n    logging_dataset: AbstractDataset = AbstractDataset.from_config(\n        logging_dataset_name, logging_config\n    )\n    # Create all datasets\n    dataset_create_status = [\n        dataset.create(\n            topic_params=TopicParams(dataset_prefix=self.dataset_prefix),\n        )\n        for dataset in datasets\n    ]\n    logging_create_status = logging_dataset.create(\n        topic_params=TopicParams()\n    )\n    datasets.append(logging_dataset)\n\n    dataset_create_status.append(logging_create_status)\n    cur_time = time.time()\n    while True:\n        if all(future.done() for future in dataset_create_status):\n            logger.info(\"All datasets created.\")\n            break\n        if time.time() - cur_time &gt; AINEKO_CONFIG.get(\n            \"DATASET_CREATION_TIMEOUT\"\n        ):\n            raise TimeoutError(\"Timeout while creating datasets.\")\n    return datasets\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.prepare_nodes","title":"prepare_nodes","text":"<pre><code>prepare_nodes(\n    pipeline_config: Pipeline, poison_pill: ActorHandle\n) -&gt; list\n</code></pre> <p>Prepare actor handles for all nodes.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_config</code> <code>Pipeline</code> <p>pipeline configuration</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>list</code> <p>mapping of node names to actor handles</p> <code>list</code> <code>list</code> <p>list of ray objects</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if error occurs while initializing actor from config</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def prepare_nodes(\n    self,\n    pipeline_config: Config.Pipeline,\n    poison_pill: ray.actor.ActorHandle,\n) -&gt; list:\n    \"\"\"Prepare actor handles for all nodes.\n\n    Args:\n        pipeline_config: pipeline configuration\n\n    Returns:\n        dict: mapping of node names to actor handles\n        list: list of ray objects\n\n    Raises:\n        ValueError: if error occurs while initializing actor from config\n    \"\"\"\n    # Collect all  actor futures\n    results = []\n    for node_name, node_config in pipeline_config.nodes.items():\n        try:\n            target_class = imports.import_from_string(\n                attr=node_config.class_name, kind=\"class\"\n            )\n        except AttributeError as exc:\n            raise ValueError(\n                \"Invalid node class name specified in config for node '\"\n                f\"{node_name}'. Please check your config file at: \"\n                f\"{self.pipeline_config_file}\\n\"\n                f\"Error: {exc}\"\n            ) from None\n\n        actor_params = {\n            \"name\": node_name,\n            \"namespace\": self.pipeline_name,\n        }\n        if pipeline_config.default_node_settings:\n            actor_params.update(\n                pipeline_config.default_node_settings.model_dump(\n                    exclude_none=True\n                )\n            )\n        # Update actor params with node specific settings to override\n        # default settings\n        if node_config.node_settings:\n            actor_params.update(\n                node_config.node_settings.model_dump(exclude_none=True)\n            )\n\n        wrapped_class = ray.remote(target_class)\n        wrapped_class.options(**actor_params)\n        actor_handle = wrapped_class.remote(\n            node_name=node_name,\n            pipeline_name=self.pipeline_name,\n            poison_pill=poison_pill,\n        )\n\n        # Setup input and output datasets\n        actor_handle.setup_datasets.remote(\n            inputs=node_config.inputs,\n            outputs=node_config.outputs,\n            datasets=pipeline_config.datasets,\n            has_pipeline_prefix=True,\n        )\n\n        # Setup internal datasets like logging, without pipeline prefix\n        logging_dataset_name = DEFAULT_KAFKA_CONFIG.get(\"LOGGING_DATASET\")\n        actor_handle.setup_datasets.remote(\n            outputs=DEFAULT_KAFKA_CONFIG.get(\"DATASETS\"),\n            datasets={\n                logging_dataset_name: {\n                    \"type\": \"aineko.datasets.kafka.KafkaDataset\",\n                    \"location\": \"localhost:9092\",\n                }\n            },\n        )\n\n        # Create actor future (for execute method)\n        results.append(\n            actor_handle.execute.remote(params=node_config.node_params)\n        )\n    return results\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Runs the pipeline.</p> <p>Step 1: Load config for pipeline</p> <p>Step 2: Set up datasets</p> <p>Step 3: Set up PoisonPill node that is available to all nodes</p> <p>Step 4: Set up nodes (including Node Manager)</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Runs the pipeline.\n\n    Step 1: Load config for pipeline\n\n    Step 2: Set up datasets\n\n    Step 3: Set up PoisonPill node that is available to all nodes\n\n    Step 4: Set up nodes (including Node Manager)\n    \"\"\"\n    # Load pipeline config\n    pipeline_config = self.load_pipeline_config()\n    self.pipeline_name = self.pipeline_name or pipeline_config.name\n\n    # Create the necessary datasets\n    self.prepare_datasets(\n        config=pipeline_config.datasets,\n        user_dataset_prefix=self.pipeline_name,\n    )\n\n    # Initialize ray cluster\n    ray.shutdown()\n    ray.init(\n        namespace=self.pipeline_name,\n        _metrics_export_port=self.metrics_export_port,\n    )\n\n    # Create poison pill actor\n    poison_pill = ray.remote(PoisonPill).remote()\n\n    # Add Node Manager to pipeline config\n    pipeline_config.nodes[\n        NODE_MANAGER_CONFIG.get(\"NAME\")\n    ] = Config.Pipeline.Node(**NODE_MANAGER_CONFIG.get(\"NODE_CONFIG\"))\n\n    # Create each node (actor)\n    results = self.prepare_nodes(\n        pipeline_config=pipeline_config,\n        poison_pill=poison_pill,  # type: ignore\n    )\n\n    ray.get(results)\n</code></pre>"},{"location":"community/","title":"Contributing to Aineko","text":"<p>Thank you for your interest in contributing to Aineko!</p> <p>Here are the steps to get started quickly:</p>"},{"location":"community/#install-aineko-from-source","title":"Install Aineko from source","text":"First, make sure you have poetry installed on your system if not already installed. <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> Then, install the source code for Aineko on your local system. <pre><code>git clone https://github.com/aineko-dev/aineko\ncd aineko &amp;&amp; poetry install --with dev,test,docs\n</code></pre>"},{"location":"community/#make-your-changes-to-aineko-source-code","title":"Make your changes to Aineko source code","text":"<p>Update Aineko on your local system.</p>"},{"location":"community/#test-using-aineko-pipeline","title":"Test using Aineko pipeline","text":"<p>We highly encourage you to validate your changes by testing the project creation process end-to-end. This means validating the changes by running a local pipeline that uses your local Aineko repository.</p> First, update poetry to use your local Aineko repository. <pre><code>poetry lock\npoetry install\n</code></pre> Next, create an Aineko project in the parent directory. <pre><code>poetry run aineko create --output-dir ../\n</code></pre> Next, update the create Aineko project to use the local Aineko repository. Go to <code>../my-awesome-pipeline/pyproject.toml</code> and update the following line. pyproject.toml<pre><code>[tool.poetry.dependencies]\npython = \"&gt;=3.8,&lt;3.12\"\naineko = { path = \"&lt;path/to/Aineko/git/repo&gt;\", develop=true}\n</code></pre> <p>Test if your changes worked by running the Aineko pipeline and any other testing methods that are relevant.</p>"},{"location":"community/#run-lints-and-tests","title":"Run lints and tests","text":"Finally, after making all the changes, it's good to validate that you adhered to the style guide and you didn't break anything. <pre><code># Within aineko git repository\nmake lint\nmake unit-test\nmake integration-test\n</code></pre>"},{"location":"community/#update-aineko-docs","title":"Update Aineko docs","text":"<p>Aineko raw documentation is in the form of markdown files found in the <code>docs</code> directory.</p> <p>Aineko uses Material for MkDocs to generate the documentation static site from markdown.</p>"},{"location":"community/#set-up-a-local-server-to-view-changes-live","title":"Set up a local server to view changes live","text":"<code>MkDocs</code> comes with a tool to display local documentation as it would on the site, allowing you to view changes as you make them. Set up a local server that automatically updates using: <pre><code>poetry run mkdocs serve\n</code></pre> <p>Navigate to localhost:8000 to see the documentation site.</p>"},{"location":"community/#run-lint","title":"Run lint","text":"Once you're happy with your changes, run the linters to keep any additional code stylistically consistent. You will need to install vale (a linter for prose) first. Installation instructions  can be found here. <pre><code>make lint\nmake lint-docs\n</code></pre>"},{"location":"community/#make-a-pull-request","title":"Make a pull request","text":"How to make a PR? <p>To make a PR, first create and push to GitHub a branch by running the following commands.</p> <pre><code>git checkout -b docs/&lt;branch-name&gt;\ngit add .\ngit commit -m \"docs: &lt;some descriptive message&gt;\"\ngit push --set-upstream origin docs/&lt;branch-name&gt;\n</code></pre> <p>Next, navigate to the Aineko GitHub repo and select the <code>docs/&lt;branch-name&gt;</code> branch in the compare box.</p> <p>Document versioning</p> <p>On every merge to the <code>develop</code> branch, our CI automatically packages and publishes the docs under the <code>dev</code> version. Additionally, when a version of Aineko is tagged, our CI publishes that version of the docs as under that minor version (for example, the version with tag <code>1.2.3</code> will be published under <code>1.2</code>).</p>"},{"location":"developer_guide/aineko_project/","title":"Aineko project","text":"<p>Note</p> <p>This is a continuation of the previous section (Quick Start). Before starting, make sure you have already created a template project using <code>aineko create</code>.</p>"},{"location":"developer_guide/aineko_project/#directory-contents","title":"Directory contents","text":"<pre><code>.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 conf\n\u2502   \u2514\u2500\u2500 pipeline.yml\n\u251c\u2500\u2500 my_awesome_pipeline\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u2514\u2500\u2500 nodes.py\n\u251c\u2500\u2500 poetry.lock\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 test_nodes.py\n</code></pre> <p>This is how the boilerplate directory look - many of these files are boilerplate files to make things work.</p> <p>Let's zoom in on the more interesting files to take note of:</p> <ol> <li><code>conf/pipeline.yml</code> - This contains your pipeline definition that you are expected to modify to define your own pipeline. It is defined in YAML.</li> <li><code>my_awesome_pipeline/nodes.py</code> - Remember how nodes are abstractions for computations? These nodes are implemented in Python. You do not have to strictly define them in this file. You can define them anywhere you like within the directory as long as you reference them correctly in <code>pipeline.yml</code>.</li> </ol>"},{"location":"developer_guide/aineko_project/#defining-a-pipeline","title":"Defining a pipeline","text":"<p>Pipelines are defined using a <code>.yml</code> file that contains specific keys. In this configuration file, you can assemble a pipeline from nodes and datasets.</p> <p>Refer to this for a detailed breakdown on pipeline configuration.</p>"},{"location":"developer_guide/aineko_project/#implementing-a-node","title":"Implementing a node","text":"<p>A node requires:</p> <ul> <li>Inheriting the base node class <code>aineko.core.node.AbstractNode</code></li> <li>Implementing at least the abstract method <code>_execute</code> and optionally <code>_pre_loop_hook</code> .</li> </ul> <p><code>_pre_loop_hook</code> (optional) is used to initialize the node's state before it starts to process data from the dataset.</p> <p><code>_execute</code> is the main logic that run recurrently. As of writing, user should explicitly read and write within this method like so:</p> <pre><code>def _execute(self, params: Optional[dict] = None):\n    \"\"\"This node takes an input number and increments it by 1.\"\"\"\n    input_number = self.inputs[\"my_input_dataset\"].next()\n    # if we want the most recent message, we can use .last()\n    output_number = input_number + 1\n    self.outputs[\"my_output_dataset\"].write(output_number)\n</code></pre>"},{"location":"developer_guide/cli/","title":"CLI documentation","text":"<p>The Aineko CLI is a development tool that allows you to get started quickly and introspect your pipeline runs more expediently.</p>"},{"location":"developer_guide/cli/#aineko","title":"aineko","text":"<p>Aineko CLI.</p> <p>Usage:</p> <pre><code>aineko [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --version  Show the version and exit.\n  --help     Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-create","title":"aineko create","text":"<p>Creates boilerplate code and config required for an Aineko pipeline.</p> <p>If repo is set, attempts to clone and use the files from the repo. The following files are required:</p> <pre><code>- aineko.yml: Contains required data to create an aineko pipeline.\n    Requires keys:\n    - aineko_version: Version of aineko to use.\n    - project_name: Name of the aineko project.\n    - project_slug: (optional) slug of project. Can only contain\n        alphanumeric characters and underscores. Will be derived from\n        project_name if not provided.\n    - project_description: (optional) Description of the pipeline.\n    - pipeline_slug: (optional) name of pipeline (alphanumeric\n        characters, dashes and underscores only).\n- &lt;&lt;project_name&gt;&gt;/nodes.py or &lt;&lt;project_name&gt;&gt;/nodes/*.py: Either a\n    file containing all node code or a directory containing multiple\n    node code files.\n- conf/*.yml: Directory containing config files for pipelines.\n</code></pre> <p>The following files are optional, and will overwrite the default files:     - README.md: README file for the repo.     - pyproject.toml: Project configuration, including poetry requirements     - deploy.yml: Project deployment configuration.     - &lt;&gt;/tests/*.py: Test directory <p>Args:     deployment_config: If True, include a deploy file when generating, else         do not include.     output_dir: Directory to create pipeline in. Defaults to current.     no_input: If True, do not prompt for parameters and use defaults.     repo: GitHub repo to create pipeline from. Link should contain branch         or rev to reference.</p> <p>Usage:</p> <pre><code>aineko create [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -d, --deployment-config  Include deploy.yml that\n                           facilitates deployment\n                           of pipelines.\n  -o, --output-dir TEXT    Directory to create\n                           pipeline in. Defaults\n                           to current directory.\n  -n, --no-input           Do not prompt for\n                           parameters and use\n                           defaults.\n  -r, --repo TEXT          GitHub repo to create\n                           pipeline from. Link\n                           should contain branch\n                           or rev to reference.\n                           (Ex aineko-dev/dream-\n                           catcher#my-branch)\n                           Refer to CLI docs for\n                           more info on repo\n                           structure.\n  --help                   Show this message and\n                           exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-dream","title":"aineko dream","text":"<p>Aineko Dream CLI.</p> <p>Usage:</p> <pre><code>aineko dream [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-dream-check","title":"aineko dream check","text":"<p>Check the status of a project generation request.</p> <p>Args:     request_id: Request ID to check status for.     api_key: API key to use for the Aineko Dream API.     url: URL to use for the Aineko Dream API.     timeout: Seconds to wait for successful project         generation before timing out.</p> <p>Usage:</p> <pre><code>aineko dream check [OPTIONS] REQUEST_ID\n</code></pre> <p>Options:</p> <pre><code>  -k, --api-key TEXT     API key to use for the\n                         Aineko Dream API.\n  -u, --url TEXT         API url to use for the\n                         Aineko Dream API.\n  -t, --timeout INTEGER  Seconds to wait for\n                         successful project\n                         generation before timing\n                         out. Will poll status\n                         every 10 seconds.\n  --help                 Show this message and\n                         exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-dream-create","title":"aineko dream create","text":"<p>Command to generate an aineko project using Aineko Dream.</p> <p>Args:     prompt: Prompt to generate a project from.     api_key: API key to use for the Aineko Dream API.     url: URL to use for the Aineko Dream API.</p> <p>Usage:</p> <pre><code>aineko dream create [OPTIONS] PROMPT\n</code></pre> <p>Options:</p> <pre><code>  -k, --api-key TEXT  API key to use for the\n                      Aineko Dream API.\n  -u, --url TEXT      API url to use for the\n                      Aineko Dream API.\n  --help              Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-run","title":"aineko run","text":"<p>Main function to run a pipeline from the command line.</p> <p>Args:     pipeline_config_file: Path to the file containing the pipeline config     pipeline_name: Name of the pipeline to run, overrides pipeline config     retry: If true, retry running the pipeline on failure every 10 seconds</p> <p>Usage:</p> <pre><code>aineko run [OPTIONS] PIPELINE_CONFIG_FILE\n</code></pre> <p>Options:</p> <pre><code>  -p, --pipeline-name TEXT  Name of the pipeline\n                            to run.\n  -r, --retry               Retry running the\n                            pipeline on failure.\n  --help                    Show this message and\n                            exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service","title":"aineko service","text":"<p>Manage Aineko docker services (Kafka and Zookeeper containers).</p> <p>Usage:</p> <pre><code>aineko service [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -c, --config TEXT  Path to the custom Docker\n                     Compose config file.\n  --help             Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service-down","title":"aineko service down","text":"<p>Kill Aineko docker services.</p> <p>Usage:</p> <pre><code>aineko service down [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service-restart","title":"aineko service restart","text":"<p>Restart Aineko docker services.</p> <p>Usage:</p> <pre><code>aineko service restart [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -H, --hard  Forces full restart Aineko docker\n              services.Clears data from Kafka\n              cache.\n  --help      Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service-start","title":"aineko service start","text":"<p>Start Aineko docker services.</p> <p>Usage:</p> <pre><code>aineko service start [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service-stop","title":"aineko service stop","text":"<p>Stop Aineko docker services.</p> <p>Usage:</p> <pre><code>aineko service stop [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-stream","title":"aineko stream","text":"<p>Stream messages from a dataset.</p> <p>Usage:</p> <pre><code>aineko stream [OPTIONS] DATASET\n</code></pre> <p>Options:</p> <pre><code>  -b, --from-beginning  If messages should be\n                        streamed from the start\n  --help                Show this message and\n                        exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-visualize","title":"aineko visualize","text":"<p>Builds mermaid graph from an Aineko pipeline config.</p> <p>Args:     config_path: file path to pipeline yaml file     direction: direction of the graph.     legend: include a legend in the graph.     browser: Whether to render graph in browser. Prints graph to stdout         otherwise.</p> <p>Usage:</p> <pre><code>aineko visualize [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> <pre><code>  -d, --direction [TD|LR]  Direction of the graph.\n                           Either LR (left to\n                           right) or TD (top\n                           down).\n  -l, --legend             Include a legend in the\n                           graph.\n  -b, --browser            Render graph in\n                           browser. Prints graph\n                           to stdout otherwise.\n  --help                   Show this message and\n                           exit.\n</code></pre>"},{"location":"developer_guide/cli/#shell-completion","title":"Shell completion","text":"<p>Aineko supports shell completion for Bash and Zsh. To enable it, follow the instructions below.</p> <p>Please select your shell</p> BashZsh <p>Add this to <code>~/.bashrc</code>:</p> <pre><code>eval \"$(_AINEKO_COMPLETE=bash_source aineko)\"\n</code></pre> <p>Add this to <code>~/.zshrc</code>:</p> <pre><code>eval \"$(_AINEKO_COMPLETE=zsh_source aineko)\"\n</code></pre>"},{"location":"developer_guide/concepts/","title":"Concepts","text":"What is an Aineko Pipeline? <p>In day-to-day conversations, the term pipeline frequently denotes either a Pipeline definition or a Pipeline execution. Aineko documentation aims to differentiate them explicitly.</p> <p>An Aineko pipeline is a streaming workflow. This means that data is continuously being transmitted and sent over to different components in a way that allows for real-time processing of the data.</p> <p>In a pipeline, you may implement arbitrary computation units (Nodes), and specify where they read data from, and where they send that data to (Datasets). An Aineko Pipeline allows you to construct complex processing graphs that processes streaming data.</p>"},{"location":"developer_guide/concepts/#pipeline-definition","title":"Pipeline definition","text":"<p>A pipeline definition is a specialised Program that you write - to tell Aineko what a pipeline comprises. A Pipeline definition is defined in YAML and essentially allows Aineko to compose computation nodes together by specifying the input and output buffers that they read data from and write data to.</p> <p>See here to learn about writing a pipeline definition.</p>"},{"location":"developer_guide/concepts/#pipeline-execution","title":"Pipeline execution","text":"<p>If a pipeline definition is a program, then a pipeline execution is a process. You can run multiple pipeline executions for a single pipeline definition.</p>"},{"location":"developer_guide/concepts/#dataset","title":"Dataset","text":"<p>A Dataset is an abstraction for a buffer for data that you can define inputs and outputs for. Outputs write data to a dataset, while inputs read data from the dataset. It's analogous to a pub-sub topic or channel. In the current version of aineko, it's a Kafka Topic, but in future, other implementations of message channels could be pluggable too.</p>"},{"location":"developer_guide/concepts/#node","title":"Node","text":"<p>A Node is an abstraction for some computation, akin to a function. At the same time a Node can be a writer and/or a reader of a Dataset.</p> <p>A node can optionally read from topics, process that data and write the output to another buffer that you can chain other Node readers on.</p>"},{"location":"developer_guide/concepts/#workflow","title":"Workflow","text":"<p>A workflow is a series of steps that occur as part of the CI/CD process. For example, a continuous integration workflow contains steps that checks the validity of your deployment configuration. A continuous deployment workflow, on the other hand, orchestrates the necessary changes in the cloud to deploy the most recent version of your code.</p>"},{"location":"developer_guide/config_kafka/","title":"Configuring Kafka","text":"Aineko uses <code>kafka</code> under the hood for sending messages between nodes. As part of running Aineko locally, it's recommended to run a local <code>kafka</code> and <code>zookeeper</code> server using <pre><code>poetry run aineko service start\n</code></pre> <p>To use a different <code>kafka</code> cluster, such as in deployment settings, Aineko allows for configuring of <code>kafka</code> parameters through environment variables. Typically, you would want to modify configuration for the consumer and producer to point to the desired cluster.</p> <p>See below for default <code>kafka</code> configuration that ships with <code>aineko</code> and how to override them.</p>"},{"location":"developer_guide/config_kafka/#aineko.config","title":"aineko.config","text":"<p>Configuration file for Aineko modules.</p> <p>Kafka configuration can be set using the following environment variables:</p> <p>KAFKA_CONFIG: JSON string with kafka configuration (see https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md for all options)</p> <p>Additionally, the following environment variables can be used to specify certain configuration values. They correspond to configuration keys found in the above link, but with a prefix. For example, <code>KAFKA_CONFIG_BOOTSTRAP_SERVERS</code> corresponds to <code>bootstrap.servers</code>.</p> <ul> <li>KAFKA_CONFIG_BOOTSTRAP_SERVERS (e.g. <code>localhost:9092,localhost:9093</code>)</li> <li>KAFKA_CONFIG_SASL_USERNAME</li> <li>KAFKA_CONFIG_SASL_PASSWORD</li> <li>KAFKA_CONFIG_SECURITY_PROTOCOL</li> <li>KAFKA_CONFIG_SASL_MECHANISM</li> </ul>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG","title":"DEFAULT_KAFKA_CONFIG","text":"<p>             Bases: <code>BaseConfig</code></p> <p>Kafka configuration.</p>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.BROKER_CONFIG","title":"BROKER_CONFIG  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BROKER_CONFIG = {'bootstrap.servers': 'localhost:9092'}\n</code></pre>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.CONSUMER_CONFIG","title":"CONSUMER_CONFIG  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CONSUMER_CONFIG: Dict[str, str] = {\n    None: BROKER_CONFIG,\n    \"auto.offset.reset\": \"earliest\",\n}\n</code></pre>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.CONSUMER_MAX_MESSAGES","title":"CONSUMER_MAX_MESSAGES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CONSUMER_MAX_MESSAGES = 1000000\n</code></pre>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.CONSUMER_TIMEOUT","title":"CONSUMER_TIMEOUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CONSUMER_TIMEOUT = 0\n</code></pre>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.DATASETS","title":"DATASETS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DATASETS = [LOGGING_DATASET]\n</code></pre>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.DATASET_PARAMS","title":"DATASET_PARAMS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DATASET_PARAMS = {\n    \"num_partitions\": 1,\n    \"replication_factor\": 1,\n    \"config\": {\"retention.ms\": 1000 * 60 * 60 * 24 * 7},\n}\n</code></pre>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.LOGGING_DATASET","title":"LOGGING_DATASET  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LOGGING_DATASET = 'logging'\n</code></pre>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.OVERRIDABLES","title":"OVERRIDABLES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OVERRIDABLES = {\n    \"KAFKA_CONFIG_BOOTSTRAP_SERVERS\": \"bootstrap.servers\",\n    \"KAFKA_CONFIG_SASL_USERNAME\": \"sasl.username\",\n    \"KAFKA_CONFIG_SASL_PASSWORD\": \"sasl.password\",\n    \"KAFKA_CONFIG_SECURITY_PROTOCOL\": \"security.protocol\",\n    \"KAFKA_CONFIG_SASL_MECHANISM\": \"sasl.mechanism\",\n}\n</code></pre>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.PRODUCER_CONFIG","title":"PRODUCER_CONFIG  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PRODUCER_CONFIG: Dict[str, str] = {None: BROKER_CONFIG}\n</code></pre>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.PRODUCER_OVERRIDABLES","title":"PRODUCER_OVERRIDABLES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PRODUCER_OVERRIDABLES = []\n</code></pre>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.kafka_config","title":"kafka_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kafka_config = get('KAFKA_CONFIG', '{}')\n</code></pre>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG.value","title":"value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value = get(env)\n</code></pre>"},{"location":"developer_guide/node_implementation/","title":"Building a node","text":"<p>Nodes are essentially units of compute that encapsulate any event-driven logic you can define in python. Whether it's a transformation, an API call or a data transfer, as long as you can express it in python, it can be contained in a node.</p>"},{"location":"developer_guide/node_implementation/#implementing-a-node","title":"Implementing a node","text":"<p>To illustrate how a node should be constructed, we will go through an example of a simple node that reads a number from an input dataset, increments it by 1, then writes it to an output dataset.</p> sum_node.py<pre><code>from aineko.core.node import AbstractNode\n\nclass MySumNode(AbstractNode):\n\n    def _pre_loop_hook(self, params=None):\n        \"\"\"Optional; used to initialize node state.\"\"\"\n        self.state = params.get(\"initial_state\", 0)\n\n    def _execute(self, params=None):\n        \"\"\"Required; function repeatedly executes.\"\"\"\n        msg = self.inputs[\"test_sequence\"].next()\n        self.log(\n            f\"Received input: {msg['message']}. Adding {params['increment']}...\"\n        )\n        self.state = int(msg[\"message\"]) + int(params[\"increment\"])\n        self.outputs[\"test_sum\"].write(self.state)\n</code></pre>"},{"location":"developer_guide/node_implementation/#_pre_loop_hook","title":"<code>_pre_loop_hook</code>","text":"<p>You can optionally define a <code>_pre_loop_hook</code> method in your node class to initialize the state of your node with class variables. If the <code>node_params</code> key is defined in <code>pipeline.yml</code>, it will be passed in under the <code>params</code> argument.</p> sum_node.py<pre><code>from aineko.core.node import AbstractNode\n\nclass MySumNode(AbstractNode):\n\n    def _pre_loop_hook(self, params=None):\n        \"\"\"Optional; used to initialize node state.\"\"\"\n        self.state = params.get(\"initial_state\", 0)\n\n    def _execute(self, params=None):\n        \"\"\"Required; function repeatedly executes.\"\"\"\n        msg = self.inputs[\"test_sequence\"].next()\n        self.log(\n            f\"Received input: {msg['message']}. Adding {params['increment']}...\"\n        )\n        self.state = int(msg[\"message\"]) + int(params[\"increment\"])\n        self.outputs[\"test_sum\"].write(self.state)\n</code></pre>"},{"location":"developer_guide/node_implementation/#_execute","title":"<code>_execute</code>","text":"<p>The <code>_execute</code> method is repeatedly executed as the pipeline runs. We recommend nodes to follow a design pattern of constantly polling for new data and taking action when new data is received.</p> sum_node.py<pre><code>from aineko.core.node import AbstractNode\n\nclass MySumNode(AbstractNode):\n\n    def _pre_loop_hook(self, params=None):\n        \"\"\"Optional; used to initialize node state.\"\"\"\n        self.state = params.get(\"initial_state\", 0)\n\n    def _execute(self, params=None):\n        \"\"\"Required; function repeatedly executes.\"\"\"\n        msg = self.inputs[\"test_sequence\"].next()\n        self.log(\n            f\"Received input: {msg['message']}. Adding {params['increment']}...\"\n        )\n        self.state = int(msg[\"message\"]) + int(params[\"increment\"])\n        self.outputs[\"test_sum\"].write(self.state)\n</code></pre> <p>A node will only terminate when the entire pipeline goes down or when the poison pill is activated. </p>"},{"location":"developer_guide/node_implementation/#inputs-outputs","title":"Inputs &amp; Outputs","text":"<p>Node classes inherit attributes named <code>self.inputs</code> and <code>self.outputs</code> that are each a dictionary, with keys being the dataset name and values being subclasses of <code>AbstractDataset</code>. These objects allow you to read/write data from/to a dataset.</p> <p>This is an example of typical usage within a node:</p> sum_node.py<pre><code>from aineko.core.node import AbstractNode\n\nclass MySumNode(AbstractNode):\n\n    def _pre_loop_hook(self, params=None):\n        \"\"\"Optional; used to initialize node state.\"\"\"\n        self.state = params.get(\"initial_state\", 0)\n\n    def _execute(self, params=None):\n        \"\"\"Required; function repeatedly executes.\"\"\"\n        msg = self.inputs[\"test_sequence\"].next()\n        self.log(\n            f\"Received input: {msg['message']}. Adding {params['increment']}...\"\n        )\n        self.state = int(msg[\"message\"]) + int(params[\"increment\"])\n        self.outputs[\"test_sum\"].write(self.state)\n</code></pre> <p>Inputs and Outputs must be included in the pipeline configuration</p> <p>They must be defined in the <code>inputs</code> and <code>outputs</code> list respectively to be available to the node. If a dataset is not available in a Node's catalog, a <code>KeyError</code> will be raised.</p> <p>A node can write to a dataset, read from a dataset, or both. Nodes that read are triggered to action by the arrival of new data in the dataset they read from.</p> <p>Examples on possible ways to connect nodes with datasets</p> Write onlyRead onlyRead and Write <p>This node only writes to two datasets, and acts like a source for datasets:</p> <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nN_node_producer_only((node_writer_only)):::nodeClass --&gt;  T_produced_dataset_1[written_dataset_1]:::datasetClass\nN_node_producer_only((node_writer_only)):::nodeClass --&gt;  T_produced_dataset_2[written_dataset_2]:::datasetClass</code></pre> <p>This node only reads from two datasets, and acts like a sink for datasets: <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_read_dataset_1[read_dataset_1]:::datasetClass --&gt;  N_node_consumer_only((node_reader_only)):::nodeClass\nT_consumed_dataset_2[read_dataset_2]:::datasetClass --&gt;  N_node_consumer_only((node_reader_only)):::nodeClass</code></pre></p> <p>A node that both reads and writes datasets acts like a transformer for datasets. The read datasets are the inputs to the transformer, and the written datasets are the outputs of the transformer: <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_consumed_dataset[read_dataset]:::datasetClass --&gt;  N_node_transformer((node_transformer)):::nodeClass\nN_node_transformer((node_transformer)):::nodeClass --&gt;  T_produced_dataset[written_dataset]:::datasetClass</code></pre></p>"},{"location":"developer_guide/node_implementation/#read-methods-for-kafka-dataset","title":"Read Methods for Kafka Dataset","text":"<p>Depending on the architecture of the node, there are several methods of reading from a dataset. The available methods are listed below.</p> The most common case is to wait till a new message arrives, then read it immediately. The best way to do this is: Waiting for the next available message<pre><code>self.inputs[\"dataset\"].next()\n</code></pre> In some cases, data is being written faster than it can be read, and we just want the freshest, most recent message each time. To do this: Getting the most recent message<pre><code>self.inputs[\"dataset\"].last(timeout=1)\n</code></pre> In cases where you might require more low-level control over reading patterns, such as reading from multiple datasets in the same node, the low-level <code>read</code> method can be used for the Kafka Dataset. More fine-tune control<pre><code>self.inputs[\"dataset\"].read(how=\"next\", timeout=1)\n</code></pre> <p>The timeout argument in these methods signify the duration in which the method has to return a message otherwise it will re-poll for a new one.</p>"},{"location":"developer_guide/node_implementation/#logging","title":"Logging","text":"Node classes inherit a method named <code>self.log</code> that allows users to log messages. You can set the appropriate level from: <code>info</code>, <code>debug</code>, <code>warning</code>, <code>error</code>, an <code>critical</code>. You can log from inside of the <code>_pre_loop_hook</code> method, the <code>_execute</code> method, or any other method you add to your node. <pre><code>self.log(f\"Produced {self.cur_integer}\", level=\"info\")\n</code></pre>"},{"location":"developer_guide/node_implementation/#poisonpill","title":"PoisonPill","text":"Poison pills refers to an \"emergency shut down\" button that can be triggered in times of emergency. Every node has access to a <code>activate_poison_pill</code> method that will terminate the entire pipeline and kill all nodes. To invoke it, use the following syntax. <pre><code>node.activate_poison_pill()\n</code></pre>"},{"location":"developer_guide/pipeline_configuration/","title":"Pipeline configuration","text":"<p>At a high-level, building a pipeline requires defining a pipeline and implementing at least a node.</p>"},{"location":"developer_guide/pipeline_configuration/#defining-a-pipeline","title":"Defining a pipeline","text":"<p>The simplest possible pipeline would consist of a node and a dataset as shown below.</p> <p><pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nN_sequence((sequence)):::nodeClass --&gt;  T_test_sequence[test_sequence]:::datasetClass</code></pre> For the sake of simplicity, we reference a truncated version of the pipeline definition:</p> Example <code>pipeline.yml</code> configuration file <pre><code>pipeline:\n  name: test-aineko-pipeline\n\n  default_node_settings:\n    num_cpus: 0.5\n\n  nodes:\n    sequence:\n      class: my_awesome_pipeline.nodes.MySequencerNode\n      outputs:\n        - test_sequence\n      node_params:\n        initial_state: 0\n        increment: 1\n\n\n  datasets:\n    test_sequence:\n      type: aineko.datasets.kafka.KafkaDataset\n</code></pre> <p>A pipeline definition should have the following attributes:</p>"},{"location":"developer_guide/pipeline_configuration/#keys","title":"Keys","text":""},{"location":"developer_guide/pipeline_configuration/#pipeline","title":"<code>pipeline</code>","text":"<p>This is the top-level key in a pipeline configuration file, a configuration map to define the name, default settings, nodes, and datasets for a pipeline.</p> Key Required Type Description <code>name</code> Y string Name of the pipeline. <code>default_node_settings</code> N map Defines common default values for node attributes which can be overridden at the node level. <code>nodes</code> Y map Defines the compute nodes for a pipeline, mapping to node names. <code>datasets</code> Y map Defines the compute nodes for a pipeline, mapping to structs with node name keys."},{"location":"developer_guide/pipeline_configuration/#default_node_settings","title":"<code>default_node_settings</code>","text":"<p>This optional section can be used to set common default settings for all nodes in the pipeline. These settings are passed into ray actors as parameters, and accept any of the arguments found here. The most common one we usually use is <code>num_cpus</code>.</p> Key Required Type Description <code>&lt;setting&gt;</code> N multiple Any of the parameters found here. <code>num_cpus</code> N float Defines default number of CPUs for a node. Can be less than one. <p>These can be overridden at the node level. More details on the node settings can be found here.</p>"},{"location":"developer_guide/pipeline_configuration/#nodes","title":"<code>nodes</code>","text":"<p>This section defines the compute nodes for a pipeline.</p> Key Required Type Description <code>&lt;name of node&gt;</code> Y map Defines map of node names to node structures in the pipeline."},{"location":"developer_guide/pipeline_configuration/#node_name","title":"<code>&lt;node_name&gt;</code>","text":"<p>A particular node instance in the pipeline, defined by a unique name. Any parameters defined at the individual node level will locally overwrite any default settings defined at the <code>default_node_settings</code> level.</p> Key Required Type Description <code>class</code> Y string Python module to run for the node. This should exist within the python module in the same repository . <code>inputs</code> N list of strings Defines which datasets to read from if applicable. <code>outputs</code> N list of strings Defines which datasets to write to if applicable. <code>node_params</code> N map Defines any arbitrary parameters relevant for node's application logic. In the example above, we defined <code>initial_state</code> and <code>increment</code> parameters, which are both integers. Environment variables can be injected into <code>node_params</code> by passing in strings that match the pattern <code>{$ENV_VAR}</code> where ENV_VAR is the environment variable to inject. <code>num_cpus</code> Y float Number of CPUs allocated to a node. Required either for each node definition or at <code>default_node_settings</code> level. How are environment variables injected? <p>The following method is used to inject environment variables into <code>node_params</code>:</p>"},{"location":"developer_guide/pipeline_configuration/#aineko.core.config_loader.ConfigLoader.inject_env_vars","title":"aineko.core.config_loader.ConfigLoader.inject_env_vars","text":"<pre><code>inject_env_vars(\n    node_params: Optional[\n        Union[Dict, List, str, int, float, bool]\n    ] = None\n) -&gt; Optional[Union[Dict, List, str, int, float, bool]]\n</code></pre> <p>Inject environment variables into node params.</p> <p>This function is used to recursively inject environment variables into strings passed through node params via the pipeline config. We only recursively parse strings, dicts, and lists, as these are the only types that can contain environment variables (i.e. excluding ints, floats, and Nones).</p> <p>Environment variables are identified in strings by the pattern {$ENV_VAR} where ENV_VAR is the name of the environment variable to inject. For example, given the following environment variables:</p> <pre><code>$ export SECRET1=secret1\n$ export SECRET2=secret2\n</code></pre> <p>The following node params dict:</p> <pre><code>```\n{\n    \"key1\": \"A string with a {$SECRET1} and a {$SECRET2}.\",\n    \"key2\": {\n        \"key3\": \"A string with a {$SECRET1} and a {$SECRET2}.\",\n        \"key4\": [\n            \"A string with a {$SECRET1} and a {$SECRET2}.\",\n            \"A string with a {$SECRET1} and a {$SECRET2}.\"\n        ]\n    }\n}\n```\n</code></pre> <p>Will be transformed to:</p> <pre><code>    ```\n    {\n        \"key1\": \"A string with a secret1 and a secret2.\",\n        \"key2\": {\n            \"key3\": \"A string with a secret1 and a secret2.\",\n            \"key4\": [\n                \"A string with a secret1 and a secret2.\",\n                \"A string with a secret1 and a secret2.\"\n            ]\n        }\n    }\n    ```\n</code></pre> Source code in <code>aineko/core/config_loader.py</code> <pre><code>def inject_env_vars(\n    self,\n    node_params: Optional[Union[Dict, List, str, int, float, bool]] = None,\n) -&gt; Optional[Union[Dict, List, str, int, float, bool]]:\n    \"\"\"Inject environment variables into node params.\n\n    This function is used to recursively inject environment variables\n    into strings passed through node params via the pipeline config.\n    We only recursively parse strings, dicts, and lists, as these are\n    the only types that can contain environment variables (i.e.\n    excluding ints, floats, and Nones).\n\n    Environment variables are identified in strings by the pattern\n    {$ENV_VAR} where ENV_VAR is the name of the environment variable\n    to inject. For example, given the following environment variables:\n\n    ```\n    $ export SECRET1=secret1\n    $ export SECRET2=secret2\n    ```\n\n    The following node params dict:\n\n        ```\n        {\n            \"key1\": \"A string with a {$SECRET1} and a {$SECRET2}.\",\n            \"key2\": {\n                \"key3\": \"A string with a {$SECRET1} and a {$SECRET2}.\",\n                \"key4\": [\n                    \"A string with a {$SECRET1} and a {$SECRET2}.\",\n                    \"A string with a {$SECRET1} and a {$SECRET2}.\"\n                ]\n            }\n        }\n        ```\n\n    Will be transformed to:\n\n            ```\n            {\n                \"key1\": \"A string with a secret1 and a secret2.\",\n                \"key2\": {\n                    \"key3\": \"A string with a secret1 and a secret2.\",\n                    \"key4\": [\n                        \"A string with a secret1 and a secret2.\",\n                        \"A string with a secret1 and a secret2.\"\n                    ]\n                }\n            }\n            ```\n    \"\"\"\n    if isinstance(node_params, dict):\n        for k, v in list(node_params.items()):\n            node_params[k] = self.inject_env_vars(v)\n    elif isinstance(node_params, list):\n        for i, v in enumerate(node_params):\n            node_params[i] = self.inject_env_vars(v)\n    elif isinstance(node_params, str):\n        env_var_pattern = r\"\\{\\$.*?\\}\"\n        env_var_match = re.search(env_var_pattern, node_params, re.DOTALL)\n        if env_var_match:\n            env_var_env_str = env_var_match.group()\n            env_var_value = os.getenv(\n                env_var_env_str[2:][:-1], default=None\n            )\n            if env_var_value is None:\n                raise ValueError(\n                    \"Failed to inject environment variable. \"\n                    f\"{env_var_env_str[2:][:-1]} was not found.\"\n                )\n            node_params = node_params.replace(\n                env_var_env_str, env_var_value\n            )\n            return self.inject_env_vars(node_params)\n\n    return node_params\n</code></pre>"},{"location":"developer_guide/pipeline_configuration/#aineko.core.config_loader.ConfigLoader.inject_env_vars","title":"Pipeline configuration","text":""},{"location":"developer_guide/pipeline_configuration/#datasets","title":"<code>datasets</code>","text":"<p>This section defines the datasets for a pipeline.</p> Key Required Type Description <code>&lt;name of dataset&gt;</code> Y map Defines map of dataset names to dataset structures in the pipeline."},{"location":"developer_guide/pipeline_configuration/#name-of-dataset","title":"<code>&lt;name of dataset&gt;</code>","text":"<p>A particular dataset instance in the pipeline, defined by a unique name. Each dataset is defined by a type.</p> Key Required Type Description <code>type</code> Y string Defines which dataset implementation to use. <code>location</code> N string Defines the location of the dataset. <code>params</code> N map Defines any arbitrary parameters relevant for dataset's application logic. <p>More details on the dataset configuration can be found here.</p> <p>Note</p> <p>Aineko is currently in the Beta release stage and is constantly improving.</p> <p>If you have any feedback, questions, or suggestions, please reach out to us.</p>"},{"location":"examples/aineko_dream/","title":"Aineko Dream: code generation with ChatGPT","text":"<p> View on GitHub Try on Slack </p> <p>We built an app called Aineko Dream to test-drive Aineko's ability to enable generative AI features. Aineko Dream uses the OpenAI API and the Aineko docs to generate template code for an Aineko pipeline based on a prompt. The pipeline automatically checks the LLM response to ensure it passes some tests and either generates a prompt to fix the errors or passes the response back to the user.</p> <p>This app demonstrates how you can rapidly prototype features that use foundation models by leveraging Aineko features, such as REST client connectors, stateful feedback loops, and API endpoints. Real-time QA allows us to ensure the quality of LLM responses while maintaining an interactive experience for users.</p> <p>Give it a try using the Aineko Dream bot in our Slack. Or checkout the code on GitHub.</p> <p>What will you dream up with Aineko?</p> <p> </p> <p>Aineko Dream in Action</p> <p>Featured Unlocks</p> <p>Tell a Story with Your Data - With Aineko, you retain all critical information with the context to help you tell a story in real-time and retroactively.</p> <p>Aineko made it simple to introduce a feedback loop for real-time QA for ChatGPT responses. We keep track of response evaluation results and submit new prompts to ChatGPT to fix the errors, without any human intervention. Additionally, since we are tracking all prompts, responses, and their evaluation results, we generate a rich dataset of our app\u2019s performance which we can use to track and improve performance.</p> <p>Move Fast, Break Nothing - By representing our use case as an Aineko pipeline, it was clear how we could swap building blocks in and out with ease.</p> <p>Aineko made it easy to try out new foundation models. Once the pipeline was constructed with GPT-3 it was trivial to use GPT-4. Later, we built a second connector node for the Cohere API and used it as a drop-replacement for OpenAI. We were able to run all three models with standalone pipelines, each with its own unique API endpoint. We can use the various endpoints to route traffic for different users or use one for production and the others for development.</p>"},{"location":"examples/aineko_dream/#tech-overview","title":"Tech overview","text":"<p>With the Aineko framework in mind, we broke the problem down into a few key steps:</p> <ol> <li>Fetch GitHub events</li> <li>Update documentation used for prompt</li> <li>Retrieve prompt from user</li> <li>Prompt engineering</li> <li>Query an LLM</li> <li>Evaluate result</li> <li>Return to step 4, or return LLM response to user</li> </ol> <p>We used the following libraries and APIs to build our app.</p> <p>FastAPI &amp; Uvicorn: used to run an API server on Aineko and configure endpoints used by the application.</p> <p>PyGitHub: used as a client to interact with GitHub to fetch relevant documents.</p> <p>OpenAI &amp; Cohere: used to run inference using their LLMs.</p> <p>Bandit: used to run security tests against Python code that gets generated by the LLMs.</p>"},{"location":"examples/aineko_dream/#the-pipeline","title":"The pipeline","text":"<p>We start first with a simple example represented by the following diagram.</p> <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_github_event[github_event]:::datasetClass --&gt;  N_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass\nN_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass --&gt;  T_document[document]:::datasetClass\nT_user_prompt[user_prompt]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nT_document[document]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nN_PromptModel((PromptModel)):::nodeClass --&gt;  T_generated_prompt[generated_prompt]:::datasetClass\nT_generated_prompt[generated_prompt]:::datasetClass --&gt;  N_GPT3Client((GPT3Client)):::nodeClass\nN_GPT3Client((GPT3Client)):::nodeClass --&gt;  T_llm_response[llm_response]:::datasetClass\nT_llm_response[llm_response]:::datasetClass --&gt;  N_APIServer((APIServer)):::nodeClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_user_prompt[user_prompt]:::datasetClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_github_event[github_event]:::datasetClass</code></pre> <p>The API server accepts commit events from GitHub and prompt requests from users. The commit events trigger updates to the document that is used to engineer the LLM prompt. The engineered prompt is passed to the OpenAI API and the response is returned to the API server.</p> <p>If we wanted to add a few evaluation steps to ensure that the LLMs response is valid, we can do so as shown in the following diagram.</p> <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_github_event[github_event]:::datasetClass --&gt;  N_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass\nN_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass --&gt;  T_document[document]:::datasetClass\nT_user_prompt[user_prompt]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nT_document[document]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nN_PromptModel((PromptModel)):::nodeClass --&gt;  T_generated_prompt[generated_prompt]:::datasetClass\nT_generated_prompt[generated_prompt]:::datasetClass --&gt;  N_GPT3Client((GPT3Client)):::nodeClass\nN_GPT3Client((GPT3Client)):::nodeClass --&gt;  T_llm_response[llm_response]:::datasetClass\nT_llm_response[llm_response]:::datasetClass --&gt;  N_PythonEvaluation((PythonEvaluation)):::nodeClass\nN_PythonEvaluation((PythonEvaluation)):::nodeClass --&gt;  T_evaluation_result[evaluation_result]:::datasetClass\nT_llm_response[llm_response]:::datasetClass --&gt;  N_SecurityEvaluation((SecurityEvaluation)):::nodeClass\nN_SecurityEvaluation((SecurityEvaluation)):::nodeClass --&gt;  T_evaluation_result[evaluation_result]:::datasetClass\nT_evaluation_result[evaluation_result]:::datasetClass --&gt;  N_EvaluationModel((EvaluationModel)):::nodeClass\nN_EvaluationModel((EvaluationModel)):::nodeClass --&gt;  T_final_response[final_response]:::datasetClass\nN_EvaluationModel((EvaluationModel)):::nodeClass --&gt;  T_generated_prompt[generated_prompt]:::datasetClass\nT_final_response[final_response]:::datasetClass --&gt;  N_APIServer((APIServer)):::nodeClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_user_prompt[user_prompt]:::datasetClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_github_event[github_event]:::datasetClass</code></pre> <p>Here we add 2 evaluation steps and an evaluation model:</p> <ul> <li>The <code>PythonEvaluation</code> node validates that the LLM proposes valid Python code.</li> <li>The <code>SecurityEvaluation</code> node runs checks using Bandit to ensure that the Python code that is proposed doesn\u2019t contain any known security concerns.</li> <li>The <code>EvaluationModel</code> node reads evaluation results and decides whether to generate another prompt or submit the final result to the API server. It keeps track of the evaluation results and the number of times we query the LLM.</li> </ul>"},{"location":"examples/aineko_dream/#pipeline-configuration","title":"Pipeline configuration","text":"<p>Here is the pipeline configuration used to generate this example. We could even configure and run three separate pipelines that use different models and expose different endpoints for each of them.</p> Pipeline configuration <pre><code>pipeline:\n  name: gpt3-template-generator\n\n  nodes:\n    # Prompt generation\n    # This node can be configured to target any GitHub repo\n    GitHubDocFetcher:\n      class: aineko_dream.nodes.GitHubDocFetcher\n      inputs:\n        - github_event\n      outputs:\n        - document\n      node_params:\n        organization: \"aineko-dev\"\n        repo: \"aineko\"\n        branch: \"documentation\"\n        file_path: \"/docs/\"\n    PromptModel:\n      class: aineko_dream.nodes.PromptModel\n      inputs:\n        - user_prompt\n        - document\n      outputs:\n        - generated_prompt\n    # LLM Client: defines model to use. Change to use another model like GPT-4\n    # If we wanted to use Cohere, switch `OpenAIClient` to `Cohere`.\n    GPT3Client:\n      class: aineko_dream.nodes.OpenAIClient\n      inputs:\n        - generated_prompt\n      outputs:\n        - llm_response\n      node_params:\n        model: \"gpt-3.5-turbo-16k\"\n        max_tokens: 4000\n        temperature: 0.1\n    # Response evaluation\n    PythonEvaluation:\n      class: aineko_dream.nodes.PythonEvaluation\n      inputs:\n        - llm_response\n      outputs:\n        - evaluation_result\n    SecurityEvaluation:\n      class: aineko_dream.nodes.SecurityEvaluation\n      inputs:\n        - llm_response\n      outputs:\n        - evaluation_result\n    EvaluationModel:\n      class: aineko_dream.nodes.EvaluationModel\n      inputs:\n        - evaluation_result\n      outputs:\n        - final_response\n        - generated_prompt\n      node_params:\n        max_cycles: 2\n    # API\n    APIServer:\n      class: aineko_dream.nodes.APIServer\n      inputs:\n        - final_response\n      outputs:\n        - user_prompt\n        - github_event\n      node_params:\n        app: aineko_dream.api.main:app\n        port: 8000\n</code></pre>"},{"location":"examples/aineko_dream/#node-code","title":"Node code","text":"<p>Here are some samples of the node code used to run this pipeline.</p> <p>The <code>GitHubDocFetcher</code> emits the latest document when it initializes and updates the document based on triggers from a GitHub webhook that is configured to target the API server. The document is passed to the <code>PromptModel</code> to engineer a prompt based on the latest document.</p> <p>The <code>OpenAIClient</code> node creates a connection to the OpenAI API and submits requests to the configured model using the ChatCompletion interface. The response is passed on to the evaluation nodes.</p> <p>The <code>SecurityEvaluation</code> node takes the LLM response and creates a temporary file with the contents. Aineko Dream then uses Bandit to run a test against the file and collect a list of issues. After cleaning up, the results are submitted to the <code>EvaluationModel</code> node.</p> <p>Node code examples</p> <code>GitHubDocFetcher</code><code>OpenAIClient</code><code>SecurityEvaluation</code> <pre><code>class GitHubDocFetcher(AbstractNode):\n    \"\"\"Node that fetches code documents from GitHub.\"\"\"\n\n    def _pre_loop_hook(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Initialize connection with GitHub and fetch latest document.\"\"\"\n        # Set parameters\n        self.access_token = os.environ.get(\"GITHUB_ACCESS_TOKEN\")\n        self.organization = params.get(\"organization\")\n        self.repo = params.get(\"repo\")\n        self.branch = params.get(\"branch\")\n        self.file_path = params.get(\"file_path\")\n\n        # Initialize github client\n        auth = Auth.Token(token=self.access_token)\n        self.github_client = Github(auth=auth)\n\n        # Fetch current document\n        self.emit_new_document()\n\n    def _execute(self, params: Optional[dict] = None) -&gt; Optional[bool]:\n        \"\"\"Update document in response to commit events.\"\"\"\n        # Check for new commit events from GitHub\n        message = self.inputs[\"github_event\"].read()\n        if message is None:\n            return\n\n        # Fetch latest document and send update\n        self.log(\"Received event from GitHub, fetching latest document.\")\n        self.emit_new_document()\n\n    def emit_new_document(self) -&gt; None:\n        \"\"\"Emit new document.\"\"\"\n        repo = self.github_client.get_repo(f\"{self.organization}/{self.repo}\")\n        contents = repo.get_contents(self.file_path, ref=self.branch)\n        github_contents = {f.path: f.decoded_content.decode(\"utf-8\") for f in contents}\n        self.outputs[\"document\"].write(github_contents)\n        self.log(\n            f\"Fetched documents for {self.organization}/{self.repo} branch {self.branch}\"\n        )\n</code></pre> <pre><code>class OpenAIClient(AbstractNode):\n    \"\"\"Node that queries OpenAI LLMs.\"\"\"\n\n    def _pre_loop_hook(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Initialize connection with OpenAI.\"\"\"\n        self.model = params.get(\"model\")\n        self.max_tokens = params.get(\"max_tokens\")\n        self.temperature = params.get(\"temperature\")\n        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    def _execute(self, params: Optional[dict] = None) -&gt; Optional[bool]:\n        \"\"\"Query OpenAI LLM.\"\"\"\n        message = self.inputs[\"generated_prompt\"].read()\n        if message is None:\n            return\n        messages = message[\"message\"][\"chat_messages\"]\n        # Query OpenAI LLM\n        self.log(\"Querying OpenAI LLM...\")\n        response = openai.ChatCompletion.create(\n            messages=messages,\n            stream=False,\n            model=self.model,\n            max_tokens=self.max_tokens,\n            temperature=self.temperature,\n        )\n        message[\"message\"][\"chat_messages\"].append(\n            {\n                \"role\": \"assistant\",\n                \"content\": response.choices[0].message.content,\n            }\n        )\n        self.outputs[\"llm_response\"].write(\n            message[\"message\"][\"chat_messages\"]\n        )\n</code></pre> <pre><code>class SecurityEvaluation(AbstractNode):\n    \"\"\"Node that evaluates security of code.\"\"\"\n\n    def _execute(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Evaluate Python code.\"\"\"\n        message = self.inputs[\"llm_response\"].read()\n        if message is None:\n            return\n\n        # Make a temporary file with the LLM code\n        python_code = message[\"message\"]\n        issues_list = []\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False, mode=\"w\") as tmpfile:\n            tmpfile.write(python_code)\n\n        # Setup Bandit and run tests on the temporary file\n        b_mgr = bandit.manager.BanditManager(bandit.config.BanditConfig(), 'file')\n        b_mgr.discover_files([tmpfile.name], None)\n        b_mgr.run_tests()\n\n        # Store results\n        results = b_mgr.get_issue_list(\n            sev_level=bandit.constants.LOW,\n            conf_level=bandit.constants.LOW,\n            )\n\n        # Cleanup (remove the temporary file)\n        tmpfile.close()\n        os.remove(tmpfile.name)\n\n        if results:\n            self.output[\"evaluation_result\"].write(results)\n</code></pre>"},{"location":"examples/aineko_dream/#try-running-locally","title":"Try running locally","text":"<p>Visit GitHub for the latest version of the code and see if you can run it yourself.</p> <p>You can install the app using poetry after cloning from GitHub using:</p> <pre><code>poetry install\n</code></pre> <p>First, make sure that docker is running and run the required docker services in the background using:</p> <pre><code>poetry run aineko service start\n</code></pre> <p>Then start the pipeline using:</p> <pre><code>poetry run aineko run ./conf/gpt3.yml\n</code></pre> <p>When the pipeline is live, you can visit http://127.0.0.1:8000/docs in your browser to interact with the endpoints via the Swagger UI.</p> <p> </p> <p>Swagger UI</p>"},{"location":"examples/aineko_dream/#join-the-community","title":"Join the community","text":"<p>If you have questions about anything related to Aineko, you're always welcome to ask the community on GitHub or Slack.</p>"},{"location":"start/quickstart/","title":"Get started with Aineko","text":""},{"location":"start/quickstart/#technical-dependencies","title":"Technical dependencies","text":"<ol> <li>Docker or Docker Desktop</li> <li>Poetry (a python dependency manager)</li> <li>Python (versions 3.8, 3.9, 3.10 &amp; 3.11 are supported)</li> <li>Pip (a python package manager)</li> </ol> Check your dependencies before starting <p>It's important to make sure you have the correct dependencies installed. The only dependency which requires a specific version is Python. The other dependencies should work with any recent version.</p> <p>Let's check each dependency one by one. You can run the following commands in your terminal to check each dependency.</p> <ul> <li><code>docker --version</code> should return something like <code>Docker version 20.10.8, build 3967b7d</code></li> <li><code>python --version</code> should return something like <code>Python 3.10.12</code>. We recommend pyenv to manage versions if you have multiple versions.</li> <li><code>pip --version</code> should return something like <code>pip 23.0.1 from xxx/python3.10/site-packages/pip (python 3.10)</code></li> <li><code>poetry --version</code> should return something like <code>Poetry (version 1.6.1)</code></li> </ul>"},{"location":"start/quickstart/#install-aineko","title":"Install Aineko","text":"<pre><code>pip install aineko\n</code></pre> Having trouble getting the correct version of python? <p>We recommend using pyenv to manage your Python versions. Once you have pyenv installed, you can run the following commands in your project directory to install one the supported versions. For example:</p> <p><pre><code>pyenv install 3.10\npyenv local 3.10\npython --version\n</code></pre> Expected output<pre><code>Python 3.10.12\n</code></pre></p> <p>Pyenv is a great tool for managing Python versions, but it can be a bit tricky to get it set up correctly. If you're having trouble, check out the pyenv documentation or this tutorial. If you're still having trouble, feel free to reach out to us on Slack!</p>"},{"location":"start/quickstart/#create-a-template-pipeline-with-the-cli","title":"Create a template pipeline with the CLI","text":"You will see the following prompts as <code>aineko</code> tries to create a project directory containing the boilerplate you need for a pipeline. Feel free to use the defaults suggested. <p><pre><code>aineko create\n</code></pre> Expected output<pre><code>[1/4] project_name (My Awesome Pipeline):\n[2/4] project_slug (my_awesome_pipeline):\n[3/4] project_description (Behold my awesome pipeline!):\n[4/4] pipeline_slug (test-aineko-pipeline):\n</code></pre></p>"},{"location":"start/quickstart/#install-dependencies-in-the-new-pipeline","title":"Install dependencies in the new pipeline","text":"<pre><code>cd my_awesome_pipeline\npoetry install\n</code></pre>"},{"location":"start/quickstart/#start-aineko-background-services","title":"Start Aineko background services","text":"<pre><code>poetry run aineko service start\n</code></pre> Expected output<pre><code>Container zookeeper  Creating\nContainer zookeeper  Created\nContainer broker  Creating\nContainer broker  Created\nContainer zookeeper  Starting\nContainer zookeeper  Started\nContainer broker  Starting\nContainer broker  Started\n</code></pre>"},{"location":"start/quickstart/#start-the-template-pipeline","title":"Start the template pipeline","text":"<pre><code>poetry run aineko run ./conf/pipeline.yml\n</code></pre> Expected output<pre><code>INFO - Application is starting.\nINFO - Creating dataset: aineko-pipeline.sequence: {'type': 'aineko.datasets.kafka.KafkaDataset'}\nINFO - All datasets created.\nINFO worker.py:1664 -- Started a local Ray instance.\n</code></pre>"},{"location":"start/quickstart/#check-the-data-being-streamed","title":"Check the data being streamed","text":"To view messages running in one of the user-defined datasets: <p><pre><code>poetry run aineko stream test-aineko-pipeline.test_sequence --from-beginning\n</code></pre> Expected output<pre><code>{\"timestamp\": \"2023-11-10 17:27:20\", \"dataset\": \"sequence\", \"source_pipeline\": \"test-aineko-pipeline\", \"source_node\": \"sequence\", \"message\": 1}\n{\"timestamp\": \"2023-11-10 17:27:20\", \"dataset\": \"sequence\", \"source_pipeline\": \"test-aineko-pipeline\", \"source_node\": \"sequence\", \"message\": 2}\n</code></pre></p> Alternatively, to view logs stored in the built-in <code>logging</code> dataset: <p><pre><code>poetry run aineko stream logging --from-beginning\n</code></pre> Expected output<pre><code>{\"timestamp\": \"2023-11-10 17:46:15\", \"dataset\": \"logging\", \"source_pipeline\": \"test-aineko-pipeline\", \"source_node\": \"sum\", \"message\": {\"log\": \"Received input: 1. Adding 1...\", \"level\": \"info\"}}\n</code></pre></p> <p>Note</p> <p>User-defined datasets have the pipeline name automatically prefixed, but the special built-in dataset <code>logging</code> doesn't.</p>"},{"location":"start/quickstart/#stop-aineko-background-services","title":"Stop Aineko background services","text":"<pre><code>poetry run aineko service stop\n</code></pre> <p>So that's it to get an Aineko pipeline running. How smooth was that?</p> What does the above output mean? <p>An aineko pipeline is made up of Dataset(s) and Node(s). A Dataset can be thought of as a mailbox. Nodes pass messages to this mailbox, that can be read by many other Nodes.</p> <p>A Node is an abstraction for some computation, a function if you will. At the same time a Node can be a reader and/or a writer of a Dataset. (mailbox)</p> <p>The output means that we have successfully created three datasets - test_sequence, test_sum and logging, and that we have created two nodes - sum and sequence.</p> <p>To learn more about Pipeline, Datasets and Nodes, see concepts.</p>"},{"location":"start/quickstart/#visualizing-the-pipeline","title":"Visualizing the pipeline","text":"Using the Aineko CLI, you can also see the above pipeline rendered in the browser. This is helpful for quickly checking your pipeline as you iterate and evolve your architecture. <pre><code>poetry run aineko visualize --browser ./conf/pipeline.yml\n</code></pre> <p>Visualization output</p> <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nN_sequence((sequence)):::nodeClass --&gt;  T_test_sequence[test_sequence]:::datasetClass\nT_test_sequence[test_sequence]:::datasetClass --&gt;  N_sum((sum)):::nodeClass\nN_sum((sum)):::nodeClass --&gt;  T_test_sum[test_sum]:::datasetClass</code></pre>"}]}