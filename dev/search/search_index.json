{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Aineko","text":"<p>Aineko is a Python framework for building powerful data applications, limited only by your imagination! Focus on transformations and business logic, leave the rest to Aineko.</p> <p>Build an Application \u2002 \u2192 \u2002 in minutes</p> What is Aineko? <p>Under the hood, Aineko automatically configures tooling needed for production-ready data apps, like message brokers, distributed compute, and more. This allows you to focus on building your application instead of spending time with configuration and infrastructure.</p>"},{"location":"#main-features","title":"Main Features","text":"Powerful Pipelines Quick to Code Increase speed to develop applications by 100% to 200%.* Intuitive Codified best practices. Less time reading docs. Less time debugging. Robust Production-ready from the get-go. Scale with ease. Flexible Nodes Composable Reuse nodes you've built. Reuse python code. Save time by not re-inventing the wheel. Stateful Work with simple logic. Work with complex logic. Aineko supports it all. Code Faster Skip the boring stuff with pre-built nodes that work with popular data sources like REST APIs. Delightful Datasets Fast Achieve microsecond latency between nodes.* Scalable Process billions or records per day with ease.* Governance Easily work with different versions of data across pipelines and environments. <p>*Estimate based on development experience by an internal team.</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Aineko pipeline configuration loader and runner</li> <li> CLI tool for running pipelines</li> <li> CLI tool for visualizing pipelines</li> <li> REST API and Websocket pre-built nodes</li> <li> Built-in API server node</li> <li> Schema validation support</li> <li> Support multiple queuing systems beyond Kafka (RabbitMQ, etc)</li> <li> Support multiple runtime engines beyond ray (k8s, etc)</li> </ul>"},{"location":"#join-our-community","title":"Join our Community","text":"<p>If you have questions about anything related to Aineko, you're always welcome to ask our community on GitHub or Slack.</p>"},{"location":"aineko_cloud/","title":"Aineko Cloud","text":"<p>Aineko Cloud allows you to deploy Aineko pipelines into the cloud in seconds, without having to worry about the details of production!</p> <p>See Aineko Cloud Documentation for more information.</p>"},{"location":"contributing/","title":"Contributing to Aineko","text":"<p>Thank you for your interest in contributing to Aineko!</p> <p>Here are the steps to get started quickly:</p> <p>Step 1: Install Aineko from source</p> <pre><code># install poetry\ncurl -sSL https://install.python-poetry.org | python3 -\n\ngit clone https://github.com/aineko-dev/aineko\n\ncd aineko &amp;&amp; poetry install\n</code></pre> <p>Step 2: Make your changes to Aineko</p> <p>Step 3: Have your own pipeline or create one</p> <p>We highly encourage you to validate your changes by testing E2E. This means to validate the changes with your pipeline by pointing <code>aineko</code> to the dev folder.</p> <pre><code># using poetry is important so that we are actually invoking the dev version of Aineko\n\npoetry run aineko create\n</code></pre> <p>Step 4: Go to <code>pyproject.toml</code> and apply the following changes:</p> <p>This will ensure your aineko is pointing to the repository you made changes to</p> <pre><code>-aineko = \"^0.2.3\"\n+aineko = { path = \"&lt;path/to/aineko/git/repo&gt;\", develop=true}\n</code></pre> <p>Step 5: Test your changes - did it work?</p> <p>Step 6: Run lints and tests</p> <p>Finally, after you have make all the changes, it is good to validate that you adhered to our style guide and you did not break anything.</p> <pre><code># Within aineko git repository\nmake lint\nmake unit-test\nmake integration-test\n</code></pre> <p>Step 7: Push, make a PR and see you on Github!</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#technical-dependencies","title":"Technical Dependencies","text":"<ol> <li>Docker or Docker Desktop</li> <li>Poetry (a python dependency manager)</li> <li>Python (version 3.10)</li> <li>Pip (a python package manager)</li> </ol>"},{"location":"quickstart/#get-started","title":"Get started","text":"<p>Check your dependencies</p> <p>It's important to make sure you have the correct dependencies installed. The only dependency which requires a specific version is Python. The other dependencies should work with any recent version.</p> <p>Let's check each dependency one by one. You can run the following commands in your terminal to check each dependency.</p> <ul> <li><code>docker --version</code> should return something like <code>Docker version 20.10.8, build 3967b7d</code></li> <li><code>python --version</code> should return something like <code>Python 3.10.12</code> Click here if you see another version.</li> <li><code>pip --version</code> should return something like <code>pip 23.0.1 from xxx/python3.10/site-packages/pip (python 3.10)</code></li> <li><code>poetry --version</code> should return something like <code>Poetry (version 1.6.1)</code></li> </ul>"},{"location":"quickstart/#step-1-install-aineko","title":"Step 1: Install Aineko","text":"<pre><code>pip install aineko\n</code></pre>"},{"location":"quickstart/#step-2-create-a-template-pipeline-with-aineko-cli","title":"Step 2: Create a template pipeline with aineko cli","text":"<pre><code>aineko create\n</code></pre> <p>You will see the following prompts as <code>aineko</code> tries to create a project directory containing the boilerplate you need for a pipeline. Feel free to use the defaults suggested!</p> <pre><code>  [1/4] project_name (My Awesome Pipeline):\n  [2/4] project_slug (my_awesome_pipeline):\n  [3/4] project_description (Behold my awesome pipeline!):\n  [4/4] pipeline_slug (test-aineko-pipeline):\n</code></pre>"},{"location":"quickstart/#step-3-install-dependencies-in-the-new-pipeline","title":"Step 3: Install dependencies in the new pipeline","text":"<pre><code>cd my_awesome_pipeline\npoetry install\n</code></pre>"},{"location":"quickstart/#step-4-start-the-aineko-background-services","title":"Step 4: Start the Aineko background services","text":"<pre><code>poetry run aineko service start\n</code></pre>"},{"location":"quickstart/#step-5-start-the-template-pipeline","title":"Step 5: Start the template pipeline","text":"<pre><code>poetry run aineko run ./conf/pipeline.yml\n</code></pre>"},{"location":"quickstart/#step-6-check-the-data-being-streamed","title":"Step 6: Check the data being streamed","text":"<p>To view messages running in one of the user-defined datasets: <pre><code>poetry run aineko stream --dataset test-aineko-pipeline.test_sequence --from-start\n</code></pre></p> <p>Alternatively, to view logs stored in the built-in <code>logging</code> dataset: <pre><code>poetry run aineko stream --dataset logging --from-start\n</code></pre></p> <p>Note: user-defined datasets have the pipeline name automatically prefixed, but the special built-in dataset <code>logging</code> does not.</p>"},{"location":"quickstart/#step-7-stop-the-aineko-background-services","title":"Step 7: Stop the Aineko background services","text":"<pre><code>poetry run aineko service stop\n</code></pre> <p>So that's it to get an Aineko pipeline running! We hope that was smooth for you!</p> What does the above output mean? <p>An aineko pipeline is made up of Dataset(s) and Node(s). A Dataset can be thought of as a mailbox. Nodes pass messages to this mailbox, that can be read by many other Nodes.</p> <p>A Node is an abstraction for some computation, a function if you will. At the same time a Node can be a producer and/or a consumer of a Dataset. (mailbox)</p> <p>The output means that we have successfully created three datasets - test_sequence, test_sum and logging, and that we have created two nodes - sum and sequence.</p> <p>To learn more about Pipeline, Datasets and Nodes, see concepts.</p>"},{"location":"quickstart/#visualizing-the-pipeline","title":"Visualizing the Pipeline","text":"<p>Using the aineko cli, you can also see the above pipeline rendered in the browser:</p> <pre><code>poetry run aineko visualize --browser ./conf/pipeline.yml\n</code></pre> <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nN_sequence((sequence)):::nodeClass --&gt;  T_test_sequence[test_sequence]:::datasetClass\nT_test_sequence[test_sequence]:::datasetClass --&gt;  N_sum((sum)):::nodeClass\nN_sum((sum)):::nodeClass --&gt;  T_test_sum[test_sum]:::datasetClass</code></pre> <p>As you iterate and augment your Aineko pipeline, you can use this command as a quick check to visualize your evolving pipeline and ensure that it has the intended architecture.</p>"},{"location":"troubleshooting/","title":"Python Versions","text":"<p>We recommend using pyenv to manage your Python versions. Once you have pyenv installed, you can run the following commands to install Python 3.10.</p> <ol> <li> <p><code>pyenv install 3.10</code> to install Python 3.10</p> </li> <li> <p>In your project directory, run the following command to set the local Python version to 3.10: <code>pyenv local 3.10</code>    This will create a <code>.python-version</code> file in your project directory which will tell pyenv to (automagically) use Python 3.10 when you're in that directory.</p> </li> <li> <p>Check that you're now using the correct version of Python by running <code>python --version</code>. You should see something like <code>Python 3.10.12</code>.</p> </li> <li> <p>You're all set! You can now proceed with step 1 of the quick start guide.</p> </li> </ol> <p>Pyenv is a great tool for managing Python versions, but it can be a bit tricky to get it set up correctly. If you're having trouble, check out the pyenv documentation or this tutorial. If you're still having trouble, feel free to reach out to us on Slack!</p>"},{"location":"api_reference/abstract_node/","title":"<code>AbstractNode</code>","text":"<p>The <code>AbstractNode</code> class serves as the base class for all user-defined nodes.</p>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode","title":"<code>aineko.AbstractNode</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Node base class for all nodes in the pipeline.</p> <p>Nodes are the building blocks of the pipeline and are responsible for executing the pipeline. Nodes are designed to be modular and can be combined to create a pipeline. The node base class provides helper methods for setting up the consumer and producer for a node. The execute method is a wrapper for the _execute method which is to be implemented by subclasses. The _execute method is where the node logic is implemented by the user.</p> <p>Attributes:</p> Name Type Description <code>consumers</code> <code>dict</code> <p>dict of DatasetConsumer objects for inputs to node</p> <code>producers</code> <code>dict</code> <p>dict of DatasetProducer objects for outputs of node</p> <code>last_hearbeat</code> <code>float</code> <p>timestamp of the last heartbeat</p> <code>test</code> <code>bool</code> <p>True if node is in test mode else False</p> <code>log_levels</code> <code>tuple</code> <p>tuple of allowed log levels</p> <code>local_state</code> <code>dict</code> <p>shared local state between nodes. Used for intra- pipeline communication without dataset dependency.</p> <p>Methods:</p> Name Description <code>setup_datasets</code> <p>setup the consumers and producers for a node</p> <code>execute</code> <p>execute the node, wrapper for _execute method</p> <code>_execute</code> <p>execute the node, to be implemented by subclasses</p> Source code in <code>aineko/core/node.py</code> <pre><code>class AbstractNode(ABC):\n    \"\"\"Node base class for all nodes in the pipeline.\n\n    Nodes are the building blocks of the pipeline and are responsible for\n    executing the pipeline. Nodes are designed to be modular and can be\n    combined to create a pipeline. The node base class provides helper methods\n    for setting up the consumer and producer for a node. The execute method is\n    a wrapper for the _execute method which is to be implemented by subclasses.\n    The _execute method is where the node logic is implemented by the user.\n\n    Attributes:\n        consumers (dict): dict of DatasetConsumer objects for inputs to node\n        producers (dict): dict of DatasetProducer objects for outputs of node\n        last_hearbeat (float): timestamp of the last heartbeat\n        test (bool): True if node is in test mode else False\n        log_levels (tuple): tuple of allowed log levels\n        local_state (dict): shared local state between nodes. Used for intra-\n            pipeline communication without dataset dependency.\n\n    Methods:\n        setup_datasets: setup the consumers and producers for a node\n        execute: execute the node, wrapper for _execute method\n        _execute: execute the node, to be implemented by subclasses\n    \"\"\"\n\n    def __init__(\n        self,\n        poison_pill: Optional[ray.actor.ActorHandle] = None,\n        test: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the node.\"\"\"\n        self.last_heartbeat = time.time()\n        self.consumers: Dict = {}\n        self.producers: Dict = {}\n        self.params: Dict = {}\n        self.test = test\n        self.log_levels = AINEKO_CONFIG.get(\"LOG_LEVELS\")\n        self.poison_pill = poison_pill\n\n    def enable_test_mode(self) -&gt; None:\n        \"\"\"Enable test mode.\"\"\"\n        self.test = True\n\n    def setup_datasets(\n        self,\n        datasets: Dict[str, dict],\n        node: str,\n        pipeline: str,\n        inputs: Optional[List[str]] = None,\n        outputs: Optional[List[str]] = None,\n        prefix: Optional[str] = None,\n        has_pipeline_prefix: bool = False,\n    ) -&gt; None:\n        \"\"\"Setup the consumer and producer for a node.\n\n        Args:\n            datasets: dataset configuration\n            node: name of the node\n            pipeline: name of the pipeline\n            inputs: list of dataset names for the inputs to the node\n            outputs: list of dataset names for the outputs of the node\n            prefix: prefix for topic name (`&lt;prefix&gt;.&lt;dataset_name&gt;`)\n            has_pipeline_prefix: whether the dataset name has pipeline name\n            prefix\n        \"\"\"\n        inputs = inputs or []\n        self.consumers.update(\n            {\n                dataset_name: DatasetConsumer(\n                    dataset_name=dataset_name,\n                    node_name=node,\n                    pipeline_name=pipeline,\n                    dataset_config=datasets.get(dataset_name, {}),\n                    prefix=prefix,\n                    has_pipeline_prefix=has_pipeline_prefix,\n                )\n                for dataset_name in inputs\n            }\n        )\n\n        outputs = outputs or []\n        self.producers.update(\n            {\n                dataset_name: DatasetProducer(\n                    dataset_name=dataset_name,\n                    node_name=node,\n                    pipeline_name=pipeline,\n                    dataset_config=datasets.get(dataset_name, {}),\n                    prefix=prefix,\n                    has_pipeline_prefix=has_pipeline_prefix,\n                )\n                for dataset_name in outputs\n            }\n        )\n\n    def setup_test(\n        self,\n        inputs: Optional[dict] = None,\n        outputs: Optional[list] = None,\n        params: Optional[dict] = None,\n    ) -&gt; None:\n        \"\"\"Setup the node for testing.\n\n        Args:\n            inputs: inputs to the node, format should be {\"dataset\": [1, 2, 3]}\n            outputs: outputs of the node, format should be\n                [\"dataset_1\", \"dataset_2\", ...]\n            params: dictionary of parameters to make accessible to _execute\n\n        Raises:\n            RuntimeError: if node is not in test mode\n        \"\"\"\n        if self.test is False:\n            raise RuntimeError(\n                \"Node is not in test mode. \"\n                \"Please initialize with `enable_test_mode()`.\"\n            )\n\n        inputs = inputs or {}\n        self.consumers = {\n            dataset_name: FakeDatasetConsumer(\n                dataset_name=dataset_name,\n                node_name=self.__class__.__name__,\n                values=values,\n            )\n            for dataset_name, values in inputs.items()\n        }\n        outputs = outputs or []\n        outputs.extend(TESTING_NODE_CONFIG.get(\"DATASETS\"))\n        self.producers = {\n            dataset_name: FakeDatasetProducer(\n                dataset_name=dataset_name,\n                node_name=self.__class__.__name__,\n            )\n            for dataset_name in outputs\n        }\n        self.params = params or {}\n\n    def log(self, message: str, level: str = \"info\") -&gt; None:\n        \"\"\"Log a message to the logging dataset.\n\n        Args:\n            message: Message to log\n            level: Logging level. Defaults to \"info\". Options are:\n                \"info\", \"debug\", \"warning\", \"error\", \"critical\"\n        Raises:\n            ValueError: if invalid logging level is provided\n        \"\"\"\n        if level not in self.log_levels:\n            raise ValueError(\n                f\"Invalid logging level {level}. Valid options are: \"\n                f\"{', '.join(self.log_levels)}\"\n            )\n        out_msg = {\"log\": message, \"level\": level}\n        self.producers[DEFAULT_KAFKA_CONFIG.get(\"LOGGING_DATASET\")].produce(\n            out_msg\n        )\n\n    def catch_exception(self) -&gt; None:\n        \"\"\"Catch an exception and report it.\n\n        Args:\n            exc: Exception to catch\n        \"\"\"\n        exc_info = traceback.format_exc()\n        self.log(exc_info, level=\"error\")\n\n    def execute(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Execute the node.\n\n        Wrapper for _execute method to be implemented by subclasses.\n\n        Args:\n            params: Parameters to use to execute the node. Defaults to None.\n        \"\"\"\n        params = params or {}\n        run_loop = True\n\n        try:\n            self._pre_loop_hook(params)\n        except Exception:  # pylint: disable=broad-except\n            self.catch_exception()\n            raise\n\n        while run_loop is not False:\n            # Monitoring\n            try:\n                run_loop = self._execute(params)  # type: ignore\n            except Exception:  # pylint: disable=broad-except\n                self.catch_exception()\n                raise\n\n        self.log(f\"Execution loop complete for node: {self.__class__.__name__}\")\n        self._post_loop_hook(params)\n\n    def activate_poison_pill(self) -&gt; None:\n        \"\"\"Activates poison pill, shutting down entire pipeline.\"\"\"\n        if self.poison_pill:\n            ray.get(self.poison_pill.activate.remote())\n\n    @abstractmethod\n    def _execute(self, params: dict) -&gt; Optional[bool]:\n        \"\"\"Execute the node.\n\n        Args:\n            params: Parameters to use to execute the node.\n\n        Note:\n            Method to be implemented by subclasses\n\n        Raises:\n            NotImplementedError: if method is not implemented in subclass\n        \"\"\"\n        raise NotImplementedError(\"_execute method not implemented\")\n\n    def run_test(self, runtime: Optional[int] = None) -&gt; dict:\n        \"\"\"Execute the node in testing mode.\n\n        Runs the steps in execute that involves the user defined methods.\n        Includes pre_loop_hook, _execute, and post_loop_hook.\n\n        Args:\n            runtime: Number of seconds to run the execute loop for.\n\n        Returns:\n            dict: dataset names and values produced by the node.\n        \"\"\"\n        if self.test is False:\n            raise RuntimeError(\n                \"Node is not in test mode. \"\n                \"Please initialize with `enable_test_mode()`.\"\n            )\n        run_loop = True\n        start_time = time.time()\n\n        self._pre_loop_hook(self.params)\n        while run_loop is not False:\n            run_loop = self._execute(self.params)  # type: ignore\n\n            # Do not end loop if runtime not exceeded\n            if runtime is not None:\n                if time.time() - start_time &lt; runtime:\n                    continue\n\n            # End loop if all consumers are empty\n            if self.consumers and all(\n                consumer.empty for consumer in self.consumers.values()\n            ):\n                run_loop = False\n\n        self._post_loop_hook(self.params)\n\n        return {\n            dataset_name: producer.values\n            for dataset_name, producer in self.producers.items()\n        }\n\n    def run_test_yield(\n        self, runtime: Optional[int] = None\n    ) -&gt; Generator[Tuple[dict, dict, \"AbstractNode\"], None, None]:\n        \"\"\"Execute the node in testing mode, yielding at each iteration.\n\n        This method is an alternative to `run_test`. Instead of returning the\n        aggregated output, it yields the most recently consumed value, the\n        produced value and the current node instance at each iteration. This is\n        useful for testing nodes that either don't produce any output or if you\n        need to test intermediate outputs. Testing state modifications is also\n        possible using this method.\n\n        Args:\n            runtime: Number of seconds to run the execute loop for.\n\n        Yields:\n            A tuple containing the most recent input value, output value and\n            the node instance.\n\n        Example:\n            &gt;&gt;&gt; for input, output, node_instance in sequencer.run_test_yield():\n            &gt;&gt;&gt;     print(f\"Input: {input}, Output: {output})\n            &gt;&gt;&gt;     print(f\"Node Instance: {node_instance}\")\n        \"\"\"\n        if self.test is False:\n            raise RuntimeError(\n                \"Node is not in test mode. \"\n                \"Please initialize with `enable_test_mode()`.\"\n            )\n        run_loop = True\n        start_time = time.time()\n\n        self._pre_loop_hook(self.params)\n        while run_loop is not False:\n            last_produced_values = {}\n            last_consumed_values = {}\n\n            run_loop = self._execute(self.params)  # type: ignore\n\n            # Do not end loop if runtime not exceeded\n            if runtime is not None:\n                if time.time() - start_time &lt; runtime:\n                    continue\n\n            # End loop if all consumers are empty\n            if self.consumers and all(\n                consumer.empty for consumer in self.consumers.values()\n            ):\n                run_loop = False\n\n            # Capture last consumed values\n            for dataset_name, consumer in self.consumers.items():\n                if consumer.values:\n                    last_value = consumer.values[0]\n                    last_consumed_values[dataset_name] = last_value\n\n            # Capture last produced values\n            for dataset_name, producer in self.producers.items():\n                if producer.values:\n                    last_value = producer.values[-1]\n                    last_produced_values[dataset_name] = last_value\n\n            yield (last_consumed_values, last_produced_values, self)\n\n        self._post_loop_hook(self.params)\n\n    def _pre_loop_hook(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Hook to be called before the node loop. User overrideable.\n\n        Args:\n            params: Parameters to use to execute the node.\n\n        Note:\n            Method (optional) to be implemented by subclasses.\n        \"\"\"\n        pass\n\n    def _post_loop_hook(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Hook to be called after the node loop. User overrideable.\n\n        Args:\n            params: Parameters to use to execute the node.\n\n        Note:\n            Method (optional) to be implemented by subclasses.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.__init__","title":"<code>__init__(poison_pill=None, test=False)</code>","text":"<p>Initialize the node.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def __init__(\n    self,\n    poison_pill: Optional[ray.actor.ActorHandle] = None,\n    test: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the node.\"\"\"\n    self.last_heartbeat = time.time()\n    self.consumers: Dict = {}\n    self.producers: Dict = {}\n    self.params: Dict = {}\n    self.test = test\n    self.log_levels = AINEKO_CONFIG.get(\"LOG_LEVELS\")\n    self.poison_pill = poison_pill\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.activate_poison_pill","title":"<code>activate_poison_pill()</code>","text":"<p>Activates poison pill, shutting down entire pipeline.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def activate_poison_pill(self) -&gt; None:\n    \"\"\"Activates poison pill, shutting down entire pipeline.\"\"\"\n    if self.poison_pill:\n        ray.get(self.poison_pill.activate.remote())\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.catch_exception","title":"<code>catch_exception()</code>","text":"<p>Catch an exception and report it.</p> <p>Parameters:</p> Name Type Description Default <code>exc</code> <p>Exception to catch</p> required Source code in <code>aineko/core/node.py</code> <pre><code>def catch_exception(self) -&gt; None:\n    \"\"\"Catch an exception and report it.\n\n    Args:\n        exc: Exception to catch\n    \"\"\"\n    exc_info = traceback.format_exc()\n    self.log(exc_info, level=\"error\")\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.enable_test_mode","title":"<code>enable_test_mode()</code>","text":"<p>Enable test mode.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def enable_test_mode(self) -&gt; None:\n    \"\"\"Enable test mode.\"\"\"\n    self.test = True\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.execute","title":"<code>execute(params=None)</code>","text":"<p>Execute the node.</p> <p>Wrapper for _execute method to be implemented by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Optional[dict]</code> <p>Parameters to use to execute the node. Defaults to None.</p> <code>None</code> Source code in <code>aineko/core/node.py</code> <pre><code>def execute(self, params: Optional[dict] = None) -&gt; None:\n    \"\"\"Execute the node.\n\n    Wrapper for _execute method to be implemented by subclasses.\n\n    Args:\n        params: Parameters to use to execute the node. Defaults to None.\n    \"\"\"\n    params = params or {}\n    run_loop = True\n\n    try:\n        self._pre_loop_hook(params)\n    except Exception:  # pylint: disable=broad-except\n        self.catch_exception()\n        raise\n\n    while run_loop is not False:\n        # Monitoring\n        try:\n            run_loop = self._execute(params)  # type: ignore\n        except Exception:  # pylint: disable=broad-except\n            self.catch_exception()\n            raise\n\n    self.log(f\"Execution loop complete for node: {self.__class__.__name__}\")\n    self._post_loop_hook(params)\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.log","title":"<code>log(message, level='info')</code>","text":"<p>Log a message to the logging dataset.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message to log</p> required <code>level</code> <code>str</code> <p>Logging level. Defaults to \"info\". Options are: \"info\", \"debug\", \"warning\", \"error\", \"critical\"</p> <code>'info'</code> <p>Raises:     ValueError: if invalid logging level is provided</p> Source code in <code>aineko/core/node.py</code> <pre><code>def log(self, message: str, level: str = \"info\") -&gt; None:\n    \"\"\"Log a message to the logging dataset.\n\n    Args:\n        message: Message to log\n        level: Logging level. Defaults to \"info\". Options are:\n            \"info\", \"debug\", \"warning\", \"error\", \"critical\"\n    Raises:\n        ValueError: if invalid logging level is provided\n    \"\"\"\n    if level not in self.log_levels:\n        raise ValueError(\n            f\"Invalid logging level {level}. Valid options are: \"\n            f\"{', '.join(self.log_levels)}\"\n        )\n    out_msg = {\"log\": message, \"level\": level}\n    self.producers[DEFAULT_KAFKA_CONFIG.get(\"LOGGING_DATASET\")].produce(\n        out_msg\n    )\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.run_test","title":"<code>run_test(runtime=None)</code>","text":"<p>Execute the node in testing mode.</p> <p>Runs the steps in execute that involves the user defined methods. Includes pre_loop_hook, _execute, and post_loop_hook.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>Optional[int]</code> <p>Number of seconds to run the execute loop for.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dataset names and values produced by the node.</p> Source code in <code>aineko/core/node.py</code> <pre><code>def run_test(self, runtime: Optional[int] = None) -&gt; dict:\n    \"\"\"Execute the node in testing mode.\n\n    Runs the steps in execute that involves the user defined methods.\n    Includes pre_loop_hook, _execute, and post_loop_hook.\n\n    Args:\n        runtime: Number of seconds to run the execute loop for.\n\n    Returns:\n        dict: dataset names and values produced by the node.\n    \"\"\"\n    if self.test is False:\n        raise RuntimeError(\n            \"Node is not in test mode. \"\n            \"Please initialize with `enable_test_mode()`.\"\n        )\n    run_loop = True\n    start_time = time.time()\n\n    self._pre_loop_hook(self.params)\n    while run_loop is not False:\n        run_loop = self._execute(self.params)  # type: ignore\n\n        # Do not end loop if runtime not exceeded\n        if runtime is not None:\n            if time.time() - start_time &lt; runtime:\n                continue\n\n        # End loop if all consumers are empty\n        if self.consumers and all(\n            consumer.empty for consumer in self.consumers.values()\n        ):\n            run_loop = False\n\n    self._post_loop_hook(self.params)\n\n    return {\n        dataset_name: producer.values\n        for dataset_name, producer in self.producers.items()\n    }\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.run_test_yield","title":"<code>run_test_yield(runtime=None)</code>","text":"<p>Execute the node in testing mode, yielding at each iteration.</p> <p>This method is an alternative to <code>run_test</code>. Instead of returning the aggregated output, it yields the most recently consumed value, the produced value and the current node instance at each iteration. This is useful for testing nodes that either don't produce any output or if you need to test intermediate outputs. Testing state modifications is also possible using this method.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>Optional[int]</code> <p>Number of seconds to run the execute loop for.</p> <code>None</code> <p>Yields:</p> Type Description <code>dict</code> <p>A tuple containing the most recent input value, output value and</p> <code>dict</code> <p>the node instance.</p> Example <p>for input, output, node_instance in sequencer.run_test_yield():     print(f\"Input: {input}, Output: {output})     print(f\"Node Instance: {node_instance}\")</p> Source code in <code>aineko/core/node.py</code> <pre><code>def run_test_yield(\n    self, runtime: Optional[int] = None\n) -&gt; Generator[Tuple[dict, dict, \"AbstractNode\"], None, None]:\n    \"\"\"Execute the node in testing mode, yielding at each iteration.\n\n    This method is an alternative to `run_test`. Instead of returning the\n    aggregated output, it yields the most recently consumed value, the\n    produced value and the current node instance at each iteration. This is\n    useful for testing nodes that either don't produce any output or if you\n    need to test intermediate outputs. Testing state modifications is also\n    possible using this method.\n\n    Args:\n        runtime: Number of seconds to run the execute loop for.\n\n    Yields:\n        A tuple containing the most recent input value, output value and\n        the node instance.\n\n    Example:\n        &gt;&gt;&gt; for input, output, node_instance in sequencer.run_test_yield():\n        &gt;&gt;&gt;     print(f\"Input: {input}, Output: {output})\n        &gt;&gt;&gt;     print(f\"Node Instance: {node_instance}\")\n    \"\"\"\n    if self.test is False:\n        raise RuntimeError(\n            \"Node is not in test mode. \"\n            \"Please initialize with `enable_test_mode()`.\"\n        )\n    run_loop = True\n    start_time = time.time()\n\n    self._pre_loop_hook(self.params)\n    while run_loop is not False:\n        last_produced_values = {}\n        last_consumed_values = {}\n\n        run_loop = self._execute(self.params)  # type: ignore\n\n        # Do not end loop if runtime not exceeded\n        if runtime is not None:\n            if time.time() - start_time &lt; runtime:\n                continue\n\n        # End loop if all consumers are empty\n        if self.consumers and all(\n            consumer.empty for consumer in self.consumers.values()\n        ):\n            run_loop = False\n\n        # Capture last consumed values\n        for dataset_name, consumer in self.consumers.items():\n            if consumer.values:\n                last_value = consumer.values[0]\n                last_consumed_values[dataset_name] = last_value\n\n        # Capture last produced values\n        for dataset_name, producer in self.producers.items():\n            if producer.values:\n                last_value = producer.values[-1]\n                last_produced_values[dataset_name] = last_value\n\n        yield (last_consumed_values, last_produced_values, self)\n\n    self._post_loop_hook(self.params)\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.setup_datasets","title":"<code>setup_datasets(datasets, node, pipeline, inputs=None, outputs=None, prefix=None, has_pipeline_prefix=False)</code>","text":"<p>Setup the consumer and producer for a node.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Dict[str, dict]</code> <p>dataset configuration</p> required <code>node</code> <code>str</code> <p>name of the node</p> required <code>pipeline</code> <code>str</code> <p>name of the pipeline</p> required <code>inputs</code> <code>Optional[List[str]]</code> <p>list of dataset names for the inputs to the node</p> <code>None</code> <code>outputs</code> <code>Optional[List[str]]</code> <p>list of dataset names for the outputs of the node</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>prefix for topic name (<code>&lt;prefix&gt;.&lt;dataset_name&gt;</code>)</p> <code>None</code> <code>has_pipeline_prefix</code> <code>bool</code> <p>whether the dataset name has pipeline name</p> <code>False</code> Source code in <code>aineko/core/node.py</code> <pre><code>def setup_datasets(\n    self,\n    datasets: Dict[str, dict],\n    node: str,\n    pipeline: str,\n    inputs: Optional[List[str]] = None,\n    outputs: Optional[List[str]] = None,\n    prefix: Optional[str] = None,\n    has_pipeline_prefix: bool = False,\n) -&gt; None:\n    \"\"\"Setup the consumer and producer for a node.\n\n    Args:\n        datasets: dataset configuration\n        node: name of the node\n        pipeline: name of the pipeline\n        inputs: list of dataset names for the inputs to the node\n        outputs: list of dataset names for the outputs of the node\n        prefix: prefix for topic name (`&lt;prefix&gt;.&lt;dataset_name&gt;`)\n        has_pipeline_prefix: whether the dataset name has pipeline name\n        prefix\n    \"\"\"\n    inputs = inputs or []\n    self.consumers.update(\n        {\n            dataset_name: DatasetConsumer(\n                dataset_name=dataset_name,\n                node_name=node,\n                pipeline_name=pipeline,\n                dataset_config=datasets.get(dataset_name, {}),\n                prefix=prefix,\n                has_pipeline_prefix=has_pipeline_prefix,\n            )\n            for dataset_name in inputs\n        }\n    )\n\n    outputs = outputs or []\n    self.producers.update(\n        {\n            dataset_name: DatasetProducer(\n                dataset_name=dataset_name,\n                node_name=node,\n                pipeline_name=pipeline,\n                dataset_config=datasets.get(dataset_name, {}),\n                prefix=prefix,\n                has_pipeline_prefix=has_pipeline_prefix,\n            )\n            for dataset_name in outputs\n        }\n    )\n</code></pre>"},{"location":"api_reference/abstract_node/#aineko.AbstractNode.setup_test","title":"<code>setup_test(inputs=None, outputs=None, params=None)</code>","text":"<p>Setup the node for testing.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Optional[dict]</code> <p>inputs to the node, format should be {\"dataset\": [1, 2, 3]}</p> <code>None</code> <code>outputs</code> <code>Optional[list]</code> <p>outputs of the node, format should be [\"dataset_1\", \"dataset_2\", ...]</p> <code>None</code> <code>params</code> <code>Optional[dict]</code> <p>dictionary of parameters to make accessible to _execute</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if node is not in test mode</p> Source code in <code>aineko/core/node.py</code> <pre><code>def setup_test(\n    self,\n    inputs: Optional[dict] = None,\n    outputs: Optional[list] = None,\n    params: Optional[dict] = None,\n) -&gt; None:\n    \"\"\"Setup the node for testing.\n\n    Args:\n        inputs: inputs to the node, format should be {\"dataset\": [1, 2, 3]}\n        outputs: outputs of the node, format should be\n            [\"dataset_1\", \"dataset_2\", ...]\n        params: dictionary of parameters to make accessible to _execute\n\n    Raises:\n        RuntimeError: if node is not in test mode\n    \"\"\"\n    if self.test is False:\n        raise RuntimeError(\n            \"Node is not in test mode. \"\n            \"Please initialize with `enable_test_mode()`.\"\n        )\n\n    inputs = inputs or {}\n    self.consumers = {\n        dataset_name: FakeDatasetConsumer(\n            dataset_name=dataset_name,\n            node_name=self.__class__.__name__,\n            values=values,\n        )\n        for dataset_name, values in inputs.items()\n    }\n    outputs = outputs or []\n    outputs.extend(TESTING_NODE_CONFIG.get(\"DATASETS\"))\n    self.producers = {\n        dataset_name: FakeDatasetProducer(\n            dataset_name=dataset_name,\n            node_name=self.__class__.__name__,\n        )\n        for dataset_name in outputs\n    }\n    self.params = params or {}\n</code></pre>"},{"location":"api_reference/config/","title":"Pipeline <code>Config</code>","text":"<p>The following pydantic schema shows the format of a pipeline configuration file. Expand the following source code blocks to view the keys at each level.</p>"},{"location":"api_reference/config/#aineko.models.config_schema.Config","title":"<code>aineko.models.config_schema.Config</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Config model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Config(BaseModel):\n    \"\"\"Config model.\"\"\"\n\n    class Pipeline(BaseModel):\n        \"\"\"Pipeline model.\"\"\"\n\n        class Dataset(BaseModel):\n            \"\"\"Dataset model.\"\"\"\n\n            type: str\n            params: Optional[dict]\n\n        class Node(BaseModel):\n            \"\"\"Node model.\"\"\"\n\n            class_name: str = Field(..., alias=\"class\")\n            node_params: Optional[dict]\n            node_settings: Optional[dict]\n            inputs: Optional[list]\n            outputs: Optional[list]\n\n        name: str\n        default_node_settings: Optional[dict]\n        nodes: dict[str, Node]\n        datasets: dict[str, Dataset]\n\n    pipeline: Pipeline\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline","title":"<code>Pipeline</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Pipeline model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Pipeline(BaseModel):\n    \"\"\"Pipeline model.\"\"\"\n\n    class Dataset(BaseModel):\n        \"\"\"Dataset model.\"\"\"\n\n        type: str\n        params: Optional[dict]\n\n    class Node(BaseModel):\n        \"\"\"Node model.\"\"\"\n\n        class_name: str = Field(..., alias=\"class\")\n        node_params: Optional[dict]\n        node_settings: Optional[dict]\n        inputs: Optional[list]\n        outputs: Optional[list]\n\n    name: str\n    default_node_settings: Optional[dict]\n    nodes: dict[str, Node]\n    datasets: dict[str, Dataset]\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.Dataset","title":"<code>Dataset</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Dataset model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Dataset(BaseModel):\n    \"\"\"Dataset model.\"\"\"\n\n    type: str\n    params: Optional[dict]\n</code></pre>"},{"location":"api_reference/config/#aineko.models.config_schema.Config.Pipeline.Node","title":"<code>Node</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Node model.</p> Source code in <code>aineko/models/config_schema.py</code> <pre><code>class Node(BaseModel):\n    \"\"\"Node model.\"\"\"\n\n    class_name: str = Field(..., alias=\"class\")\n    node_params: Optional[dict]\n    node_settings: Optional[dict]\n    inputs: Optional[list]\n    outputs: Optional[list]\n</code></pre>"},{"location":"api_reference/config_loader/","title":"<code>ConfigLoader</code>","text":"<p>Reference for the <code>ConfigLoader</code> class, which contains the logic used for parsing a pipeline config file into a format that can be understood by the <code>Runner</code>.</p>"},{"location":"api_reference/config_loader/#aineko.ConfigLoader","title":"<code>aineko.ConfigLoader</code>","text":"<p>Class to read yaml config files.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_config_file</code> <code>str</code> <p>path of pipeline config file. Defaults</p> required <p>Attributes:</p> Name Type Description <code>pipeline_config_file</code> <code>str</code> <p>path to the pipeline configuration file</p> <code>config_schema</code> <code>Config</code> <p>Pydantic model to validate a pipeline config</p> <p>Methods:</p> Name Description <code>load_config</code> <p>loads and validates the pipeline config from a yaml file</p> Source code in <code>aineko/core/config_loader.py</code> <pre><code>class ConfigLoader:\n    \"\"\"Class to read yaml config files.\n\n    Args:\n        pipeline_config_file: path of pipeline config file. Defaults\n        to `DEFAULT_PIPELINE_CONFIG`.\n\n    Attributes:\n        pipeline_config_file (str): path to the pipeline configuration file\n        config_schema (Config): Pydantic model to validate a pipeline config\n\n    Methods:\n        load_config: loads and validates the pipeline config from a yaml file\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline_config_file: str,\n    ):\n        \"\"\"Initialize ConfigLoader.\"\"\"\n        self.pipeline_config_file = pipeline_config_file or AINEKO_CONFIG.get(\n            \"DEFAULT_PIPELINE_CONFIG\"\n        )\n\n        # Setup config schema\n        self.config_schema = Config\n\n    def load_config(self) -&gt; dict:\n        \"\"\"Load and validate the pipeline config.\n\n        Raises:\n            ValidationError: If the config does not match the schema\n\n        Returns:\n            The pipeline config as a dictionary\n        \"\"\"\n        config = load_yaml(self.pipeline_config_file)\n\n        try:\n            Config(**config)\n        except ValidationError as e:\n            logger.error(\n                \"Schema validation failed for pipeline `%s` loaded from %s. \"\n                \"See detailed error below.\",\n                config[\"pipeline\"][\"name\"],\n                self.pipeline_config_file,\n            )\n            raise e\n\n        return config\n\n    @overload\n    def _update_params(self, value: dict, params: dict) -&gt; dict:\n        ...\n\n    @overload\n    def _update_params(self, value: list, params: dict) -&gt; list:\n        ...\n\n    @overload\n    def _update_params(self, value: str, params: dict) -&gt; str:\n        ...\n\n    @overload\n    def _update_params(self, value: int, params: dict) -&gt; int:\n        ...\n\n    def _update_params(\n        self, value: Union[dict, list, str, int], params: dict\n    ) -&gt; Union[dict, list, str, int]:\n        \"\"\"Update value with params.\n\n        Recursively calls the method if value is a list or dictionary until it\n        reaches a string or int. If string then formats the str with variable\n        mapping in params dict.\n\n        Args:\n            value: value to update\n            params: params to update value with\n\n        Returns:\n            object with updated values (dict, list, str, or int)\n        \"\"\"\n        if isinstance(value, dict):\n            new_dict_val = {}\n            for key, val in value.items():\n                new_dict_val[key] = self._update_params(val, params)\n            return new_dict_val\n        if isinstance(value, list):\n            new_list_val: list = []\n            for val in value:\n                new_list_val.append(self._update_params(val, params))\n            return new_list_val\n        if isinstance(value, str):\n            for key, val in params.items():\n                value = value.replace(f\"${key}\", val)\n            return value\n        if isinstance(value, (int, float)):\n            return value\n        raise ValueError(\n            f\"Invalid value type {type(value)}. \"\n            \"Expected dict, list, str, or int.\"\n        )\n</code></pre>"},{"location":"api_reference/config_loader/#aineko.ConfigLoader.__init__","title":"<code>__init__(pipeline_config_file)</code>","text":"<p>Initialize ConfigLoader.</p> Source code in <code>aineko/core/config_loader.py</code> <pre><code>def __init__(\n    self,\n    pipeline_config_file: str,\n):\n    \"\"\"Initialize ConfigLoader.\"\"\"\n    self.pipeline_config_file = pipeline_config_file or AINEKO_CONFIG.get(\n        \"DEFAULT_PIPELINE_CONFIG\"\n    )\n\n    # Setup config schema\n    self.config_schema = Config\n</code></pre>"},{"location":"api_reference/config_loader/#aineko.ConfigLoader.load_config","title":"<code>load_config()</code>","text":"<p>Load and validate the pipeline config.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the config does not match the schema</p> <p>Returns:</p> Type Description <code>dict</code> <p>The pipeline config as a dictionary</p> Source code in <code>aineko/core/config_loader.py</code> <pre><code>def load_config(self) -&gt; dict:\n    \"\"\"Load and validate the pipeline config.\n\n    Raises:\n        ValidationError: If the config does not match the schema\n\n    Returns:\n        The pipeline config as a dictionary\n    \"\"\"\n    config = load_yaml(self.pipeline_config_file)\n\n    try:\n        Config(**config)\n    except ValidationError as e:\n        logger.error(\n            \"Schema validation failed for pipeline `%s` loaded from %s. \"\n            \"See detailed error below.\",\n            config[\"pipeline\"][\"name\"],\n            self.pipeline_config_file,\n        )\n        raise e\n\n    return config\n</code></pre>"},{"location":"api_reference/dataset_consumer/","title":"<code>DatasetConsumer</code>","text":"<p>The <code>DatasetConsumer</code> class is used to consume from <code>Kafka</code> topics.</p>"},{"location":"api_reference/dataset_consumer/#aineko.DatasetConsumer","title":"<code>aineko.DatasetConsumer</code>","text":"<p>Wrapper class for Kafka consumer object.</p> <p>DatasetConsumer objects are designed to consume messages from a single dataset and will consume the next unconsumed message in the queue.</p> <p>When accessing kafka topics, prefixes will automatically be added to the dataset name as part of namespacing. For datasets defined in the pipeline config, <code>has_pipeline_prefix</code> will be set to <code>True</code>, so a dataset named <code>my_dataset</code> will point to a topic named <code>my_pipeline.my_dataset</code>.</p> <p>Optionally, a custom prefix can be provided that will apply to all datasets. In the above example, if the prefix is set to <code>test</code>, the topic name will be <code>test.my_pipeline.my_dataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset</p> required <code>node_name</code> <code>str</code> <p>name of the node that is consuming the dataset</p> required <code>pipeline_name</code> <code>str</code> <p>name of the pipeline</p> required <code>dataset_config</code> <code>Dict[str, Any]</code> <p>dataset config</p> required <code>broker</code> <p>broker to connect to (ip and port: \"54.88.142.21:9092\")</p> required <code>prefix</code> <code>Optional[str]</code> <p>prefix for topic name (.) <code>None</code> <code>has_pipeline_prefix</code> <code>bool</code> <p>whether the dataset name has pipeline name prefix</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>consumer</code> <p>Kafka consumer object</p> <p>Methods:</p> Name Description <code>consume</code> <p>reads a message from the dataset</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>class DatasetConsumer:\n    \"\"\"Wrapper class for Kafka consumer object.\n\n    DatasetConsumer objects are designed to consume messages from a single\n    dataset and will consume the next unconsumed message in the queue.\n\n    When accessing kafka topics, prefixes will automatically be added to the\n    dataset name as part of namespacing. For datasets defined in the pipeline\n    config, `has_pipeline_prefix` will be set to `True`, so a dataset named\n    `my_dataset` will point to a topic named `my_pipeline.my_dataset`.\n\n    Optionally, a custom prefix can be provided that will apply to all datasets.\n    In the above example, if the prefix is set to `test`, the topic name will\n    be `test.my_pipeline.my_dataset`.\n\n    Args:\n        dataset_name: name of the dataset\n        node_name: name of the node that is consuming the dataset\n        pipeline_name: name of the pipeline\n        dataset_config: dataset config\n        broker: broker to connect to (ip and port: \"54.88.142.21:9092\")\n        prefix: prefix for topic name (&lt;prefix&gt;.&lt;dataset_name&gt;)\n        has_pipeline_prefix: whether the dataset name has pipeline name prefix\n\n    Attributes:\n        consumer: Kafka consumer object\n\n    Methods:\n        consume: reads a message from the dataset\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_name: str,\n        node_name: str,\n        pipeline_name: str,\n        dataset_config: Dict[str, Any],\n        bootstrap_servers: Optional[str] = None,\n        prefix: Optional[str] = None,\n        has_pipeline_prefix: bool = False,\n    ):\n        \"\"\"Initialize the consumer.\"\"\"\n        self.pipeline_name = pipeline_name\n        self.kafka_config = DEFAULT_KAFKA_CONFIG\n        self.prefix = prefix\n        self.has_pipeline_prefix = has_pipeline_prefix\n\n        consumer_config = self.kafka_config.get(\"CONSUMER_CONFIG\")\n        # Overwrite bootstrap server with broker if provided\n        if bootstrap_servers:\n            consumer_config[\"bootstrap.servers\"] = bootstrap_servers\n\n        # Override default config with dataset specific config\n        for param, value in dataset_config.get(\"params\", {}).items():\n            consumer_config[param] = value\n\n        topic_name = dataset_name\n        if has_pipeline_prefix:\n            topic_name = f\"{pipeline_name}.{dataset_name}\"\n\n        if self.prefix:\n            consumer_config[\n                \"group.id\"\n            ] = f\"{prefix}.{pipeline_name}.{node_name}\"\n            self.consumer = Consumer(consumer_config)\n            self.consumer.subscribe([f\"{prefix}.{topic_name}\"])\n\n        else:\n            consumer_config[\"group.id\"] = f\"{pipeline_name}.{node_name}\"\n            self.consumer = Consumer(consumer_config)\n            self.consumer.subscribe([topic_name])\n\n        self.topic_name = topic_name\n\n    @staticmethod\n    def _validate_message(\n        message: Optional[Message] = None,\n    ) -&gt; Optional[dict]:\n        \"\"\"Checks if a message is valid and converts it to appropriate format.\n\n        Args:\n            message: message to check\n\n        Returns:\n            message: message if valid, None if not\n        \"\"\"\n        # Check if message is valid\n        if message is None or message.value() is None:\n            return None\n\n        # Check if message is an error\n        if message.error():\n            logger.error(str(message.error()))\n            return None\n\n        # Convert message to dict\n        message = message.value()\n        if isinstance(message, bytes):\n            message = message.decode(\"utf-8\")\n        return json.loads(message)\n\n    def consume(\n        self,\n        how: str = \"next\",\n        timeout: Optional[int] = None,\n    ) -&gt; Optional[dict]:\n        \"\"\"Reads a message from the dataset.\n\n        Args:\n            how: how to read the message\n                \"next\": read the next message in the queue\n\n        Returns:\n            msg: message from the dataset\n        \"\"\"\n        timeout = timeout or self.kafka_config.get(\"CONSUMER_TIMEOUT\")\n        if how == \"next\":\n            # next unread message from queue\n            message = self._validate_message(\n                self.consumer.poll(timeout=timeout)\n            )\n        else:\n            raise ValueError(f\"Invalid how: {how}. Expected 'next'.\")\n\n        return message\n\n    def consume_all(self, end_message: str | bool = False) -&gt; list:\n        \"\"\"Reads all messages from the dataset until a specific one is found.\n\n        Args:\n            end_message: Message to trigger the completion of consumption\n\n        Returns:\n            list: list of messages from the dataset\n        \"\"\"\n        messages = []\n        while True:\n            message = self.consume()\n            if message is None:\n                continue\n            if message[\"message\"] == end_message:\n                break\n            messages.append(message)\n        return messages\n</code></pre>"},{"location":"api_reference/dataset_consumer/#aineko.DatasetConsumer.__init__","title":"<code>__init__(dataset_name, node_name, pipeline_name, dataset_config, bootstrap_servers=None, prefix=None, has_pipeline_prefix=False)</code>","text":"<p>Initialize the consumer.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_name: str,\n    node_name: str,\n    pipeline_name: str,\n    dataset_config: Dict[str, Any],\n    bootstrap_servers: Optional[str] = None,\n    prefix: Optional[str] = None,\n    has_pipeline_prefix: bool = False,\n):\n    \"\"\"Initialize the consumer.\"\"\"\n    self.pipeline_name = pipeline_name\n    self.kafka_config = DEFAULT_KAFKA_CONFIG\n    self.prefix = prefix\n    self.has_pipeline_prefix = has_pipeline_prefix\n\n    consumer_config = self.kafka_config.get(\"CONSUMER_CONFIG\")\n    # Overwrite bootstrap server with broker if provided\n    if bootstrap_servers:\n        consumer_config[\"bootstrap.servers\"] = bootstrap_servers\n\n    # Override default config with dataset specific config\n    for param, value in dataset_config.get(\"params\", {}).items():\n        consumer_config[param] = value\n\n    topic_name = dataset_name\n    if has_pipeline_prefix:\n        topic_name = f\"{pipeline_name}.{dataset_name}\"\n\n    if self.prefix:\n        consumer_config[\n            \"group.id\"\n        ] = f\"{prefix}.{pipeline_name}.{node_name}\"\n        self.consumer = Consumer(consumer_config)\n        self.consumer.subscribe([f\"{prefix}.{topic_name}\"])\n\n    else:\n        consumer_config[\"group.id\"] = f\"{pipeline_name}.{node_name}\"\n        self.consumer = Consumer(consumer_config)\n        self.consumer.subscribe([topic_name])\n\n    self.topic_name = topic_name\n</code></pre>"},{"location":"api_reference/dataset_consumer/#aineko.DatasetConsumer.consume","title":"<code>consume(how='next', timeout=None)</code>","text":"<p>Reads a message from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>how</code> <code>str</code> <p>how to read the message \"next\": read the next message in the queue</p> <code>'next'</code> <p>Returns:</p> Name Type Description <code>msg</code> <code>Optional[dict]</code> <p>message from the dataset</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>def consume(\n    self,\n    how: str = \"next\",\n    timeout: Optional[int] = None,\n) -&gt; Optional[dict]:\n    \"\"\"Reads a message from the dataset.\n\n    Args:\n        how: how to read the message\n            \"next\": read the next message in the queue\n\n    Returns:\n        msg: message from the dataset\n    \"\"\"\n    timeout = timeout or self.kafka_config.get(\"CONSUMER_TIMEOUT\")\n    if how == \"next\":\n        # next unread message from queue\n        message = self._validate_message(\n            self.consumer.poll(timeout=timeout)\n        )\n    else:\n        raise ValueError(f\"Invalid how: {how}. Expected 'next'.\")\n\n    return message\n</code></pre>"},{"location":"api_reference/dataset_consumer/#aineko.DatasetConsumer.consume_all","title":"<code>consume_all(end_message=False)</code>","text":"<p>Reads all messages from the dataset until a specific one is found.</p> <p>Parameters:</p> Name Type Description Default <code>end_message</code> <code>str | bool</code> <p>Message to trigger the completion of consumption</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of messages from the dataset</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>def consume_all(self, end_message: str | bool = False) -&gt; list:\n    \"\"\"Reads all messages from the dataset until a specific one is found.\n\n    Args:\n        end_message: Message to trigger the completion of consumption\n\n    Returns:\n        list: list of messages from the dataset\n    \"\"\"\n    messages = []\n    while True:\n        message = self.consume()\n        if message is None:\n            continue\n        if message[\"message\"] == end_message:\n            break\n        messages.append(message)\n    return messages\n</code></pre>"},{"location":"api_reference/dataset_producer/","title":"<code>DatasetProducer</code>","text":"<p>The <code>DatasetProducer</code> class is used to produce to <code>kafka</code> topics.</p>"},{"location":"api_reference/dataset_producer/#aineko.DatasetProducer","title":"<code>aineko.DatasetProducer</code>","text":"<p>Wrapper class for Kafka producer object.</p> <p>See DatasetConsumer for prefix rules.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>dataset name</p> required <code>node_name</code> <code>str</code> <p>name of the node that is producing the message</p> required <code>pipeline_name</code> <code>str</code> <p>name of the pipeline</p> required <code>dataset_config</code> <code>Dict[str, Any]</code> <p>dataset config</p> required <code>prefix</code> <code>Optional[str]</code> <p>prefix for topic name (.) <code>None</code> <code>has_pipeline_prefix</code> <code>bool</code> <p>whether the dataset name has pipeline name prefix</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>producer</code> <p>Kafka producer object</p> <p>Methods:</p> Name Description <code>produce</code> <p>produce a message to the dataset</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>class DatasetProducer:\n    \"\"\"Wrapper class for Kafka producer object.\n\n    See DatasetConsumer for prefix rules.\n\n    Args:\n        dataset_name: dataset name\n        node_name: name of the node that is producing the message\n        pipeline_name: name of the pipeline\n        dataset_config: dataset config\n        prefix: prefix for topic name (&lt;prefix&gt;.&lt;dataset_name&gt;)\n        has_pipeline_prefix: whether the dataset name has pipeline name prefix\n\n    Attributes:\n        producer: Kafka producer object\n\n    Methods:\n        produce: produce a message to the dataset\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_name: str,\n        node_name: str,\n        pipeline_name: str,\n        dataset_config: Dict[str, Any],\n        prefix: Optional[str] = None,\n        has_pipeline_prefix: bool = False,\n    ):\n        \"\"\"Initialize the producer.\"\"\"\n        self.source_pipeline = pipeline_name\n        self.dataset = dataset_name\n        self.source_node = node_name\n        self.prefix = prefix\n        self.has_pipeline_prefix = has_pipeline_prefix\n\n        # Create topic name based on prefix rules\n        topic_name = dataset_name\n        if has_pipeline_prefix:\n            topic_name = f\"{pipeline_name}.{topic_name}\"\n        if prefix:\n            topic_name = f\"{prefix}.{topic_name}\"\n        self.topic_name = topic_name\n\n        # Assign kafka config\n        self.kafka_config = DEFAULT_KAFKA_CONFIG\n\n        # Set producer parameters\n        producer_config = self.kafka_config.get(\"PRODUCER_CONFIG\")\n\n        # Override default config with dataset specific config\n        if \"params\" in dataset_config:\n            for param in self.kafka_config.get(\"PRODUCER_OVERRIDABLES\"):\n                if param in dataset_config[\"params\"]:\n                    producer_config[param] = dataset_config[\"params\"][param]\n\n        # Create producer\n        self.producer = Producer(producer_config)\n\n    @staticmethod\n    def _delivery_report(err: Any, message: Message) -&gt; None:\n        \"\"\"Called once for each message produced to indicate delivery result.\n\n        Triggered by poll() or flush().\n\n        Args:\n            err: error message\n            message: message object from Kafka\n        \"\"\"\n        if err is not None:\n            logger.error(\"Message %s delivery failed: %s\", message, err)\n\n    def produce(self, message: dict, key: Optional[str] = None) -&gt; None:\n        \"\"\"Produce a message to the dataset.\n\n        Args:\n            message: message to produce to the dataset\n            key: key to use for the message\n        \"\"\"\n        message = {\n            \"timestamp\": datetime.datetime.now().strftime(\n                AINEKO_CONFIG.get(\"MSG_TIMESTAMP_FORMAT\")\n            ),\n            \"dataset\": self.dataset,\n            \"source_pipeline\": self.source_pipeline,\n            \"source_node\": self.source_node,\n            \"message\": message,\n        }\n        self.producer.poll(0)\n\n        key_bytes = str(key).encode(\"utf-8\") if key is not None else None\n\n        self.producer.produce(\n            topic=self.topic_name,\n            key=key_bytes,\n            value=json.dumps(message).encode(\"utf-8\"),\n            callback=self._delivery_report,\n        )\n        self.producer.flush()\n</code></pre>"},{"location":"api_reference/dataset_producer/#aineko.DatasetProducer.__init__","title":"<code>__init__(dataset_name, node_name, pipeline_name, dataset_config, prefix=None, has_pipeline_prefix=False)</code>","text":"<p>Initialize the producer.</p> Source code in <code>aineko/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_name: str,\n    node_name: str,\n    pipeline_name: str,\n    dataset_config: Dict[str, Any],\n    prefix: Optional[str] = None,\n    has_pipeline_prefix: bool = False,\n):\n    \"\"\"Initialize the producer.\"\"\"\n    self.source_pipeline = pipeline_name\n    self.dataset = dataset_name\n    self.source_node = node_name\n    self.prefix = prefix\n    self.has_pipeline_prefix = has_pipeline_prefix\n\n    # Create topic name based on prefix rules\n    topic_name = dataset_name\n    if has_pipeline_prefix:\n        topic_name = f\"{pipeline_name}.{topic_name}\"\n    if prefix:\n        topic_name = f\"{prefix}.{topic_name}\"\n    self.topic_name = topic_name\n\n    # Assign kafka config\n    self.kafka_config = DEFAULT_KAFKA_CONFIG\n\n    # Set producer parameters\n    producer_config = self.kafka_config.get(\"PRODUCER_CONFIG\")\n\n    # Override default config with dataset specific config\n    if \"params\" in dataset_config:\n        for param in self.kafka_config.get(\"PRODUCER_OVERRIDABLES\"):\n            if param in dataset_config[\"params\"]:\n                producer_config[param] = dataset_config[\"params\"][param]\n\n    # Create producer\n    self.producer = Producer(producer_config)\n</code></pre>"},{"location":"api_reference/dataset_producer/#aineko.DatasetProducer.produce","title":"<code>produce(message, key=None)</code>","text":"<p>Produce a message to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>dict</code> <p>message to produce to the dataset</p> required <code>key</code> <code>Optional[str]</code> <p>key to use for the message</p> <code>None</code> Source code in <code>aineko/core/dataset.py</code> <pre><code>def produce(self, message: dict, key: Optional[str] = None) -&gt; None:\n    \"\"\"Produce a message to the dataset.\n\n    Args:\n        message: message to produce to the dataset\n        key: key to use for the message\n    \"\"\"\n    message = {\n        \"timestamp\": datetime.datetime.now().strftime(\n            AINEKO_CONFIG.get(\"MSG_TIMESTAMP_FORMAT\")\n        ),\n        \"dataset\": self.dataset,\n        \"source_pipeline\": self.source_pipeline,\n        \"source_node\": self.source_node,\n        \"message\": message,\n    }\n    self.producer.poll(0)\n\n    key_bytes = str(key).encode(\"utf-8\") if key is not None else None\n\n    self.producer.produce(\n        topic=self.topic_name,\n        key=key_bytes,\n        value=json.dumps(message).encode(\"utf-8\"),\n        callback=self._delivery_report,\n    )\n    self.producer.flush()\n</code></pre>"},{"location":"api_reference/runner/","title":"<code>Runner</code>","text":"<p>The <code>Runner</code> builds the pipeline from the given pipeline configuration file, creating the necessary datasets and orchestrating the nodes.</p>"},{"location":"api_reference/runner/#aineko.Runner","title":"<code>aineko.Runner</code>","text":"<p>Runs the pipeline described in the config.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_config_file</code> <code>str</code> <p>Path to pipeline config file</p> required <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline</p> <code>None</code> <code>kafka_config</code> <code>dict</code> <p>Config for kafka broker</p> <code>get('BROKER_CONFIG')</code> <code>dataset_prefix</code> <code>Optional[str]</code> <p>Prefix for dataset names.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>pipeline_config_file</code> <code>str</code> <p>Path to pipeline config file</p> <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline, overrides pipeline config</p> <code>kafka_config</code> <code>dict</code> <p>Config for kafka broker</p> <code>pipeline_name</code> <code>str</code> <p>Name of the pipeline, loaded from config</p> <code>dataset_prefix</code> <code>Optional[str]</code> <p>Prefix for dataset names</p> Source code in <code>aineko/core/runner.py</code> <pre><code>class Runner:\n    \"\"\"Runs the pipeline described in the config.\n\n    Args:\n        pipeline_config_file (str): Path to pipeline config file\n        pipeline_name (str): Name of the pipeline\n        kafka_config (dict): Config for kafka broker\n        dataset_prefix (Optional[str]): Prefix for dataset names.\n        Kafka topics will be called &lt;prefix&gt;.&lt;pipeline&gt;.&lt;dataset_name&gt;.\n\n    Attributes:\n        pipeline_config_file (str): Path to pipeline config file\n        pipeline_name (str): Name of the pipeline, overrides pipeline config\n        kafka_config (dict): Config for kafka broker\n        pipeline_name (str): Name of the pipeline, loaded from config\n        dataset_prefix (Optional[str]): Prefix for dataset names\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline_config_file: str,\n        pipeline_name: Optional[str] = None,\n        kafka_config: dict = DEFAULT_KAFKA_CONFIG.get(\"BROKER_CONFIG\"),\n        metrics_export_port: int = AINEKO_CONFIG.get(\"RAY_METRICS_PORT\"),\n        dataset_prefix: Optional[str] = None,\n    ):\n        \"\"\"Initializes the runner class.\"\"\"\n        self.pipeline_config_file = pipeline_config_file\n        self.kafka_config = kafka_config\n        self.metrics_export_port = metrics_export_port\n        self.pipeline_name = pipeline_name\n        self.dataset_prefix = dataset_prefix or \"\"\n\n    def run(self) -&gt; None:\n        \"\"\"Runs the pipeline.\n\n        Step 1: Load config for pipeline\n\n        Step 2: Set up datasets\n\n        Step 3: Set up PoisonPill node that is available to all nodes\n\n        Step 4: Set up nodes (including Node Manager)\n        \"\"\"\n        # Load pipeline config\n        pipeline_config = self.load_pipeline_config()\n        self.pipeline_name = self.pipeline_name or pipeline_config[\"name\"]\n\n        # Create the necessary datasets\n        self.prepare_datasets(\n            config=pipeline_config[\"datasets\"],\n            user_dataset_prefix=self.pipeline_name,\n        )\n\n        # Initialize ray cluster\n        ray.shutdown()\n        ray.init(\n            namespace=self.pipeline_name,\n            _metrics_export_port=self.metrics_export_port,\n        )\n\n        # Create poison pill actor\n        poison_pill = ray.remote(PoisonPill).remote()\n\n        # Add Node Manager to pipeline config\n        pipeline_config[\"nodes\"][\n            NODE_MANAGER_CONFIG.get(\"NAME\")\n        ] = NODE_MANAGER_CONFIG.get(\"NODE_CONFIG\")\n\n        # Create each node (actor)\n        results = self.prepare_nodes(\n            pipeline_config=pipeline_config,\n            poison_pill=poison_pill,  # type: ignore\n        )\n\n        ray.get(results)\n\n    def load_pipeline_config(self) -&gt; dict:\n        \"\"\"Loads the config for a given pipeline.\n\n        Returns:\n            pipeline config\n        \"\"\"\n        config = ConfigLoader(\n            pipeline_config_file=self.pipeline_config_file,\n        ).load_config()\n\n        return config[\"pipeline\"]\n\n    def prepare_datasets(\n        self, config: dict, user_dataset_prefix: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Creates the required datasets for a given pipeline.\n\n        Datasets can be configured using the `params` key, using config keys\n        found in: https://kafka.apache.org/documentation.html#topicconfigs\n\n        Args:\n            config: dataset configuration found in pipeline config\n            Should follow the schema\n                {\n                    \"dataset_name\": {\n                        \"type\": str (\"kafka_stream\"),\n                        \"params\": dict\n                }\n            user_dataset_prefix: prefix only for datasets defined by the user.\n            i.e. &lt;prefix&gt;.&lt;user_dataset_prefix&gt;.&lt;dataset_name&gt;\n\n        Returns:\n            True if successful\n\n        Raises:\n            ValueError: if dataset \"logging\" is defined in the catalog\n        \"\"\"\n        # Connect to kafka cluster\n        kafka_client = AdminClient(self.kafka_config)\n\n        # Add prefix to user defined datasets\n        if user_dataset_prefix:\n            config = {\n                f\"{user_dataset_prefix}.{dataset_name}\": dataset_config\n                for dataset_name, dataset_config in config.items()\n            }\n\n        # Fail if reserved dataset names are defined in catalog\n        for reserved_dataset in DEFAULT_KAFKA_CONFIG.get(\"DATASETS\"):\n            if reserved_dataset in config:\n                raise ValueError(\n                    f\"Unable to create dataset `{reserved_dataset}`. \"\n                    \"Reserved for internal use.\"\n                )\n\n        # Add logging dataset to catalog\n        config[DEFAULT_KAFKA_CONFIG.get(\"LOGGING_DATASET\")] = {\n            \"type\": AINEKO_CONFIG.get(\"KAFKA_STREAM_TYPE\"),\n            \"params\": DEFAULT_KAFKA_CONFIG.get(\"DATASET_PARAMS\"),\n        }\n\n        # Create all dataset defined in the catalog\n        dataset_list = []\n        for dataset_name, dataset_config in config.items():\n            logger.info(\n                \"Creating dataset: %s: %s\", dataset_name, dataset_config\n            )\n            # Create dataset for kafka streams\n            if dataset_config[\"type\"] == AINEKO_CONFIG.get(\"KAFKA_STREAM_TYPE\"):\n                # User defined\n                dataset_params = {\n                    **DEFAULT_KAFKA_CONFIG.get(\"DATASET_PARAMS\"),\n                    **dataset_config.get(\"params\", {}),\n                }\n\n                # Configure dataset\n                if self.dataset_prefix:\n                    topic_name = f\"{self.dataset_prefix}.{dataset_name}\"\n                else:\n                    topic_name = dataset_name\n\n                new_dataset = NewTopic(\n                    topic=topic_name,\n                    num_partitions=dataset_params.get(\"num_partitions\"),\n                    replication_factor=dataset_params.get(\"replication_factor\"),\n                    config=dataset_params.get(\"config\"),\n                )\n\n                # Add dataset to appropriate list\n                dataset_list.append(new_dataset)\n\n            else:\n                raise ValueError(\n                    \"Unknown dataset type. Expected: \"\n                    f\"{AINEKO_CONFIG.get('STREAM_TYPES')}.\"\n                )\n\n        # Create all configured datasets\n        datasets = kafka_client.create_topics(dataset_list)\n\n        # Block until all datasets finish creation\n        cur_time = time.time()\n        while True:\n            if all(future.done() for future in datasets.values()):\n                logger.info(\"All datasets created.\")\n                break\n            if time.time() - cur_time &gt; AINEKO_CONFIG.get(\n                \"DATASET_CREATION_TIMEOUT\"\n            ):\n                raise TimeoutError(\n                    \"Timeout while creating Kafka datasets. \"\n                    \"Please check your Kafka cluster.\"\n                )\n\n        return datasets\n\n    def prepare_nodes(\n        self, pipeline_config: dict, poison_pill: ray.actor.ActorHandle\n    ) -&gt; list:\n        \"\"\"Prepare actor handles for all nodes.\n\n        Args:\n            pipeline_config: pipeline configuration\n\n        Returns:\n            dict: mapping of node names to actor handles\n            list: list of ray objects\n        \"\"\"\n        # Collect all  actor futures\n        results = []\n\n        default_node_config = pipeline_config.get(\"default_node_settings\", {})\n\n        for node_name, node_config in pipeline_config[\"nodes\"].items():\n            # Initialize actor from specified class in config\n            target_class = imports.import_from_string(\n                attr=node_config[\"class\"], kind=\"class\"\n            )\n            actor_params = {\n                **default_node_config,\n                **node_config.get(\"node_settings\", {}),\n                \"name\": node_name,\n                \"namespace\": self.pipeline_name,\n            }\n\n            wrapped_class = ray.remote(target_class)\n            wrapped_class.options(**actor_params)\n            actor_handle = wrapped_class.remote(poison_pill=poison_pill)\n\n            # Setup input and output datasets\n            outputs = node_config.get(\"outputs\", [])\n            actor_handle.setup_datasets.remote(\n                inputs=node_config.get(\"inputs\", None),\n                outputs=outputs,\n                datasets=pipeline_config[\"datasets\"],\n                node=node_name,\n                pipeline=self.pipeline_name,\n                has_pipeline_prefix=True,\n            )\n\n            # Setup internal datasets like logging, without pipeline prefix\n            actor_handle.setup_datasets.remote(\n                outputs=DEFAULT_KAFKA_CONFIG.get(\"DATASETS\"),\n                datasets=pipeline_config[\"datasets\"],\n                node=node_name,\n                pipeline=self.pipeline_name,\n            )\n\n            # Create actor future (for execute method)\n            results.append(\n                actor_handle.execute.remote(\n                    params=node_config.get(\"node_params\", None)\n                )\n            )\n\n        return results\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.__init__","title":"<code>__init__(pipeline_config_file, pipeline_name=None, kafka_config=DEFAULT_KAFKA_CONFIG.get('BROKER_CONFIG'), metrics_export_port=AINEKO_CONFIG.get('RAY_METRICS_PORT'), dataset_prefix=None)</code>","text":"<p>Initializes the runner class.</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def __init__(\n    self,\n    pipeline_config_file: str,\n    pipeline_name: Optional[str] = None,\n    kafka_config: dict = DEFAULT_KAFKA_CONFIG.get(\"BROKER_CONFIG\"),\n    metrics_export_port: int = AINEKO_CONFIG.get(\"RAY_METRICS_PORT\"),\n    dataset_prefix: Optional[str] = None,\n):\n    \"\"\"Initializes the runner class.\"\"\"\n    self.pipeline_config_file = pipeline_config_file\n    self.kafka_config = kafka_config\n    self.metrics_export_port = metrics_export_port\n    self.pipeline_name = pipeline_name\n    self.dataset_prefix = dataset_prefix or \"\"\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.load_pipeline_config","title":"<code>load_pipeline_config()</code>","text":"<p>Loads the config for a given pipeline.</p> <p>Returns:</p> Type Description <code>dict</code> <p>pipeline config</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def load_pipeline_config(self) -&gt; dict:\n    \"\"\"Loads the config for a given pipeline.\n\n    Returns:\n        pipeline config\n    \"\"\"\n    config = ConfigLoader(\n        pipeline_config_file=self.pipeline_config_file,\n    ).load_config()\n\n    return config[\"pipeline\"]\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.prepare_datasets","title":"<code>prepare_datasets(config, user_dataset_prefix=None)</code>","text":"<p>Creates the required datasets for a given pipeline.</p> <p>Datasets can be configured using the <code>params</code> key, using config keys found in: https://kafka.apache.org/documentation.html#topicconfigs</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>dataset configuration found in pipeline config</p> required <code>user_dataset_prefix</code> <code>Optional[str]</code> <p>prefix only for datasets defined by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if dataset \"logging\" is defined in the catalog</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def prepare_datasets(\n    self, config: dict, user_dataset_prefix: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Creates the required datasets for a given pipeline.\n\n    Datasets can be configured using the `params` key, using config keys\n    found in: https://kafka.apache.org/documentation.html#topicconfigs\n\n    Args:\n        config: dataset configuration found in pipeline config\n        Should follow the schema\n            {\n                \"dataset_name\": {\n                    \"type\": str (\"kafka_stream\"),\n                    \"params\": dict\n            }\n        user_dataset_prefix: prefix only for datasets defined by the user.\n        i.e. &lt;prefix&gt;.&lt;user_dataset_prefix&gt;.&lt;dataset_name&gt;\n\n    Returns:\n        True if successful\n\n    Raises:\n        ValueError: if dataset \"logging\" is defined in the catalog\n    \"\"\"\n    # Connect to kafka cluster\n    kafka_client = AdminClient(self.kafka_config)\n\n    # Add prefix to user defined datasets\n    if user_dataset_prefix:\n        config = {\n            f\"{user_dataset_prefix}.{dataset_name}\": dataset_config\n            for dataset_name, dataset_config in config.items()\n        }\n\n    # Fail if reserved dataset names are defined in catalog\n    for reserved_dataset in DEFAULT_KAFKA_CONFIG.get(\"DATASETS\"):\n        if reserved_dataset in config:\n            raise ValueError(\n                f\"Unable to create dataset `{reserved_dataset}`. \"\n                \"Reserved for internal use.\"\n            )\n\n    # Add logging dataset to catalog\n    config[DEFAULT_KAFKA_CONFIG.get(\"LOGGING_DATASET\")] = {\n        \"type\": AINEKO_CONFIG.get(\"KAFKA_STREAM_TYPE\"),\n        \"params\": DEFAULT_KAFKA_CONFIG.get(\"DATASET_PARAMS\"),\n    }\n\n    # Create all dataset defined in the catalog\n    dataset_list = []\n    for dataset_name, dataset_config in config.items():\n        logger.info(\n            \"Creating dataset: %s: %s\", dataset_name, dataset_config\n        )\n        # Create dataset for kafka streams\n        if dataset_config[\"type\"] == AINEKO_CONFIG.get(\"KAFKA_STREAM_TYPE\"):\n            # User defined\n            dataset_params = {\n                **DEFAULT_KAFKA_CONFIG.get(\"DATASET_PARAMS\"),\n                **dataset_config.get(\"params\", {}),\n            }\n\n            # Configure dataset\n            if self.dataset_prefix:\n                topic_name = f\"{self.dataset_prefix}.{dataset_name}\"\n            else:\n                topic_name = dataset_name\n\n            new_dataset = NewTopic(\n                topic=topic_name,\n                num_partitions=dataset_params.get(\"num_partitions\"),\n                replication_factor=dataset_params.get(\"replication_factor\"),\n                config=dataset_params.get(\"config\"),\n            )\n\n            # Add dataset to appropriate list\n            dataset_list.append(new_dataset)\n\n        else:\n            raise ValueError(\n                \"Unknown dataset type. Expected: \"\n                f\"{AINEKO_CONFIG.get('STREAM_TYPES')}.\"\n            )\n\n    # Create all configured datasets\n    datasets = kafka_client.create_topics(dataset_list)\n\n    # Block until all datasets finish creation\n    cur_time = time.time()\n    while True:\n        if all(future.done() for future in datasets.values()):\n            logger.info(\"All datasets created.\")\n            break\n        if time.time() - cur_time &gt; AINEKO_CONFIG.get(\n            \"DATASET_CREATION_TIMEOUT\"\n        ):\n            raise TimeoutError(\n                \"Timeout while creating Kafka datasets. \"\n                \"Please check your Kafka cluster.\"\n            )\n\n    return datasets\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.prepare_nodes","title":"<code>prepare_nodes(pipeline_config, poison_pill)</code>","text":"<p>Prepare actor handles for all nodes.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_config</code> <code>dict</code> <p>pipeline configuration</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>list</code> <p>mapping of node names to actor handles</p> <code>list</code> <code>list</code> <p>list of ray objects</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def prepare_nodes(\n    self, pipeline_config: dict, poison_pill: ray.actor.ActorHandle\n) -&gt; list:\n    \"\"\"Prepare actor handles for all nodes.\n\n    Args:\n        pipeline_config: pipeline configuration\n\n    Returns:\n        dict: mapping of node names to actor handles\n        list: list of ray objects\n    \"\"\"\n    # Collect all  actor futures\n    results = []\n\n    default_node_config = pipeline_config.get(\"default_node_settings\", {})\n\n    for node_name, node_config in pipeline_config[\"nodes\"].items():\n        # Initialize actor from specified class in config\n        target_class = imports.import_from_string(\n            attr=node_config[\"class\"], kind=\"class\"\n        )\n        actor_params = {\n            **default_node_config,\n            **node_config.get(\"node_settings\", {}),\n            \"name\": node_name,\n            \"namespace\": self.pipeline_name,\n        }\n\n        wrapped_class = ray.remote(target_class)\n        wrapped_class.options(**actor_params)\n        actor_handle = wrapped_class.remote(poison_pill=poison_pill)\n\n        # Setup input and output datasets\n        outputs = node_config.get(\"outputs\", [])\n        actor_handle.setup_datasets.remote(\n            inputs=node_config.get(\"inputs\", None),\n            outputs=outputs,\n            datasets=pipeline_config[\"datasets\"],\n            node=node_name,\n            pipeline=self.pipeline_name,\n            has_pipeline_prefix=True,\n        )\n\n        # Setup internal datasets like logging, without pipeline prefix\n        actor_handle.setup_datasets.remote(\n            outputs=DEFAULT_KAFKA_CONFIG.get(\"DATASETS\"),\n            datasets=pipeline_config[\"datasets\"],\n            node=node_name,\n            pipeline=self.pipeline_name,\n        )\n\n        # Create actor future (for execute method)\n        results.append(\n            actor_handle.execute.remote(\n                params=node_config.get(\"node_params\", None)\n            )\n        )\n\n    return results\n</code></pre>"},{"location":"api_reference/runner/#aineko.Runner.run","title":"<code>run()</code>","text":"<p>Runs the pipeline.</p> <p>Step 1: Load config for pipeline</p> <p>Step 2: Set up datasets</p> <p>Step 3: Set up PoisonPill node that is available to all nodes</p> <p>Step 4: Set up nodes (including Node Manager)</p> Source code in <code>aineko/core/runner.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Runs the pipeline.\n\n    Step 1: Load config for pipeline\n\n    Step 2: Set up datasets\n\n    Step 3: Set up PoisonPill node that is available to all nodes\n\n    Step 4: Set up nodes (including Node Manager)\n    \"\"\"\n    # Load pipeline config\n    pipeline_config = self.load_pipeline_config()\n    self.pipeline_name = self.pipeline_name or pipeline_config[\"name\"]\n\n    # Create the necessary datasets\n    self.prepare_datasets(\n        config=pipeline_config[\"datasets\"],\n        user_dataset_prefix=self.pipeline_name,\n    )\n\n    # Initialize ray cluster\n    ray.shutdown()\n    ray.init(\n        namespace=self.pipeline_name,\n        _metrics_export_port=self.metrics_export_port,\n    )\n\n    # Create poison pill actor\n    poison_pill = ray.remote(PoisonPill).remote()\n\n    # Add Node Manager to pipeline config\n    pipeline_config[\"nodes\"][\n        NODE_MANAGER_CONFIG.get(\"NAME\")\n    ] = NODE_MANAGER_CONFIG.get(\"NODE_CONFIG\")\n\n    # Create each node (actor)\n    results = self.prepare_nodes(\n        pipeline_config=pipeline_config,\n        poison_pill=poison_pill,  # type: ignore\n    )\n\n    ray.get(results)\n</code></pre>"},{"location":"developer_guide/aineko_project/","title":"Aineko Project","text":"<p>Note</p> <p>This is a continuation of the previous section (Quick Start). Before starting, make sure you have already created a template project using <code>aineko create</code>.</p>"},{"location":"developer_guide/aineko_project/#directory-contents","title":"Directory contents","text":"<pre><code>.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 conf\n\u2502   \u2514\u2500\u2500 pipeline.yml\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 my_awesome_pipeline\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u2514\u2500\u2500 nodes.py\n\u251c\u2500\u2500 poetry.lock\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 test_nodes.py\n</code></pre> <p>This is how the boilerplate directory look - many of these files are boilerplate files to make things work.</p> <p>Let's zoom in on the more interesting files to take note of:</p> <ol> <li><code>conf/pipeline.yml</code> - This contains your pipeline definition that you are expected to modify to define your own pipeline. It is defined in yaml.</li> <li><code>my_awesome_pipeline/nodes.py</code> - Remember how nodes are abstractions for computations? These nodes are implemented in Python. You do not have to strictly define them in this file. You can define them anywhere you like within the directory as long as you reference them correctly in <code>pipeline.yml</code>.</li> <li><code>docker-compose.yml</code> - When we invoke <code>aineko run</code> , datasets has to be initialised - which means that we need to create Kafka topics. This file contains the image we need to create containers from, as well as other configurations like env var and network settings. You typically do not have to change this file.</li> </ol>"},{"location":"developer_guide/aineko_project/#defining-a-pipeline","title":"Defining a Pipeline","text":"<p>Pipelines are defined using a <code>.yml</code> file that contains specific keys. In this configuration file, you can assemble a pipeline from nodes and datasets.</p> <p>Refer to this for a detailed breakdown on pipeline configuration.</p>"},{"location":"developer_guide/aineko_project/#implementing-a-node","title":"Implementing a Node","text":"<p>A node requires:</p> <ul> <li>Inheriting the base node class <code>aineko.core.node.AbstractNode</code></li> <li>Implementing at least the abstract method <code>_execute</code> and optionally <code>_pre_loop_hook</code> .</li> </ul> <p><code>_pre_loop_hook</code> (optional) is used to initialize the node's state before it starts to process data from the dataset.</p> <p><code>_execute</code> is the main logic that run recurrently. As of writing, user should explicitly produce and consume within this method like so:</p> <pre><code>for dataset, consumer in self.consumers.items():\n    # dataset is the name of the dataset as defined in the pipeline yml configuration\n    # consumer is a DatasetConsumer object\n    msg = consumer.consume(how=\"next\")\n</code></pre>"},{"location":"developer_guide/cli/","title":"CLI Documentation","text":"<p>The Aineko CLI is a dev tool that allows you to get started quickly and introspect your pipeline runs more expediently.</p>"},{"location":"developer_guide/cli/#aineko","title":"aineko","text":"<p>Aineko CLI.</p> <p>Usage:</p> <pre><code>aineko [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --version  Show the version and exit.\n  --help     Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-create","title":"aineko create","text":"<p>Creates boilerplate code and config required for an Aineko pipeline.</p> <p>Args:     deployment_config: If True, include a deploy file when generating, else     do not include.     output_dir: Directory to create pipeline in. Defaults to current.     no_input: If True, do not prompt for parameters and use defaults.</p> <p>Usage:</p> <pre><code>aineko create [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -d, --deployment-config  Include deploy.yml that\n                           facilitates deployment\n                           of pipelines.\n  -o, --output-dir TEXT    Directory to create\n                           pipeline in. Defaults\n                           to current directory.\n  -n, --no-input           Do not prompt for\n                           parameters and use\n                           defaults.\n  --help                   Show this message and\n                           exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-run","title":"aineko run","text":"<p>Main function to run a pipeline from the command line.</p> <p>Args:     pipeline_config_file: Path to the file containing the pipeline config     pipeline_name: Name of the pipeline to run, overrides pipeline config     retry: If true, retry running the pipeline on failure every 10 seconds</p> <p>Usage:</p> <pre><code>aineko run [OPTIONS] PIPELINE_CONFIG_FILE\n</code></pre> <p>Options:</p> <pre><code>  -p, --pipeline-name TEXT  Name of the pipeline\n                            to run.\n  -r, --retry               Retry running the\n                            pipeline on failure.\n  --help                    Show this message and\n                            exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service","title":"aineko service","text":"<p>Manage Aineko docker services (Kafka and Zookeeper containers).</p> <p>Usage:</p> <pre><code>aineko service [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  -c, --config TEXT  Path to the custom Docker\n                     Compose config file.\n  --help             Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service-restart","title":"aineko service restart","text":"<p>Restart Aineko docker services.</p> <p>Usage:</p> <pre><code>aineko service restart [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service-start","title":"aineko service start","text":"<p>Start Aineko docker services.</p> <p>Usage:</p> <pre><code>aineko service start [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-service-stop","title":"aineko service stop","text":"<p>Stop Aineko docker services.</p> <p>Usage:</p> <pre><code>aineko service stop [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-stream","title":"aineko stream","text":"<p>Stream messages from a dataset.</p> <p>Usage:</p> <pre><code>aineko stream [OPTIONS] DATASET\n</code></pre> <p>Options:</p> <pre><code>  -b, --from-beginning  If messages should be\n                        streamed from the start\n  --help                Show this message and\n                        exit.\n</code></pre>"},{"location":"developer_guide/cli/#aineko-visualize","title":"aineko visualize","text":"<p>Builds mermaid graph from an Aineko pipeline config.</p> <p>Args:     config_path: file path to pipeline yaml file     direction: direction of the graph.     legend: include a legend in the graph.     browser: Whether to render graph in browser. Prints     graph to stdout otherwise.</p> <p>Usage:</p> <pre><code>aineko visualize [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> <pre><code>  -d, --direction [TD|LR]  Direction of the graph.\n                           Either LR (left to\n                           right) or TD (top\n                           down).\n  -l, --legend             Include a legend in the\n                           graph.\n  -b, --browser            Render graph in\n                           browser. Prints graph\n                           to stdout otherwise.\n  --help                   Show this message and\n                           exit.\n</code></pre>"},{"location":"developer_guide/concepts/","title":"Concepts","text":"What is an Aineko Pipeline? <p>In day-to-day conversations, the term pipeline frequently denotes either a Pipeline definition or a Pipeline execution. Aineko documentation aims to differentiate them explicitly.</p> <p>An Aineko pipeline is a streaming workflow. This means that data is continuously being transmitted and sent over to different components in a way that allows for real-time processing of the data.</p> <p>In a pipeline, you may implement arbitrary computation units (Nodes), and specify where they consume data from, and where they send that data to (Datasets). An Aineko Pipeline allows you to construct complex processing graphs that processes streaming data.</p>"},{"location":"developer_guide/concepts/#pipeline-definition","title":"Pipeline Definition","text":"<p>A pipeline definition is a specialised Program that we write - to tell us what a pipeline comprises. A Pipeline definition is defined in yaml and essentially allows us to compose computation nodes together by specifying the input and output buffers that they consume data from and produce data to.</p> <p>See here to learn about writing a pipeline definition.</p>"},{"location":"developer_guide/concepts/#pipeline-execution","title":"Pipeline Execution","text":"<p>If a pipeline definition is a program, then a pipeline execution is a process. You can run multiple pipeline executions for a single pipeline definition.</p>"},{"location":"developer_guide/concepts/#dataset","title":"Dataset","text":"<p>A Dataset is an abstraction for a buffer for data that we can define producers and consumers for. Producers write data to a dataset, while consumers read data from the dataset. It is analogous to a pub-sub topic or channel. In the current version of aineko, it is a Kafka Topic, but in future, other implementations of message channels could be pluggable too.</p>"},{"location":"developer_guide/concepts/#node","title":"Node","text":"<p>A Node is an abstraction for some computation, a function if you will. At the same time a Node can be a producer and/or a consumer of a Dataset.</p> <p>A node can optionally consume from topics, process that data and produce the output to another buffer that we can chain other Node consumers on.</p>"},{"location":"developer_guide/config_kafka/","title":"Configuring Kafka","text":"<p>Aineko uses <code>kafka</code> under the hood for sending messages between nodes. As part of running Aineko locally, we recommend running a local <code>kafka</code> and <code>zookeeper</code> server using</p> <pre><code>poetry run aineko service start\n</code></pre> <p>To use a different <code>kafka</code> cluster, such as in deployment settings, we allow for configuring of <code>kafka</code> parameters through environment variables. Typically, you would want to modify configuration for the consumer and producer to point to the desired cluster.</p> <p>See below for default <code>kafka</code> configuration that ships with <code>aineko</code> and how to override them.</p>"},{"location":"developer_guide/config_kafka/#aineko.config","title":"<code>aineko.config</code>","text":"<p>Configuration file for Aineko modules.</p> <p>Kafka configuration can be set using the following environment variables:</p> <p>KAFKA_CONFIG: JSON string with kafka configuration (see https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md for all options)</p> <p>Additionally, the following environment variables can be used to specify certain configuration values. They correspond to configuration keys found in the above link, but with a prefix. For example, <code>KAFKA_CONFIG_BOOTSTRAP_SERVERS</code> corresponds to <code>bootstrap.servers</code>.</p> <ul> <li>KAFKA_CONFIG_BOOTSTRAP_SERVERS (e.g. <code>localhost:9092,localhost:9093</code>)</li> <li>KAFKA_CONFIG_SASL_USERNAME</li> <li>KAFKA_CONFIG_SASL_PASSWORD</li> <li>KAFKA_CONFIG_SECURITY_PROTOCOL</li> <li>KAFKA_CONFIG_SASL_MECHANISM</li> </ul>"},{"location":"developer_guide/config_kafka/#aineko.config.DEFAULT_KAFKA_CONFIG","title":"<code>DEFAULT_KAFKA_CONFIG</code>","text":"<p>             Bases: <code>BaseConfig</code></p> <p>Kafka configuration.</p> Source code in <code>aineko/config.py</code> <pre><code>class DEFAULT_KAFKA_CONFIG(BaseConfig):\n    \"\"\"Kafka configuration.\"\"\"\n\n    # Default Kafka broker settings\n    BROKER_CONFIG = {\n        \"bootstrap.servers\": \"localhost:9092\",\n    }\n\n    kafka_config = os.environ.get(\"KAFKA_CONFIG\", \"{}\")\n    BROKER_CONFIG.update(json.loads(kafka_config))\n\n    # Override these fields if set\n    OVERRIDABLES = {\n        \"KAFKA_CONFIG_BOOTSTRAP_SERVERS\": \"bootstrap.servers\",\n        \"KAFKA_CONFIG_SASL_USERNAME\": \"sasl.username\",\n        \"KAFKA_CONFIG_SASL_PASSWORD\": \"sasl.password\",\n        \"KAFKA_CONFIG_SECURITY_PROTOCOL\": \"security.protocol\",\n        \"KAFKA_CONFIG_SASL_MECHANISM\": \"sasl.mechanism\",\n    }\n    for env, config in OVERRIDABLES.items():\n        value = os.environ.get(env)\n        if value:\n            BROKER_CONFIG[config] = value\n\n    # Config for default kafka consumer\n    CONSUMER_CONFIG: Dict[str, str] = {\n        **BROKER_CONFIG,\n        \"auto.offset.reset\": \"earliest\",\n    }\n\n    # Config for default kafka producer\n    PRODUCER_CONFIG: Dict[str, str] = {**BROKER_CONFIG}\n\n    # Default dataset config\n    DATASET_PARAMS = {\n        # One single partition for each dataset\n        \"num_partitions\": 1,\n        # No replication\n        \"replication_factor\": 1,\n        \"config\": {\n            # Keep messages for 7 days\n            \"retention.ms\": 1000\n            * 60\n            * 60\n            * 24\n            * 7,\n        },\n    }\n\n    # Default Kafka consumer settings\n    # Timeout for kafka consumer polling (seconds)\n    CONSUMER_TIMEOUT = 0\n    # Max number of messages to retreive when getting the last message\n    CONSUMER_MAX_MESSAGES = 1000000\n\n    # Default Kafka producer settings\n    # Producer overridables\n    # See: https://kafka.apache.org/documentation/#producerconfigs\n    # Empty list means no overridable settings\n    PRODUCER_OVERRIDABLES = []  # type: ignore\n\n    # Default datasets to create for every pipeline\n    LOGGING_DATASET = \"logging\"\n    DATASETS = [LOGGING_DATASET]\n</code></pre>"},{"location":"developer_guide/node_implementation/","title":"Building a Node","text":"<p>Nodes are essentially units of compute that encapsulate any event-driven logic you can define in python. Whether it is a transformation, an API call or a data transfer, as long as you can express it in python, it can be contained in a node!</p>"},{"location":"developer_guide/node_implementation/#implementing-a-node","title":"Implementing a Node","text":"<pre><code>from aineko.config import AINEKO_CONFIG\nfrom aineko.internals.node import AbstractNode\n\nclass TestSequencer(AbstractNode):\n    \"\"\"Test sequencer node.\"\"\"\n    ...\n</code></pre> <p>Pre-Loop Hook</p> <p>You can optionally define a <code>_pre_loop_hook</code> method in your node class to intialize the state of your node with class variables. The <code>_pre_loop_hook</code> method consumes params that are provided in the pipeline configuration, so you can define the initial state of your node via your pipeline config.</p> <pre><code>def _pre_loop_hook(self, params: dict = None):\n    \"\"\"Pre loop hook.\"\"\"\n    self.cur_integer = int(params.get(\"start_int\", 0))\n    self.num_messages = 0\n</code></pre> <p>Execute Method</p> <p>The <code>_execute</code> method of the node is wrapped by the <code>execute</code> method in the <code>AbstractNode</code> base class. The <code>_execute</code> method is called constantly in a while loop. The loop only terminates based on the strategy provided to the <code>NodeManager</code> (executes as an infinite loop by default) or by the user returning False from the <code>_execute</code> method. Nodes work best when they are constantly polling for new data from consumers or from outside systems and taking actions depending on the data that is consumed.</p> <pre><code>def _execute(self, params: dict = None):\n    \"\"\"Generates a sequence of integers and writes them to a dataset.\n    Args:\n        params: Parameters for the node\n    Returns:\n        None\n    \"\"\"\n    # Break if duration has been exceeded\n    if self.num_messages &gt;= params.get(\"num_messages\", 25):\n        return False\n    ...\n</code></pre> <p>Producers &amp; Consumers</p> <p>Node classes inherit attributes named <code>self.producers</code> and <code>self.consumers</code> that are each a dictionary, keyed by dataset name with values being <code>DatasetProducer</code> and <code>DatasetConsumer</code> objects, respectively. These objects allow you to produce/consume data to/from a dataset from your catalog config.</p> <pre><code>def _execute(self, params: dict = None):\n    \"\"\"Generates a sequence of integers and writes them to a dataset.\n    Args:\n        params: Parameters for the node\n    Returns:\n        None\n    \"\"\"\n    # Break if duration has been exceeded\n    if self.num_messages &gt;= params.get(\"num_messages\", 25):\n        return False\n\n    # Write message to producer\n    self.producers[\"integer_sequence\"].produce(self.cur_integer)\n    self.log(f\"Produced {self.cur_integer}\", level=\"info\")\n    self.num_messages += 1\n\n    # Increment integer and sleep\n    self.cur_integer += 1\n    time.sleep(params.get(\"sleep_time\", 1))\n</code></pre> <p>The producers and consumers you use in your node must be made available to your node via the pipeline configuration. If a dataset is not available in a Node's catalog, a <code>KeyError</code> will be raised.</p> <p>A node can produce to a dataset, consume from a dataset, or both. Nodes that consume are triggered to action by the arrival of new data in the dataset they consume from.</p> <p>Below is a node that only produces to two datasets, and acts like a source for datasets:</p> <p><pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nN_node_producer_only((node_producer_only)):::nodeClass --&gt;  T_produced_dataset_1[produced_dataset_1]:::datasetClass\nN_node_producer_only((node_producer_only)):::nodeClass --&gt;  T_produced_dataset_2[produced_dataset_2]:::datasetClass</code></pre> The next node only consumes from two datasets, and acts like a sink for datasets: <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_consumed_dataset_1[consumed_dataset_1]:::datasetClass --&gt;  N_node_consumer_only((node_consumer_only)):::nodeClass\nT_consumed_dataset_2[consumed_dataset_2]:::datasetClass --&gt;  N_node_consumer_only((node_consumer_only)):::nodeClass</code></pre></p> <p>A node that both consumes and produces datasets acts like a transformer for datasets. The consumed datasets are the inputs to the transformer, and the produced datasets are the outputs of the transformer: <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_consumed_dataset[consumed_dataset]:::datasetClass --&gt;  N_node_transformer((node_transformer)):::nodeClass\nN_node_transformer((node_transformer)):::nodeClass --&gt;  T_produced_dataset[produced_dataset]:::datasetClass</code></pre> Logging</p> <p>Node classes inherit a method named <code>self.log</code> that allows users to log messages to Amber, where logs are aggregated and triaged across observability pipelines. You can set the appropriate level from: <code>info</code>, <code>debug</code>, <code>warning</code>, <code>error</code>, an <code>critical</code>.</p> <pre><code>self.log(f\"Produced {self.cur_integer}\", level=\"info\")\n</code></pre> <p>You can log from inside of the <code>_pre_loop_hook</code> method, the <code>_execute</code> method, or any other method you add to your node.</p>"},{"location":"developer_guide/pipeline_configuration/","title":"Building a Pipeline","text":"<p>At a high-level, building a pipeline requires defining a pipeline and implementing at least a node.</p>"},{"location":"developer_guide/pipeline_configuration/#defining-a-pipeline","title":"Defining a pipeline","text":"<p><pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nN_sequence((sequence)):::nodeClass --&gt;  T_test_sequence[test_sequence]:::datasetClass</code></pre> For the sake of simplicity, we reference a truncated version of the pipeline definition below: <pre><code>pipeline:\n  name: test-aineko-pipeline\n\n  default_node_settings:\n    num_cpus: 0.5\n\n  nodes:\n    sequence:\n      class: my_awesome_pipeline.nodes.MySequencerNode\n      outputs:\n        - test_sequence\n      node_params:\n        initial_state: 0\n        increment: 1\n\n\n  datasets:\n    test_sequence:\n      type: kafka_stream\n</code></pre></p>"},{"location":"developer_guide/pipeline_configuration/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>A pipeline configuration <code>yml</code> file should have the following schema:</p> <ul> <li><code>name</code> - name of the pipeline</li> <li><code>default_node_settings (optional)</code><ul> <li>&lt;parameter&gt; - Any of the parameters that are passed to ray actor options (e.g. <code>num_cpus</code>)</li> </ul> </li> <li><code>nodes</code></li> <li>&lt;name of node&gt;<ul> <li><code>class</code> - python class to run</li> <li><code>inputs</code> (optional) - dataset(s) to consume from if applicable, should exist in the <code>datasets</code> block</li> <li><code>outputs</code>(optional) - dataset(s) to produce to if applicable, should exist in the <code>datasets</code> block</li> <li><code>node_params</code>(optional) - define any arbitrary params relevant for node's application logic, these are passed as a dictionary into the <code>params</code> argument found in the nodes</li> <li><code>node_settings</code>(optional) - parameters that are passed to ray actor options, will override the ones set in <code>default_node_settings</code></li> </ul> </li> <li><code>datasets</code><ul> <li>&lt;name_of_dataset&gt;<ul> <li><code>type</code> - only <code>kafka_stream</code> is supported right now, which maps to a kafka topic</li> </ul> </li> </ul> </li> </ul> <p>Note</p> <p>Aineko is currently in the Beta release stage and is constantly improving.</p> <p>If you have any feedback, questions, or suggestions, please reach out to us.</p>"},{"location":"examples/aineko-dream/","title":"Aineko Dream: Code Generation using ChatGPT with Real-time QA","text":"<p> View on Github Try on Slack </p> <p>We built an app called Aineko Dream to test-drive Aineko's ability to enable generative AI features. Aineko Dream uses the OpenAI API and the Aineko docs to generate template code for an Aineko pipeline based on a prompt. The pipeline automatically checks the LLM response to ensure it passes some tests and either generates a prompt to fix the errors or passes the response back to the user.</p> <p>This app demonstrates how you can rapidly prototype features that use foundation models by leveraging Aineko features, such as REST client connectors, stateful feedback loops, and API endpoints. Real-time QA allows us to ensure the quality of LLM responses while maintaining an interactive experience for users.</p> <p>Give it a try using the Aineko Dream bot in our Slack. Or checkout the code on GitHub.</p> <p>What will you dream up with Aineko?</p> <p> </p> <p>Aineko Dream in Action</p> <p>Featured Unlocks</p> <p>Tell a Story with Your Data - With Aineko, you retain all critical information with the context to help you tell a story in real-time and retroactively.</p> <p>Aineko made it simple to introduce a feedback loop for real-time QA for ChatGPT responses. We keep track of response evaluation results and submit new prompts to ChatGPT to fix the errors, without any human intervention. Additionally, since we are tracking all prompts, responses, and their evaluation results, we generate a rich dataset of our app\u2019s performance which we can use to track and improve performance.</p> <p>Move Fast, Break Nothing - By representing our use case as an Aineko pipeline, it was clear how we could swap building blocks in and out with ease.</p> <p>Aineko made it easy to try out new foundation models. Once the pipeline was constructed with GPT-3 it was trivial to use GPT-4. Later, we built a second connector node for the the Cohere API and used it as a drop-replacement for OpenAI. We were able to run all three models with standalone pipelines, each with its own unique API endpoint. We can use the various endpoints to route traffic for different users or use one for production and the others for development.</p>"},{"location":"examples/aineko-dream/#tech-overview","title":"Tech overview","text":"<p>With the Aineko framework in mind, we broke the problem down into a few key steps:</p> <ol> <li>Fetch GitHub events</li> <li>Update documentation used for prompt</li> <li>Retrieve prompt from user</li> <li>Prompt engineering</li> <li>Query an LLM</li> <li>Evaluate result</li> <li>Return to step 4, or return LLM response to user</li> </ol> <p>We used the following libraries and APIs to build our app.</p> <p>FastAPI &amp; Uvicorn: Used to run an API server on Aineko and configure endpoints used by the application.</p> <p>PyGitHub: Used as a client to interact with GitHub to fetch relevant documents.</p> <p>OpenAI &amp; Cohere: Used to run inference using their LLMs.</p> <p>Bandit: Used to run security tests against Python code that gets generated by the LLMs.</p>"},{"location":"examples/aineko-dream/#the-pipeline","title":"The Pipeline","text":"<p>We start first with a simple example represented by the following diagram.</p> <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_github_event[github_event]:::datasetClass --&gt;  N_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass\nN_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass --&gt;  T_document[document]:::datasetClass\nT_user_prompt[user_prompt]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nT_document[document]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nN_PromptModel((PromptModel)):::nodeClass --&gt;  T_generated_prompt[generated_prompt]:::datasetClass\nT_generated_prompt[generated_prompt]:::datasetClass --&gt;  N_GPT3Client((GPT3Client)):::nodeClass\nN_GPT3Client((GPT3Client)):::nodeClass --&gt;  T_llm_response[llm_response]:::datasetClass\nT_llm_response[llm_response]:::datasetClass --&gt;  N_APIServer((APIServer)):::nodeClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_user_prompt[user_prompt]:::datasetClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_github_event[github_event]:::datasetClass</code></pre> <p>The API server accepts commit events from GitHub and prompt requests from users. The commit events trigger updates to the document that is used to engineer the LLM prompt. The engineered prompt is passed to the OpenAI API and the response is returned to the API server.</p> <p>If we wanted to add a few evaluation steps to ensure that the LLMs response is valid, we can do so as shown in the following diagram.</p> <pre><code>flowchart LR\nclassDef datasetClass fill:#87CEEB\nclassDef nodeClass fill:#eba487\nT_github_event[github_event]:::datasetClass --&gt;  N_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass\nN_GitHubDocFetcher((GitHubDocFetcher)):::nodeClass --&gt;  T_document[document]:::datasetClass\nT_user_prompt[user_prompt]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nT_document[document]:::datasetClass --&gt;  N_PromptModel((PromptModel)):::nodeClass\nN_PromptModel((PromptModel)):::nodeClass --&gt;  T_generated_prompt[generated_prompt]:::datasetClass\nT_generated_prompt[generated_prompt]:::datasetClass --&gt;  N_GPT3Client((GPT3Client)):::nodeClass\nN_GPT3Client((GPT3Client)):::nodeClass --&gt;  T_llm_response[llm_response]:::datasetClass\nT_llm_response[llm_response]:::datasetClass --&gt;  N_PythonEvaluation((PythonEvaluation)):::nodeClass\nN_PythonEvaluation((PythonEvaluation)):::nodeClass --&gt;  T_evaluation_result[evaluation_result]:::datasetClass\nT_llm_response[llm_response]:::datasetClass --&gt;  N_SecurityEvaluation((SecurityEvaluation)):::nodeClass\nN_SecurityEvaluation((SecurityEvaluation)):::nodeClass --&gt;  T_evaluation_result[evaluation_result]:::datasetClass\nT_evaluation_result[evaluation_result]:::datasetClass --&gt;  N_EvaluationModel((EvaluationModel)):::nodeClass\nN_EvaluationModel((EvaluationModel)):::nodeClass --&gt;  T_final_response[final_response]:::datasetClass\nN_EvaluationModel((EvaluationModel)):::nodeClass --&gt;  T_generated_prompt[generated_prompt]:::datasetClass\nT_final_response[final_response]:::datasetClass --&gt;  N_APIServer((APIServer)):::nodeClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_user_prompt[user_prompt]:::datasetClass\nN_APIServer((APIServer)):::nodeClass --&gt;  T_github_event[github_event]:::datasetClass</code></pre> <p>Here we add 2 evaluation steps and an evaluation model:</p> <ul> <li>The <code>PythonEvaluation</code> node validates that the LLM proposes valid Python code.</li> <li>The <code>SecurityEvaluation</code> node runs checks using Bandit to ensure that the Python code that is proposed doesn\u2019t contain any known security concerns.</li> <li>The <code>EvaluationModel</code> node consumes evaluation results and decides wether to generate another prompt or submit the final result to the API server. It keeps track of the evaluation results and the number of times we query the LLM.</li> </ul>"},{"location":"examples/aineko-dream/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Here is the pipeline config used to generate this example. We could even configure and run three separate pipelines that use different models and expose different endpoints for each of them.</p> <pre><code>pipeline:\n  name: gpt3-template-generator\n\n  nodes:\n    # Prompt generation\n    # This node can be configured to target any GitHub repo\n    GitHubDocFetcher:\n      class: aineko_dream.nodes.GitHubDocFetcher\n      inputs:\n        - github_event\n      outputs:\n        - document\n      node_params:\n        organization: \"aineko-dev\"\n        repo: \"aineko\"\n        branch: \"documentation\"\n        file_path: \"/docs/\"\n    PromptModel:\n      class: aineko_dream.nodes.PromptModel\n      inputs:\n        - user_prompt\n        - document\n      outputs:\n        - generated_prompt\n    # LLM Client: defines model to use. Change to use another model like GPT-4\n    # If we wanted to use Cohere, switch `OpenAIClient` to `Cohere`.\n    GPT3Client:\n      class: aineko_dream.nodes.OpenAIClient\n      inputs:\n        - generated_prompt\n      outputs:\n        - llm_response\n      node_params:\n        model: \"gpt-3.5-turbo-16k\"\n        max_tokens: 4000\n        temperature: 0.1\n    # Response evaluation\n    PythonEvaluation:\n      class: aineko_dream.nodes.PythonEvaluation\n      inputs:\n        - llm_response\n      outputs:\n        - evaluation_result\n    SecurityEvaluation:\n      class: aineko_dream.nodes.SecurityEvaluation\n      inputs:\n        - llm_response\n      outputs:\n        - evaluation_result\n    EvaluationModel:\n      class: aineko_dream.nodes.EvaluationModel\n      inputs:\n        - evaluation_result\n      outputs:\n        - final_response\n        - generated_prompt\n            node_params:\n                max_cycles: 2\n    # API\n    APIServer:\n      class: aineko_dream.nodes.APIServer\n      inputs:\n        - final_response\n      outputs:\n        - user_prompt\n        - github_event\n      node_params:\n        app: aineko_dream.api.main:app\n        port: 8000\n</code></pre>"},{"location":"examples/aineko-dream/#node-code","title":"Node Code","text":"<p>Here are some samples of the node code used to run this pipeline.</p> <p>The <code>GitHubDocFetcher</code> emits the latest document when it initializes and updates the document based on triggers from a GitHub webhook that is configured to target our API server. The document is passed to the <code>PromptModel</code> to engineer a prompt based on the latest document.</p> <pre><code>class GitHubDocFetcher(AbstractNode):\n    \"\"\"Node that fetches code documents from GitHub.\"\"\"\n\n    def _pre_loop_hook(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Initialize connection with GitHub and fetch latest document.\"\"\"\n        # Set parameters\n        self.access_token = os.environ.get(\"GITHUB_ACCESS_TOKEN\")\n        self.organization = params.get(\"organization\")\n        self.repo = params.get(\"repo\")\n        self.branch = params.get(\"branch\")\n        self.file_path = params.get(\"file_path\")\n\n        # Initialize github client\n        auth = Auth.Token(token=self.access_token)\n        self.github_client = Github(auth=auth)\n\n        # Fetch current document\n        self.emit_new_document()\n\n    def _execute(self, params: Optional[dict] = None) -&gt; Optional[bool]:\n        \"\"\"Update document in response to commit events.\"\"\"\n        # Check for new commit events from GitHub\n        message = self.consumers[\"github_event\"].consume()\n        if message is None:\n            return\n\n        # Fetch latest document and send update\n        self.log(\"Received event from GitHub, fetching latest document.\")\n        self.emit_new_document()\n\n    def emit_new_document(self) -&gt; None:\n        \"\"\"Emit new document.\"\"\"\n        repo = self.github_client.get_repo(f\"{self.organization}/{self.repo}\")\n        contents = repo.get_contents(self.file_path, ref=self.branch)\n        github_contents = {f.path: f.decoded_content.decode(\"utf-8\") for f in contents}\n        self.producers[\"document\"].produce(github_contents)\n        self.log(\n            f\"Fetched documents for {self.organization}/{self.repo} branch {self.branch}\"\n        )\n</code></pre> <p>The <code>OpenAIClient</code> node creates a connection to the OpenAI API and submits requests to the configured model using the ChatCompletion interface. The response is passed on to the evaluation nodes.</p> <pre><code>class OpenAIClient(AbstractNode):\n    \"\"\"Node that queries OpenAI LLMs.\"\"\"\n\n    def _pre_loop_hook(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Initialize connection with OpenAI.\"\"\"\n        self.model = params.get(\"model\")\n        self.max_tokens = params.get(\"max_tokens\")\n        self.temperature = params.get(\"temperature\")\n        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    def _execute(self, params: Optional[dict] = None) -&gt; Optional[bool]:\n        \"\"\"Query OpenAI LLM.\"\"\"\n        message = self.consumers[\"generated_prompt\"].consume()\n        if message is None:\n            return\n        messages = message[\"message\"][\"chat_messages\"]\n        # Query OpenAI LLM\n        self.log(\"Querying OpenAI LLM...\")\n        response = openai.ChatCompletion.create(\n            messages=messages,\n            stream=False,\n            model=self.model,\n            max_tokens=self.max_tokens,\n            temperature=self.temperature,\n          )\n        message[\"message\"][\"chat_messages\"].append(\n            {\n                \"role\": \"assistant\",\n                \"content\": response.choices[0].message.content,\n            }\n        )\n        self.producers[\"llm_response\"].produce(\n                        message[\"message\"][\"chat_messages\"]\n                )\n</code></pre> <p>The <code>SecurityEvaluation</code> node takes the LLM response and creates a temporary file with the contents. We then use Bandit to run a test against the file and collect a list of issues. After cleaning up, the results are submitted to the the <code>EvaluationModel</code> node.</p> <pre><code>class SecurityEvaluation(AbstractNode):\n    \"\"\"Node that evaluates security of code.\"\"\"\n\n    def _execute(self, params: Optional[dict] = None) -&gt; None:\n        \"\"\"Evaluate Python code.\"\"\"\n        message = self.consumers[\"llm_response\"].consume()\n        if message is None:\n            return\n\n                # Make a temporary file with the LLM code\n        python_code = message[\"message\"]\n        issues_list = []\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False, mode=\"w\") as tmpfile:\n            tmpfile.write(python_code)\n\n        # Setup Bandit and run tests on the temporary file\n        b_mgr = bandit.manager.BanditManager(bandit.config.BanditConfig(), 'file')\n        b_mgr.discover_files([tmpfile.name], None)\n        b_mgr.run_tests()\n\n        # Store results\n        results = b_mgr.get_issue_list(\n            sev_level=bandit.constants.LOW,\n            conf_level=bandit.constants.LOW,\n            )\n\n        # Cleanup (remove the temporary file)\n        tmpfile.close()\n        os.remove(tmpfile.name)\n\n        if results:\n            self.producers[\"evaluation_result\"].produce(results)\n</code></pre>"},{"location":"examples/aineko-dream/#try-running-locally","title":"Try running locally","text":"<p>Visit GitHub for the latest version of the code and see if you can run it yourself.</p> <p>You can install the app using poetry after cloning from GitHub using:</p> <pre><code>poetry install\n</code></pre> <p>First, make sure that docker is running and run the required docker services in the background using:</p> <pre><code>poetry run aineko service start\n</code></pre> <p>Then start the pipeline using:</p> <pre><code>poetry run aineko run ./conf/gpt3.yml\n</code></pre> <p>When the pipeline is live, you can visit http://127.0.0.1:8000/docs in your browser to interact with the endpoints via the Swagger UI.</p> <p> </p> <p>Swagger UI</p>"},{"location":"examples/aineko-dream/#join-our-community","title":"Join our Community","text":"<p>If you have questions about anything related to Aineko, you're always welcome to ask our community on GitHub or Slack.</p>"}]}